<!doctype html><html lang=es-es dir=ltr data-wc-theme-default=system><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 0.3.1"><meta name=author content="Maicel"><meta name=description content="The highly-customizable Hugo Academic theme powered by Hugo Blox Builder. Easily create your personal academic website."><link rel=alternate hreflang=es-es href=https://maicel.netlify.app/posts/2025-09-06-ia/><link rel=stylesheet href=/css/themes/sky.min.css><link href=/dist/wc.min.40d365a5c94bd94585e708f7c92e5782e00a8d8eefc348f5d2f21a80bb7783c8.css rel=stylesheet><script>window.hbb={defaultTheme:document.documentElement.dataset.wcThemeDefault,setDarkTheme:()=>{document.documentElement.classList.add("dark"),document.documentElement.style.colorScheme="dark"},setLightTheme:()=>{document.documentElement.classList.remove("dark"),document.documentElement.style.colorScheme="light"}},console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`),"wc-color-theme"in localStorage?localStorage.getItem("wc-color-theme")==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme():(window.hbb.defaultTheme==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme(),window.hbb.defaultTheme==="system"&&(window.matchMedia("(prefers-color-scheme: dark)").matches?window.hbb.setDarkTheme():window.hbb.setLightTheme()))</script><script>document.addEventListener("DOMContentLoaded",function(){let e=document.querySelectorAll("li input[type='checkbox'][disabled]");e.forEach(e=>{e.parentElement.parentElement.classList.add("task-list")});const t=document.querySelectorAll(".task-list li");t.forEach(e=>{let t=Array.from(e.childNodes).filter(e=>e.nodeType===3&&e.textContent.trim().length>1);if(t.length>0){const n=document.createElement("label");t[0].after(n),n.appendChild(e.querySelector("input[type='checkbox']")),n.appendChild(t[0])}})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-VN0YPKYGX4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","G-VN0YPKYGX4",{}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><link rel=alternate href=/posts/2025-09-06-ia/index.xml type=application/rss+xml title="Bioestadística edu"><link rel=icon type=image/png href=/media/icon_hu_b45ae4411cf43192.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu_1f7e870b5f23a0ce.png><link rel=canonical href=https://maicel.netlify.app/posts/2025-09-06-ia/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@GetResearchDev"><meta property="twitter:creator" content="@GetResearchDev"><meta property="og:site_name" content="Bioestadística edu"><meta property="og:url" content="https://maicel.netlify.app/posts/2025-09-06-ia/"><meta property="og:title" content="Una Inmersión Intuitiva en la Arquitectura de los LLMs | Bioestadística edu"><meta property="og:description" content="The highly-customizable Hugo Academic theme powered by Hugo Blox Builder. Easily create your personal academic website."><meta property="og:image" content="https://maicel.netlify.app/posts/2025-09-06-ia/featured.png"><meta property="twitter:image" content="https://maicel.netlify.app/posts/2025-09-06-ia/featured.png"><meta property="og:locale" content="es-es"><meta property="og:updated_time" content="2025-09-06T00:00:00+00:00"><title>Una Inmersión Intuitiva en la Arquitectura de los LLMs | Bioestadística edu</title><style>@font-face{font-family:inter var;font-style:normal;font-weight:100 900;font-display:swap;src:url(/dist/font/Inter.var.woff2)format(woff2)}</style><link type=text/css rel=stylesheet href=/dist/pagefind/pagefind-ui.be766eb419317a14ec769d216e9779bfe8f3737c80e780f4ba0dafb57a41a482.css integrity="sha256-vnZutBkxehTsdp0hbpd5v+jzc3yA54D0ug2vtXpBpII="><script src=/dist/pagefind/pagefind-ui.87693d7c6f2b3b347ce359d0ede762c033419f0a32b22ce508c335a81d841f1b.js integrity="sha256-h2k9fG8rOzR841nQ7ediwDNBnwoysizlCMM1qB2EHxs="></script><script>window.hbb.pagefind={baseUrl:"/"}</script><style>html.dark{--pagefind-ui-primary:#eeeeee;--pagefind-ui-text:#eeeeee;--pagefind-ui-background:#152028;--pagefind-ui-border:#152028;--pagefind-ui-tag:#152028}</style><script>window.addEventListener("DOMContentLoaded",e=>{new PagefindUI({element:"#search",showSubResults:!0,baseUrl:window.hbb.pagefind.baseUrl,bundlePath:window.hbb.pagefind.baseUrl+"pagefind/"})}),document.addEventListener("DOMContentLoaded",()=>{let e=document.getElementById("search"),t=document.getElementById("search_toggle");t&&t.addEventListener("click",()=>{if(e.classList.toggle("hidden"),e.querySelector("input").value="",e.querySelector("input").focus(),!e.classList.contains("hidden")){let t=document.querySelector(".pagefind-ui__search-clear");t&&!t.hasAttribute("listenerOnClick")&&(t.setAttribute("listenerOnClick","true"),t.addEventListener("click",()=>{e.classList.toggle("hidden")}))}})})</script><script defer src=/js/hugo-blox-es.min.e6965f872775fcb889e784756e346e58af6799db6f12e6c21a12272f7d3b1c1e.js integrity="sha256-5pZfhyd1/LiJ54R1bjRuWK9nmdtvEubCGhInL307HB4="></script></head><body class="dark:bg-hb-dark dark:text-white page-wrapper" id=top><div id=page-bg></div><div class="page-header sticky top-0 z-30"><header id=site-header class=header><nav class="navbar px-3 flex justify-left"><div class="order-0 h-100"><a class=navbar-brand href=/ title="Bioestadística edu">BiestadisticaEdu</a></div><input id=nav-toggle type=checkbox class=hidden>
<label for=nav-toggle class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1"><svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20"><title>Open Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0V0z"/></svg><svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20"><title>Close Menu</title><polygon points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2" transform="rotate(45 10 10)"/></svg></label><ul id=nav-menu class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left"><li class=nav-item><a class=nav-link href=/>Biografía</a></li><li class=nav-item><a class=nav-link href=/#papers>Publicaciones</a></li><li class=nav-item><a class=nav-link href=/#talks>Conferencias</a></li><li class=nav-item><a class=nav-link href=/experience/>Experiencia</a></li><li class=nav-item><a class=nav-link href=/projects/>Proyectos</a></li><li class=nav-item><a class=nav-link href=/teaching/>Docencia</a></li><li class="nav-item nav-dropdown group relative"><span class="nav-link
inline-flex items-center">Blog<svg class="h-4 w-4 fill-current inline-block" viewBox="0 0 20 20"><path d="M9.293 12.95l.707.707L15.657 8l-1.414-1.414L10 10.828 5.757 6.586 4.343 8z"/></svg></span><ul class="nav-dropdown-list lg:group-hover:visible lg:group-hover:opacity-100"><li class=nav-dropdown-item><a class=nav-dropdown-link href=/posts/>Ver Todos los Artículos</a></li><li class=nav-dropdown-item><a class=nav-dropdown-link href=/categories/estadistica-en-salud/>Estadística en Salud</a></li><li class=nav-dropdown-item><a class=nav-dropdown-link href=/categories/metodologia-de-investigacion/>Metodología de Investigación</a></li><li class=nav-dropdown-item><a class=nav-dropdown-link href=/categories/evaluacion-regulatoria/>Evaluación Regulatoria</a></li><li class=nav-dropdown-item><a class=nav-dropdown-link href=/categories/codigo-practico-en-r-o-python/>Código Práctico en R/Python</a></li><li class=nav-dropdown-item><a class=nav-dropdown-link href=/categories/reflexiones-criticas/>Reflexiones Críticas</a></li></ul></li><li class=nav-item><a class=nav-link href=/subscribe/>Suscríbete</a></li></ul><div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0"><button aria-label=search class="text-black hover:text-primary inline-block px-3 text-xl dark:text-white" id=search_toggle><svg height="16" width="16" viewBox="0 0 512 512" fill="currentcolor"><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8.0 45.3s-32.8 12.5-45.3.0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9.0 208S93.1.0 208 0 416 93.1 416 208zM208 352a144 144 0 100-288 144 144 0 100 288z"/></svg></button><div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
[&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white"><button class="theme-toggle mt-1" accesskey=t title=appearance><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:hidden"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:block [&:not(dark)]:hidden"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div></nav></header><div id=search class="hidden p-3"></div></div><div class="page-body my-10"><div class="max-w-prose mx-auto flex justify-center"><article class="prose prose-slate lg:prose-xl dark:prose-invert"><h1 class=lg:text-6xl>Una Inmersión Intuitiva en la Arquitectura de los LLMs</h1><p>Todos conocemos los Modelos de Lenguaje Grandes (LLMs) como <strong>ChatGPT de OpenAI, Claude de Anthropic, Gemini de Google</strong> y otros modelos similares.</p><p>Son esos asistentes de IA con los que conversamos, que nos ayudan a escribir correos (Stanford Online 2024)electrónicos, a generar ideas e incluso a codificar.</p><p>Pero, ¿alguna vez te has preguntado cómo funcionan realmente estas herramientas?
¿Están pensando o simplemente están creando una <strong>magnífica ilusión de razonamiento</strong>?
En este blog, te mostraré cómo cobran vida estas maravillas tecnológicas, desde el vasto océano de datos hasta su afinada inteligencia, y exploraremos la naturaleza de esa “inteligencia” que tanto nos asombra.</p><p>Puedes ver la versión en vídeo de esta publicación aquí:</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/SxIFozcvCAU?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="YouTube video"></iframe></div><p>A continuación, te explico algunos elementos importantes para entender los Modelos de Lenguaje Grandes (LLMs), como se construyen, cómo se entrenan y predicen sus resultado.</p><img src=fig0.png style=width:30%><h2 id=la-anatomía-de-un-llm-redes-neuronales-y-transformadores>La Anatomía de un LLM: Redes Neuronales y Transformadores</h2><p>En esencia, los LLMs son <strong>redes neuronales</strong>.
Lejos de simular el cerebro humano en un sentido biológico, se basan casi universalmente en una arquitectura particular conocida como <strong>Transformadores</strong>.</p><div class="flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900"><span class="pr-3 pt-1 text-primary-600 dark:text-primary-300"><svg height="24" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25.041-.02a.75.75.0 011.063.852l-.708 2.836a.75.75.0 001.063.853l.041-.021M21 12A9 9 0 113 12a9 9 0 0118 0m-9-3.75h.008v.008H12z"/></svg>
</span><span class=dark:text-neutral-300><strong>Red neuronal artificial:</strong> <em>“Una red neuronal artificial es un sistema de procesamiento paralelo y distribuido, compuesto por unidades simples de procesamiento que tienen la propensión natural de almacenar conocimiento experimental y hacerlo disponible para su uso”</em> (Haykin, n.d.).</span></div><p>Estos Transformadores fueron propuestos por Vaswani et al. en 2017 y se destacaron por su capacidad para “dibujar dependencias globales entre la entrada y la salida” utilizando únicamente mecanismos de atención, sin necesidad de redes recurrentes o convolucionales.</p><p>Esta capacidad es clave para su éxito: permite que el modelo procese grandes cantidades de texto en paralelo y capte relaciones a larga distancia dentro de una secuencia.(Vaswani et al., n.d.)</p><p>Cuando hablamos de entrenar un LLM, hay varios componentes clave que entran en juego:</p><p><figure><div class="flex justify-center"><div class=w-100><img alt srcset="/posts/2025-09-06-ia/fig1_hu_18182d8f524833af.webp 400w,
/posts/2025-09-06-ia/fig1_hu_f33bba6717280fcf.webp 760w,
/posts/2025-09-06-ia/fig1_hu_4ee0477a1071e0cf.webp 1200w" src=/posts/2025-09-06-ia/fig1_hu_18182d8f524833af.webp width=760 height=410 loading=lazy data-zoomable></div></div></figure></p><h2 id=la-primera-etapa-pre-entrenamiento-modelado-del-lenguaje>La Primera Etapa: Pre-entrenamiento (Modelado del Lenguaje)</h2><p>El viaje de un LLM comienza con el <strong>pre-entrenamiento</strong>, un paradigma clásico donde el modelo se entrena para <strong>“modelar todo Internet”</strong>.</p><p>En esta fase, un modelo de lenguaje es, a grandes rasgos, un modelo de <strong>distribución de probabilidad sobre secuencias de tokens o palabras</strong>.</p><div class="flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900"><span class="pr-3 pt-1 text-primary-600 dark:text-primary-300"><svg height="24" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25.041-.02a.75.75.0 011.063.852l-.708 2.836a.75.75.0 001.063.853l.041-.021M21 12A9 9 0 113 12a9 9 0 0118 0m-9-3.75h.008v.008H12z"/></svg>
</span><span class=dark:text-neutral-300><strong>Tokens:</strong> <em>“Un token es una instancia de una secuencia de caracteres en un documento, agrupada como una unidad semántica útil para el procesamiento automático de texto. Esta explicación se basa en una definición clara y rigurosa en el contexto del análisis de información y recuperación de documentos.”</em></span></div><p>Imagina la frase <strong>“El ratón comió el queso”</strong>.</p><img src=fig2.png style=width:30%><p>Un modelo de lenguaje te daría la probabilidad de que esta frase sea pronunciada por un humano o encontrada en línea.</p><p>Si la frase tuviera errores gramaticales como <strong>“El el ratón queso”</strong>, el modelo, con su conocimiento sintáctico, sabría que es menos <strong>probable</strong>.</p><p>Y si fuera <strong>“El queso comió el ratón”</strong>, su conocimiento semántico le indicaría que esto es <strong>improbable</strong>.</p><img src=fig3.png style=width:30%><p><strong>Aquí es donde entra el primer matiz crítico</strong>: este <strong>“conocimiento sintáctico y semántico”</strong> no implica que el modelo <strong>entienda</strong> realmente la gramática o que los quesos no comen ratones.</p><p>Más bien, ha aprendido, a partir de patrones en billones de textos, que ciertas secuencias de palabras son estadísticamente más probables o coherentes que otras.
Es una habilidad predictiva, no una comprensión conceptual.</p><p>Los LLMs son <strong>modelos generativos</strong>.</p><p>Esto significa que, una vez que tienen esta comprensión de las distribuciones de probabilidad, pueden <strong>generar nuevas oraciones o datos</strong> simplemente muestreando de esa distribución.</p><div class="flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900"><span class="pr-3 pt-1 text-primary-600 dark:text-primary-300"><svg height="24" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25.041-.02a.75.75.0 011.063.852l-.708 2.836a.75.75.0 001.063.853l.041-.021M21 12A9 9 0 113 12a9 9 0 0118 0m-9-3.75h.008v.008H12z"/></svg>
</span><span class=dark:text-neutral-300><strong>Modelos generativos:</strong> <em>“Son algoritmos diseñados para crear datos nuevos que parecen provenir de la misma distribución que los datos originales con los que fueron entrenados.”</em></span></div><p>Es decir, <em>saben</em> cómo sonar convincentes y coherentes, pero no necesariamente <em>por qué</em> lo que dicen es correcto o verdadero.</p><h2 id=modelos-de-lenguaje-autorregresivos-prediciendo-la-siguiente-palabra>Modelos de Lenguaje Autorregresivos: Prediciendo la Siguiente Palabra</h2><p>Vamos a adaptar el texto que proporcionaste para que se conecte con nuestro ejemplo del ratón y el queso.🐁🧀</p><h2 id=modelos-de-lenguaje-autorregresivos-prediciendo-la-siguiente-palabra-1>Modelos de Lenguaje Autorregresivos: Prediciendo la Siguiente Palabra</h2><p>Los modelos de lenguaje más modernos, como Gemini, son <strong>autorregresivos</strong>.
Esto significa que predicen la <strong>siguiente palabra basándose en todas las palabras que ya han visto</strong> en la secuencia.</p><p>Piensa en ellos como un narrador que va construyendo una historia palabra por palabra.</p><h2 id=el-proceso-con-el-ratón-comió-el-queso>El Proceso con <strong>“El ratón comió el queso”</strong></h2><p>Imaginemos que el modelo está generando nuestra frase, “El ratón comió el queso.” Este es el fascinante proceso que ocurre:</p><ol><li><p><strong>Secuencia de palabras:</strong> El modelo empieza con la primera palabra de la oración.
Luego, toma las palabras que ya ha generado: <strong>“El ratón”</strong>.</p></li><li><p><strong>Tokenización:</strong> Las palabras se convierten en <strong>tokens</strong> (números o identificadores internos).
Por ejemplo, “El” podría ser <code>143</code>, “ratón” <code>56</code>, y “comió” <code>25</code>.</p></li><li><p><strong>El modelo predice:</strong> Estos tokens numerados entran en el modelo (la “caja negra”).
Basado en todo lo que ha aprendido de internet, el modelo calcula cuál es el próximo token más probable.</p></li><li><p><strong>Distribución de probabilidad:</strong> El modelo no solo predice una palabra, sino que le asigna una <strong>probabilidad a cada palabra</strong> en su vocabulario.
Por ejemplo, después de <strong>“El ratón comió el”</strong>, la palabra <strong>“queso”</strong> podría tener una probabilidad del 85%, “pan” un 10%, y “semillas” un 5%.</p></li><li><p><strong>Muestreo:</strong> El modelo elige el token con la probabilidad más alta, que en este caso es el token para “queso”.
A veces, para no sonar robótico, el modelo elige una palabra con una probabilidad un poco menor, pero en la mayoría de los casos elige la más probable.</p></li><li><p><strong>Detokenización:</strong> El token seleccionado se convierte de nuevo en la palabra “queso”, completando así la frase.</p></li></ol><hr><h2 id=aprendizaje-del-modelo>Aprendizaje del Modelo</h2><p>Durante el <strong>entrenamiento</strong>, el modelo hace este mismo proceso, pero en lugar de generar una frase nueva, compara su predicción con la palabra real en un texto de entrenamiento.</p><ul><li><p>Si el modelo predice <strong>“pan”</strong> y la palabra correcta es <strong>“queso”</strong>, la <strong>función de pérdida de entropía cruzada</strong> le da un “castigo”.</p></li><li><p>Ese castigo se usa para ajustar los pesos del modelo.
El objetivo es que, la próxima vez que vea un contexto similar (“El ratón comió el…”), la probabilidad de que prediga “queso” sea mucho mayor.</p></li></ul><p>Así, la “fluidez” del modelo para generar frases como “El ratón comió el queso” se basa en su capacidad para <strong>predecir estadísticamente</strong> la palabra más probable en cada paso, no en un razonamiento sobre los hábitos alimenticios de los roedores.</p><h2 id=los-tokenizadores-el-primer-paso-crucial-para-la-coherencia>Los Tokenizadores: El Primer Paso Crucial para la “Coherencia”</h2><p>Los <strong>tokenizadores</strong> son componentes extremadamente importantes pero a menudo poco valorados.</p><p>¿Por qué los necesitamos?</p><ul><li><p><strong>Más generales que las palabras</strong>: Las palabras como tokens directos fallan con errores tipográficos o en idiomas que no usan espacios (como el tailandés).</p></li><li><p><strong>Eficiencia de secuencia</strong>: Tokenizar carácter por carácter haría las secuencias demasiado largas, lo que es ineficiente para los Transformadores (cuya complejidad crece cuadráticamente con la longitud de la secuencia).</p></li></ul><p>Los tokenizadores buscan encontrar <strong>subsecuencias comunes</strong> y darles un token específico.
En promedio, un token suele representar alrededor de <strong>tres o cuatro letras</strong>.</p><p>Un algoritmo muy común es la <strong>Codificación de Pares de Bytes (Byte Pair Encoding o BPE)</strong>.
Es fundamental considerar cómo se tokeniza el texto, ya que el <strong>tamaño del vocabulario afecta directamente la dimensionalidad de la salida</strong> del modelo.</p><p><strong>Un punto crítico aquí</strong>: Si bien son útiles, los tokenizadores tienen limitaciones, especialmente con números (matemáticas) y código.</p><p>Por ejemplo, un número como “327” puede tener su propio token, lo que significa que el modelo no lo ve como una composición de “3”, “2”, “7”, lo que dificulta su capacidad para razonar matemáticamente o con la estructura del código.</p><p>Esto nos recuerda que, a pesar de la fluidez, los LLMs operan sobre representaciones simbólicas (tokens) que no siempre se alinean con nuestra comprensión conceptual del lenguaje o las matemáticas.</p><h2 id=de-modelo-de-lenguaje-a-asistente-de-ia-el-post-entrenamiento-o-la-ilusión-de-la-intencionalidad>De Modelo de Lenguaje a Asistente de IA: El Post-entrenamiento (o la Ilusión de la Intencionalidad)</h2><p>Un modelo pre-entrenado es un experto en <strong>“hablar como Internet”</strong>, pero no es un asistente de IA.</p><p>Si le preguntaras a <strong>GPT-3</strong> (un modelo puramente de lenguaje) “explícame el aterrizaje en la luna a un niño de seis años”, podría responder con “explícame la teoría de la gravedad a un niño de seis años” porque ha aprendido que en Internet, una pregunta a menudo es seguida por preguntas similares, no por una respuesta directa.</p><p>El <strong>post-entrenamiento (alignment)</strong> es el proceso que transforma estos modelos en asistentes útiles, asegurándose de que <strong>sigan las instrucciones de los usuarios</strong> y los deseos de los diseñadores (por ejemplo, evitar contenido tóxico).</p><p><strong>Este es el punto donde la ilusión de intencionalidad se vuelve más fuerte.</strong></p><h2 id=1-ajuste-fino-supervisado-supervised-fine-tuning---sft>1. Ajuste Fino Supervisado (Supervised Fine-Tuning - SFT)</h2><p>El primer paso es el <strong>Ajuste Fino Supervisado (SFT)</strong>.</p><p>Aquí, el LLM pre-entrenado se afina con <strong>respuestas deseadas recogidas de humanos</strong>.
Es decir, se le dan ejemplos de preguntas y sus respuestas “correctas” o “ideales” escritas por humanos.</p><p>Este paso fue crucial para el salto de <strong>GPT-3</strong> a <strong>ChatGPT</strong>.</p><p>Curiosamente, no se necesita una cantidad masiva de datos para SFT; <strong>unos pocos miles de ejemplos bien elegidos pueden ser suficientes</strong>.</p><p>Esto sugiere que el SFT no enseña al modelo nuevo conocimiento, sino que le enseña <strong>cómo formatear las respuestas</strong> y optimizar para un “tipo de usuario” específico que ya había visto en sus datos de pre-entrenamiento.</p><p>En otras palabras, el modelo ya tenía el conocimiento latente; el SFT le enseña a <em>expresarlo</em> de la manera que un asistente de IA “debería” hacerlo.</p><p>No está aprendiendo a <em>pensar</em> como un asistente, sino a <em>simular</em> el comportamiento de uno.</p><h2 id=2-aprendizaje-por-refuerzo-a-partir-de-retroalimentación-humana-reinforcement-learning-from-human-feedback---rlhf>2. Aprendizaje por Refuerzo a partir de Retroalimentación Humana (Reinforcement Learning from Human Feedback - RLHF)</h2><p>El SFT tiene sus limitaciones: <strong>Limitado por la habilidad humana</strong>: Los humanos pueden juzgar mejor lo que es una buena respuesta de lo que pueden escribirla ellos mismos.</p><p><strong>Posibles alucinaciones</strong>: Como el SFT se entrena con poca data, si un humano da una respuesta que el modelo no ha visto antes (y por tanto no sabe si es cierta), el modelo puede aprender a “inventar” información plausible pero falsa.</p><p><strong>Aquí el matiz crítico es fundamental</strong>: la “alucinación” (generación de información falsa pero plausible) es una clara evidencia de que los LLMs no “saben” lo que es verdad o mentira, ni tienen un sentido de la realidad.</p><p>Simplemente generan secuencias de tokens que <em>parecen</em> correctas, basándose en los patrones que han aprendido, incluso si no tienen fundamento.</p><p>Es la culminación de la ilusión de razonamiento.
<strong>Costo</strong>: Generar respuestas ideales es muy caro.</p><p>Aquí es donde entra el <strong>RLHF</strong>.</p><p>En lugar de simplemente clonar el comportamiento humano, el objetivo es <strong>maximizar la preferencia humana</strong>.</p><p>El proceso es el siguiente:</p><ol><li><p>Para una instrucción dada, el modelo genera <strong>dos respuestas diferentes</strong>.</p></li><li><p>Etiquetadores humanos seleccionan <strong>cuál de las dos respuestas es mejor</strong>.</p></li><li><p>Con esta retroalimentación, el modelo se afina para generar más de las respuestas “buenas” y menos de las “malas”.</p></li></ol><p>Para hacer esto, se entrena un <strong>modelo de recompensa (Reward Model)</strong>, un clasificador que aprende a predecir cuánto prefiere un humano una respuesta sobre otra, dando una señal de recompensa continua.</p><p>Posteriormente, métodos más simples como la <strong>Optimización Directa por Preferencia (Direct Preference Optimization - DPO)</strong> han demostrado ser igual de efectivos, evitando la complejidad del aprendizaje por refuerzo tradicional.</p><p>En definitiva, RLHF moldea el comportamiento del LLM para alinearse con lo que <em>deseamos</em> ver, no con lo que el modelo <em>sabe</em> o <em>piensa</em>.</p><p>Le enseña a ser complaciente y a evitar lo “tóxico” porque los humanos así lo prefieren, no por un juicio moral inherente.</p><h2 id=la-materia-prima-datos-masivos-y-su-filtrado>La Materia Prima: Datos Masivos y su Filtrado</h2><p>El pre-entrenamiento de los LLMs se realiza sobre <strong>“todo Internet”</strong>.</p><p>Esto incluye vastas colecciones como Common Crawl, que contiene alrededor de <strong>250 mil millones de páginas web y un petabyte de datos</strong>.</p><p>Pero el Internet es “sucio” y no representativo.</p><p>Imagina una página web aleatoria: llena de HTML, publicidad, fragmentos sin terminar.</p><p>Para que estos datos sean útiles, se requieren pasos de procesamiento intensivos, que incluyen:</p><p><figure><div class="flex justify-center"><div class=w-100><img alt srcset="/posts/2025-09-06-ia/fig4_hu_b74c97a4d4debd70.webp 400w,
/posts/2025-09-06-ia/fig4_hu_f0bdf3ad77c25970.webp 760w,
/posts/2025-09-06-ia/fig4_hu_6e63ba7ee8334dc3.webp 1200w" src=/posts/2025-09-06-ia/fig4_hu_b74c97a4d4debd70.webp width=507 height=760 loading=lazy data-zoomable></div></div></figure></p><p>La escala de estos conjuntos de datos es asombrosa, pasando de <strong>150 mil millones de tokens (800 GB)</strong> en benchmarks académicos anteriores, hasta <strong>15 billones de tokens</strong> para modelos de última generación como Llama 3 (equivalente a miles de terabytes).</p><p>La recopilación y curación de datos sigue siendo un desafío enorme y un área activa de investigación.</p><h2 id=las-leyes-de-escalado-el-poder-de-lo-grande-y-sus-implicaciones-en-la-inteligencia>Las Leyes de Escalado: El Poder de lo Grande (y sus Implicaciones en la “Inteligencia”)</h2><p>Uno de los descubrimientos más sorprendentes en LLMs es que <strong>cuantos más datos se entrenen los modelos y más grandes sean los modelos, mejor será su rendimiento</strong>.</p><p>A diferencia de lo que se enseña en muchas clases de aprendizaje automático, el “sobreajuste” (overfitting) no parece ocurrir con los LLMs.</p><p>Las <strong>leyes de escalado</strong> nos muestran que si se aumenta la computación, los datos o el número de parámetros, la pérdida de validación del modelo disminuye de forma predecible y lineal en una escala logarítmica.</p><p>Esto es crucial porque permite a las compañías predecir cuánto mejorarán sus modelos en el futuro y cómo optimizar la asignación de recursos.</p><p>Por ejemplo, el famoso artículo Chinchilla de DeepMind mostró que la relación óptima es entrenar con <strong>20 tokens por cada parámetro</strong> del modelo para maximizar la eficiencia del entrenamiento.</p><p><strong>Un punto analítico aquí</strong>: Que los modelos “mejoren” al escalar no significa que se vuelvan intrínsecamente “más inteligentes” en un sentido humano, o que estén más cerca de la conciencia.</p><p>Simplemente, son <strong>máquinas de patrones increíblemente sofisticadas</strong> que, con más datos y más capacidad computacional, son capaces de reconocer y generar patrones cada vez más complejos y coherentes, reduciendo su “pérdida” (es decir, volviéndose mejores en la predicción del siguiente token).</p><p>La “inteligencia” que percibimos es una propiedad emergente de esta capacidad de predicción a gran escala, no una mente.</p><h2 id=sistemas-el-cerebro-detrás-de-la-eficiencia>Sistemas: El Cerebro Detrás de la Eficiencia</h2><p>La computación es el cuello de botella más grande en el desarrollo de LLMs.
Comprar más GPUs es difícil por su alto costo y escasez, además de las limitaciones físicas en la comunicación entre ellas.</p><p>Es crucial optimizar cómo se asignan los recursos y el pipeline de entrenamiento.</p><p>Algunos trucos clave a nivel de sistemas incluyen: <strong>Baja Precisión (Low Precision)</strong>: Usar números de punto flotante de 16 bits en lugar de 32 bits.</p><p>Esto reduce la cantidad de datos que deben enviarse a las GPUs, acelerando la comunicación y disminuyendo el consumo de memoria.</p><p><strong>Fusión de Operadores (Operator Fusion)</strong>: Las GPUs son muy lentas en la comunicación.
La fusión de operadores combina varias operaciones consecutivas en una sola llamada al kernel, lo que significa que los datos se envían a la GPU una sola vez, todas las operaciones se realizan y luego los resultados se devuelven, lo que acelera significativamente el proceso (por ejemplo, <code>torch.compile</code> en PyTorch puede duplicar la velocidad).</p><h2 id=conclusión-una-ilusión-poderosa-no-un-pensamiento-consciente>Conclusión: Una Ilusión Poderosa, No un Pensamiento Consciente</h2><p>Desde sus cimientos como redes neuronales Transformer, pasando por el pre-entrenamiento con datos masivos de Internet y el afinamiento con retroalimentación humana, hasta la optimización de sistemas y la gestión de costos astronómicos, la creación de un LLM es una hazaña de ingeniería y ciencia de datos.</p><p>La próxima vez que interactúes con un chatbot, recordarás que detrás de esa respuesta fluida hay billones de tokens procesados, complejos algoritmos de entrenamiento, ingeniosas técnicas de afinamiento y una infraestructura computacional masiva trabajando en conjunto.
<strong>Estos modelos no “piensan” en el sentido humano de la palabra, ni poseen conciencia o una comprensión profunda y holística del mundo</strong>.</p><p>Lo que hacen, y lo hacen de manera magistral, es <strong>identificar y reproducir patrones estadísticos</strong> en los datos con los que fueron entrenados.</p><p>Su habilidad para generar texto coherente, relevante y a menudo sorprendentemente “inteligente” es una testamentación de la <strong>efectividad de la predicción a escala masiva</strong>.
Es una <strong>ilusión de razonamiento</strong> tan convincente que a menudo nos hace cuestionar la naturaleza de la inteligencia misma.
Y es, sin duda, una de las maravillas tecnológicas más grandes de nuestro tiempo.</p><h2 id=bibliografía>Bibliografía</h2><div id=refs class="references csl-bib-body hanging-indent" entry-spacing=0><div id=ref-haykin class=csl-entry><p>Haykin, Simon. n.d. “Neural Networks and Learning Machines.” <a href=http://dni.dali.dartmouth.edu/9umv9yghhaoq/13-dayana-hermann-1/read-0131471392-neural-networks-and-learning-machines.pdf target=_blank rel=noopener>http://dni.dali.dartmouth.edu/9umv9yghhaoq/13-dayana-hermann-1/read-0131471392-neural-networks-and-learning-machines.pdf</a>.</p></div><div id=ref-stanfordonline2024 class=csl-entry><p>Stanford Online. 2024. “Stanford CS229 i Machine Learning i Building Large Language Models (LLMs),” August. <a href="https://www.youtube.com/watch?v=9vM4p9NN0Ts" target=_blank rel=noopener>https://www.youtube.com/watch?v=9vM4p9NN0Ts</a>.</p></div><div id=ref-vaswani class=csl-entry><p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. n.d. “Attention Is All You Need.” <a href=https://doi.org/10.48550/ARXIV.1706.03762 target=_blank rel=noopener>https://doi.org/10.48550/ARXIV.1706.03762</a>.</p></div></div></article></div><div class="flex flex-col items-center"><div class="container max-w-[65ch] mx-auto bg-white dark:bg-zinc-900 rounded-xl border-gray-100 dark:border-gray-700 border shadow-md overflow-hidden my-5"></div></div></div><div class=page-footer><footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200"><p class="powered-by text-center">© 2025 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class="powered-by text-center">Made with <a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a>.</p></footer></div></body></html>