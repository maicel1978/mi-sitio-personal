---
title: "C√≥mo Entrenar y Validar Modelos de Predicci√≥n Cl√≠nica: Gu√≠a Paso a Paso para Profesionales de la Salud"
subtitle: "Tutorial para el desarrollo de un modelo de predicci√≥n cl√≠nica con ejemplos en R" 
categories: ["C√≥digo Pr√°ctico en R o Python"]
summary: "Gu√≠a pr√°ctica con R para desarrollar modelos predictivos robustos en entornos cl√≠nicos"
authors:
  - "admin"
  - "Maicel Monz√≥n P√©rez"
date: "2025-10-22" #a√±o mes dia
show_author_profile: true
tags: 
  - "entrenar modelo en R para predicciones cl√≠nicas"
  - "tutorial en r para modelos predictivos m√©dicos"
  - "gu√≠a Steyerberg para modelos predictivos m√©dicos"
  - "evitar overfitting en validaci√≥n de ML"
  - "paso a paso R para machine learning en tesis"
slug: como-entrenar-y-validar-un-modelo-de-machine-learning
featured: true  # Destacar en la p√°gina principal
translationKey: "MPC"
# languages:
#   es: "/es/como-entrenar-y-validar-un-modelo-de-machine-learnig"
#   en: "/en/how-to-train-and-validate-clinical-prediction-model"
related_posts:
  - "eda"  # Slug de tu post sobre EDA, luego a√±adir categoria superior para cluster
# control
show_date: true
reading_time: true
share: true
feedback: true
profile: true
commentable: true
pager: true
show_related: true
show_breadcrumb: true
draft: false# True para un borrador
---


üéß **Escucha el podcast de esta publicaci√≥n**

{{< audio src="https://ia600607.us.archive.org/24/items/articulo-steyember/_Articulo_Steyember.mp3" controls="yes" >}}



```{r setup, echo=FALSE, results='hide'}
knitr::opts_chunk$set(
	echo = TRUE,
	fig.align = "center",
	fig.height = 6,
	fig.width = 8,
	message = FALSE,
	warning = FALSE,
	cache = FALSE,
	collapse = TRUE,
	dev = "png",
	dpi = 150,
	out.width = "80%",
	fig.path = ''
)
```


Si est√° leyendo esta publicaci√≥n, es probable que le interese desarrollar **modelos de predicci√≥n cl√≠nica** para **diagnosticar** o **pronosticar** enfermedades en pacientes. Seguramente sea investigador o estudiante de posgrado ‚Äîen maestr√≠a o doctorado‚Äî en ciencias biom√©dicas, y busque c√≥mo **desarrollar** y **validar** esos modelos en un **art√≠culo cient√≠fico** o **tesis**.

Casi seguro ha llegado a esa fase en la que la metodolog√≠a se siente como un muro de ladrillos: no sabe **qu√© pasos seguir**, **qu√© software usar**, si lo que necesita se puede hacer en **SPSS** o si tendr√° que meterse con esos **c√≥digos** inquietantes de **R o Python** ‚Äîy encima no tiene experiencia en programaci√≥n. Y, lo peor de todo: le asalta la duda de si su modelo servir√° para algo m√°s all√° de llenar un repositorio de tesis‚Ä¶ o si siquiera **ser√° √∫til** para los propios pacientes de su **muestra**, sin hablar de otros **contextos cl√≠nicos**.

Seamos francos: en el fondo, todos le huimos a ese comentario punzante que nos estalla la burbuja predictiva de un solo golpe:

> ¬°Vaya!, un √°rea bajo la curva de ensue√±o, sin rastros de calibraci√≥n ni validaci√≥n externa‚Ä¶ probablemente fruto de una receta cl√°sica: exceso de predictores, dicotomizaci√≥n entusiasta, escasez de eventos y una pizca de selecci√≥n autom√°tica. TRIPOD lo llamar√≠a ‚Äòoptimismo aparente‚Äô; nosotros, la receta del sobreajuste gourmet ‚Äîcon un cigarro y un gui√±o para rematar.

{{% callout note %}} El **sobreajuste** ocurre cuando un modelo aprende tan bien los datos de entrenamiento (incluyendo ruido o detalles irrelevantes) que su rendimiento en datos nuevos es mucho peor. {{% /callout %}}

En otras palabras, el modelo **funciona** espectacularmente bien‚Ä¶ pero **solo con su muestra** de pacientes. *¬°Las m√©tricas de desempe√±o en otros conjuntos de datos son un desastre!* M√°s adelante retomaremos esa idea.

Mientras las tesis de alto nivel se centran en construir modelos de predicci√≥n cl√≠nica con **t√©cnicas estad√≠sticas** cl√°sicas o sofisticados **algoritmos de aprendizaje autom√°tico** como los bosques aleatorios, los cursos de bioestad√≠stica *‚Äî¬°incluso en programas de estudio de esta especialidad!‚Äî* apenas rozan el asunto. La **brecha es real**, y con ella viene la ansiedad de tener que aprenderlo todo ya.

Pero respire hondo. Este post puede endurecer su burbuja predictiva con una capa extra de titanio: su traductor metodol√≥gico no oficial, (casi) libre de l√°grimas y cargado de herramientas pr√°cticas. Yo mismo navegu√© ese infierno metodol√≥gico en mi **proceso doctoral**, as√≠ que no solo le ofrezco **consejos gen√©ricos**: comparto las lecciones de alguien que ya se quem√≥ las pesta√±as por usted, para que avance con **confianza** y **evite** los mismos **tropiezos**.

Olv√≠dese del c√≥digo complejo y enrevesado. Aqu√≠ nos enfocamos en la **l√≥gica esencial** para construir un **modelo predictivo fiable**. Desglosaremos el **proceso paso a paso**, quit√°ndole el miedo al procesamiento de datos con ejemplos de **c√≥digo pr√°ctico en R**.

Si bien este post se centra en las cuestiones pr√°cticas del **procesamiento de datos**, es importante destacar que la elecci√≥n del dise√±o de la investigaci√≥n est√° determinada por el **objetivo del estudio** de predicci√≥n, el cual seg√∫n *TRIPOD (Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis)* se **clasifica** en tres **categor√≠as fundamentales** (1):

- **Desarrollo** de un nuevo modelo

- **Validaci√≥n** de un modelo existente

- **Actualizaci√≥n** de un modelo

Seg√∫n el **objetivo de la investigaci√≥n** se pueden optar por uno de los siguientes **dise√±os** de estudios:

```markmap {height="200px"}
- Dise√±os seg√∫n Objetivo
  - Desarrollo de Modelos
    - Estudios de Cohorte
    - Ensayos Aleatorizados
    - Datos de Atenci√≥n Rutinaria
  - Validaci√≥n de Modelos
    - Estudios de Cohorte Independientes
    - Meta-an√°lisis
    - Datos de Registro
  - Actualizaci√≥n de Modelos
    - Cualquier dise√±o anterior con nuevos datos
    - Datos con Estructura de Cl√∫ster
```
Se recomienda profundizar en estos temas a partir de la bibliograf√≠a propuesta para esta publicaci√≥n. En este post nos centraremos en la **secuencia de etapas clave** para desarrollar un **modelo predictivo**. Como base usar√© el art√≠culo *‚ÄúSiete pasos para el desarrollo y un ABCD para la validaci√≥n‚Äù* de  Ewout Steyerberg‚ÄØ(1), as√≠ como su libro *Clinical Prediction Models*‚ÄØ(2). A la receta a√±adir√© las recomendaciones de Frank Harrell desde su obra *Regression Modeling Strategies*‚ÄØ(3), y nos basaremos en su biblioteca **rms** para desarrollar este tutorial con **c√≥digo pr√°ctico en R**. Finalmente dar√© mi propia visi√≥n del pol√©mico asunto de particionar los datos en **conjunto de entrenamiento** y **conjunto de prueba** en una **proporci√≥n 70:30** y otras cuestiones que seguramente resultar√°n interesantes.

Empezaremos con la **estrategia de modelado**; el tutorial con todo el c√≥digo vendr√° despu√©s.  
**¬°Empecemos!**

## Estrategia de modelado

Seguir una **estrategia de modelado** adecuada es esencial para **desarrollar y validar modelos de predicci√≥n**: no solo **mitiga el sesgo** y el **sobreajuste**, sino que nos **orienta** en un proceso que, de otro modo, puede volverse ca√≥tico.

A continuaci√≥n, se muestran un conjunto de **bibliotecas en R** que nos ayudar√° implementar estas ideas, pero este proceso se puede realizar de igual manera en en SPSS o Python [^1]. 

[^1]: Puede contactarme para una consultor√≠a personalizada donde puedo ayudarle a desarrollar un modelo predictivo o ense√±arle a desarrollar la estrategia de modelado  con otras herramientas como **SPSS** o **Python** a mi correo personal maicel.monzon@gmail.com .

[Puedes  descargar e instalar el paquete **rms** desde su p√°gina oficial en el repositorio de CRAN.](https://cran.r-project.org/web/packages/rms/index.html) 
Tambien le har√°n falta otras bibliotecas para el manejo de datos,  confeccionar tablas y gr√°ficos para su publicaci√≥n, entre otras acciones. 

```{r bibliotecas}
library(conflicted)   # Detecta y resuelve conflictos de funciones
library(rms)          # Modelado log√≠stico restringido (lrm) y splines (rcs)
library(pROC)         # Curvas ROC y AUC
library(ggplot2)      # Gr√°ficos pulidos (usado por rms internamente)
library(missRanger)   # Imputaci√≥n iterativa con random forests
library(mlbench)      # Dataset de ejemplo: PimaIndiansDiabetes
library(dplyr)        # Manipulaci√≥n de datos (pipe %>%)
library(caret)        # Particiones de datos (createDataPartition)
```



### 1. Definici√≥n del problema e inspecci√≥n de datos

El primer paso en cualquier proyecto de modelado es **definir claramente el problema de investigaci√≥n** y seleccionar la **variable de resultado** adecuada.


{{% callout note %}} La **variable de resultado** debe definirse con precisi√≥n: especifique **qu√© evento se predice**, **c√≥mo y cu√°ndo se mide**, y el **horizonte temporal de predicci√≥n** (por ejemplo, mortalidad a los 30 d√≠as). Tambi√©n es clave indicar el m√©todo de evaluaci√≥n del evento y si se aplic√≥ **cegamiento** respecto a los predictores, para asegurar coherencia interna y validez del modelo. {{% /callout %}}

Durante esta fase tambi√©n realizamos un *an√°lisis exploratorio de datos (EDA, por sus siglas en ingles)* para entender las caracter√≠sticas de las variables y detectar posibles problemas, como **datos at√≠picos** o **valores faltantes**. 

[Para conocer m√°s detalles, ver mi post sobre EDA]({{< relref "/post/2025-02-11-eda" >}}).


```{r datos, results='hide'}
set.seed(123)  # Reproducibilidad total (imputaci√≥n, splits, bootstrap)
# Carga del dataset nativo
data("PimaIndiansDiabetes")
datos <- PimaIndiansDiabetes
# Variables cl√≠nicas con 0s imposibles (biol√≥gicamente)
vars_clinicas <- c("glucose", "pressure", "triceps", "insulin", "mass")
# Limpieza: 0 ‚Üí NA, luego imputaci√≥n
datos <- datos %>%
  mutate(across(all_of(vars_clinicas), ~ ifelse(.x == 0, NA, .x))) %>%
  missRanger()  # Imputa NA con random forests iterativos
# nota: idealmente, si se usara una divisi√≥n simple, la imputaci√≥n deber√≠a hacerse solo en el conjunto de entrenamiento.
# Configuraci√≥n rms: datadist para predicciones autom√°ticas
dd <- datadist(as.data.frame(datos))
options(datadist = "dd")
```


### 2. Codificaci√≥n de las variables predictoras

La **codificaci√≥n adecuada de las variables predictoras** es fundamental para construir **modelos robustos**.  

Es probable que necesites **agrupar categor√≠as poco frecuentes** o crear **predictores resumen** para condensar informaci√≥n redundante o altamente correlacionada. Y si tu modelo se basa en **regresi√≥n log√≠stica**, no asumas linealidad de entrada: muchas veces es necesario aplicar **splines c√∫bicos restringidos** para relajar el **supuesto de linealidad** entre  los predictores y el resultado.

üí° **Tip para la publicaci√≥n:**  Reporta cada predictor con su *m√©todo de medici√≥n*, *momento* de registro y *unidades* (si es continuo). Si categorizas, *justifica los puntos de corte*. Si es categ√≥rico, muestra todas las categor√≠as y la de referencia. El modelo final debe reflejar *exactamente* la *codificaci√≥n* usada.


{{% callout warning %}}
**Alerta:** Dicotomizar predictores cuantitativos ‚Äîpor ejemplo, convertir una variable continua como la edad o la presi√≥n arterial en una binaria (‚Äú‚â•65 a√±os = 1‚Äù, ‚Äú<65 = 0‚Äù)‚Äî es una mala pr√°ctica ampliamente desaconsejada. Esta estrategia desperdicia informaci√≥n, reduce el poder estad√≠stico, introduce puntos de corte arbitrarios y aumenta el riesgo de sobreajuste. En lugar de categorizar, modela la relaci√≥n continua (por ejemplo, con splines) para preservar la se√±al cl√≠nica real.
{{% /callout %}}


### 3 .Especificaci√≥n del tipo de modelo

En esta etapa se define la **estructura formal del modelo**, lo que incluye el tipo de relaci√≥n entre variables (p. ej., lineal, no lineal) y, de manera crucial, la **selecci√≥n de los predictores** finales que lo integrar√°n.


{{% callout warning %}}
**Alerta:** La elecci√≥n de predictores no debe basarse en la aplicaci√≥n mec√°nica de m√©todos algor√≠tmicos como la Regresi√≥n Paso a Paso (RPP), ya que suelen producir modelos inestables y sobreajustados, especialmente en contextos biom√©dicos y sociales. 
{{% /callout %}}


En  lugar de la preselecci√≥n de variables basada √∫nicamente en valores *p* de an√°lisis bivariados, se recomienda:

-   Priorizar el **juicio cl√≠nico, la revisi√≥n sistem√°tica de la literatura y la experiencia previa.**

-   Optar por un conjunto reducido de predictores cl√≠nicamente relevantes definidos a priori, o incluir todos los candidatos en el modelo multivariable inicial sin filtrado estad√≠stico previo.

Existen algoritmos que tienen  enfoques alternativos a la selecci√≥n autom√°tica de predictores  como la regresi√≥n Lasso o los √°rboles de clasificaci√≥n. En la gu√≠a propuesta por Heinze se puede profundizar sobre este tema (5)


### 4. Estimaci√≥n del Modelo

Una vez especificado el modelo (es decir, definidos los predictores y la estructura funcional), el siguiente paso tiene como objetivo calcular los coeficientes o par√°metros que mejor se ajusten a los datos.

```{r, fit}
modelo <- lrm(
  diabetes ~ rcs(glucose, 3) + rcs(mass, 3) + rcs(age, 3) + rcs(pedigree, 3) + pregnant,
  data = datos,
  x = TRUE,  # Guarda dise√±o para bootstrap
  y = TRUE   # Guarda respuesta para bootstrap
)
```

En modelos de regresi√≥n, la **estimaci√≥n del modelo**  se realiza a partir de m√©todos como la **m√°xima verosimilitud**. Sin embargo, cuando el n√∫mero de eventos es limitado o el de predictores es alto, el riesgo de sobreajuste es elevado, lo que genera predicciones extremas y poco generalizables. Para mitigarlo, se emplean t√©cnicas de **regularizaci√≥n, penalizaci√≥n o shrinkage**, que ajustan los coeficientes hacia cero para mejorar la estabilidad y la calibraci√≥n en nuevas poblaciones.

El objetivo final no es maximizar el rendimiento aparente en la muestra de desarrollo, sino obtener un modelo con **predicciones estables, bien calibradas y cl√≠nicamente √∫tiles**.

üí° **Tip para la publicaci√≥n:** La ecuaci√≥n final del modelo debe presentarse de forma completa ‚Äîincluyendo todos los coeficientes, el intercepto y, si corresponde, la supervivencia basal‚Äî, reportando m√©tricas de calibraci√≥n y discriminaci√≥n con sus intervalos de confianza. 

En machine learning es com√∫n dividir los datos en entrenamiento y prueba. Sin embargo, en contextos cl√≠nicos es diferente:

- Los **conjuntos de datos** suelen ser **peque√±os o medianos** (usualmente menos de 1000 filas) ‚Üí dividir reduce la informaci√≥n disponible para entrenar.

- Las **m√©tricas** derivadas de la prueba pueden ser **altamente variables**, dependiendo de qu√© observaciones caen en la partici√≥n.

- Esto genera un **modelo menos estable** y menos confiable, especialmente en estimaciones de probabilidades individuales.

{{% callout note %}} La ausencia de particiones en su estudio ser un **tema pol√©mico**, pero no se preocupe!
[M√°s adelante, se comentan algunos argumentos para la discusi√≥n.](#hi) 
{{% /callout %}}


### 5. Evaluaci√≥n del Rendimiento del Modelo


Una vez desarrollado el modelo, es esencial cuantificar su capacidad predictiva antes de su validaci√≥n. Esta evaluaci√≥n se centra en tres aspectos clave:

-   **Calibraci√≥n:** Mide la concordancia entre las probabilidades predichas y las observadas. Por ejemplo, ¬øun 10% de riesgo predicho se corresponde con un 10% de eventos observados? Se eval√∫a visualmente con curvas de calibraci√≥n y cuantitativamente con par√°metros como el intercepto (A, calibraci√≥n-in-the-large) y la pendiente de calibraci√≥n (B).

```{r calibracion}
cal_boot <- calibrate(modelo, method = "boot", B = 100)

plot(cal_boot, main = "Calibraci√≥n: Predichas vs Observadas")
abline(0, 1, lty = 2, col = "red")  # L√≠nea ideal
```

{{< spoiler text="El gr√°fico muestra que el modelo est√° bien calibrado (Click para saber por qu√©)" >}}

La calibraci√≥n perfecta ocurre cuando las probabilidades predichas coinciden con las frecuencias reales. Por ejemplo, si el modelo predice 30% de riesgo, aproximadamente 30 de cada 100 pacientes similares deber√≠an tener la condici√≥n.

{{< /spoiler >}}


-   **Discriminaci√≥n:** Eval√∫a la capacidad del modelo para distinguir entre pacientes que experimentan el evento y aquellos que no. La m√©trica m√°s com√∫n es el estad√≠stico C (o AUC-ROC), que representa la probabilidad de que un paciente con el evento tenga una puntuaci√≥n de riesgo m√°s alta que uno sin √©l.



```{r Discriminacion}
val_boot <- validate(modelo, method = "boot", B = 100)

roc_obj <- roc(datos$diabetes, predict(modelo, type = "fitted"))
plot(roc_obj,
     main = paste("Curva ROC - AUC =", round(auc(roc_obj), 3)),
     col = "blue",
     legacy.axes = TRUE)
```





{{< spoiler text="El gr√°fico del √°rea bajo la curva ROC (AUC) muestra muy buena discriminaci√≥n (Click para saber por qu√©)" >}}
- La curva ROC eval√∫a la capacidad del modelo para distinguir entre pacientes con y sin la condici√≥n.
  - AUC = Excelente (>0.8 se considera muy bueno en medicina)
  - Eje Y: Sensibilidad (capacidad de detectar enfermos)
  - Eje X: 1 - Especificidad (tasa de falsos positivos)

{{< /spoiler >}}



-   **Utilidad Cl√≠nica:** Determina si el modelo es √∫til para la toma de decisiones. El an√°lisis de curvas de decisi√≥n y el Beneficio Neto (NB) permiten evaluar si el uso del modelo conduce a mejores resultados cl√≠nicos netos en comparaci√≥n con estrategias alternativas (como tratar a todos o a ninguno).

### 6. Evaluaci√≥n de la Validez del Modelo

La **evaluaci√≥n del rendimiento** en los datos suele ser optimista. Por ello, es crucial evaluar la validez del modelo en datos no utilizados para su construcci√≥n, un proceso que se divide en:

- **Validaci√≥n Interna:** Eval√∫a la reproducibilidad del modelo, es decir, su rendimiento en m√∫ltiples muestras de la misma poblaci√≥n subyacente. T√©cnicas como el bootstrapping o la validaci√≥n cruzada son superiores a la divisi√≥n simple de la muestra, ya que cuantifican y corrigen el optimismo en las m√©tricas de rendimiento sin reducir el tama√±o de la muestra de desarrollo.

- **Validaci√≥n Externa:** Es la prueba definitiva de la generalizabilidad o transportabilidad del modelo. Consiste en aplicar el modelo a una poblaci√≥n completamente independiente, lo que puede incluir:

  -   **Validaci√≥n Temporal:** Usar pacientes reclutados en un per√≠odo posterior.

  -   **Validaci√≥n Geogr√°fica:** Aplicar el modelo en pacientes de otros centros u hospitales.


### 7. Presentaci√≥n del modelo

La presentaci√≥n efectiva es crucial para la adopci√≥n cl√≠nica. Un modelo perfecto es in√∫til si los m√©dicos no pueden usarlo f√°cilmente. Algunas opciones para llevar el modelo a la pr√°ctica asistencial son:

-   **Nomogramas**: Ideales para uso r√°pido en consulta.

```{r nomograma}
nom <- nomogram(modelo, fun = plogis, funlabel = "Riesgo de Diabetes")
plot(nom, main = "Nomograma del Modelo")

```

{{< spoiler text="Nomograma: calculadora visual del riesgo (Click para aprender a usarlo)" >}}

El **nomograma** es una herramienta visual que permite calcular el riesgo individual de diabetes manualmente, usando las variables clave del modelo. Permite calcular el riesgo espec√≠fico para cada paciente sin necesidad de software, facilitando su uso en consulta

¬øC√≥mo funciona?
- Por cada variable cl√≠nica (glucosa, IMC, edad, etc.) se asigna un puntaje en la escala "Points"
- Se suman todos los puntos para obtener el "Total Points"
- Se proyecta ese total sobre las escalas inferiores para obtener el riesgo predictivo

{{< /spoiler >}}

-   **Aplicaciones web/m√≥viles**: Para integraci√≥n en flujos de trabajo cl√≠nicos

Una aplicaci√≥n funcional para ejecutar el modelo en una pagina web, tel√©fono movil o tableta es una muy buena opci√≥n para generalizar el modelo.

En este enlace te muestro la calculadora web que program√© en js, html y css, para el modelo de mi tesis.

[ver CovidCencecAPK]({{< relref "/project/2025-02-28-covidcencecapk" >}})


## Mis recomendaciones y posici√≥n {#hi}


Reconozco que la **partici√≥n simple de datos (70/30)** es un est√°ndar frecuente y, a menudo, el punto de partida en la pr√°ctica del Machine Learning. No obstante, si nuestro objetivo es fortalecer la credibilidad y la robustez de nuestros hallazgos, especialmente en el √°mbito de los **modelos cl√≠nicos**, este enfoque merece una pausa y una seria reconsideraci√≥n.

La cr√≠tica a esta pr√°ctica no es nueva ni aislada. Autores influyentes en el campo como Frank Harrell y Edmund Steyerberg han expresado su clara oposici√≥n, y otros expertos se suman a este consenso. Como bien lo resume Smedenen en este *tweet*:




{{< x user="MaartenvSmeden" id="1544599686488723461" >}}


Comparto plenamente esta visi√≥n: para muestras peque√±as, como suelen ser los conjuntos de datos de investigaciones cl√≠nicas, una partici√≥n simple ‚Äîaunque sea aleatoria‚Äî introduce m√°s riesgos que beneficios. Esta es una pr√°ctica m√°s com√∫n en entornos de Machine Learning general, donde los datasets son, t√≠picamente, mucho m√°s extensos.

A continuaci√≥n, exploremos en detalle las principales razones por las que este enfoque puede no ser la mejor pr√°ctica cuando la precisi√≥n y la vida de los pacientes est√°n en juego.

### 1. Ineficiencia en el uso de los datos y p√©rdida de poder

Los conjuntos de datos cl√≠nicos son, por naturaleza, a menudo peque√±os (pensemos en menos de 500 pacientes), limitados por costes o la baja incidencia de eventos raros.

Una partici√≥n t√≠pica 70/30 desperdicia hasta un 30% de los datos solo en la evaluaci√≥n, reduciendo dr√°sticamente el tama√±o efectivo para el entrenamiento. Esto provoca una ca√≠da en la m√©trica clave de Eventos por Variable Predictora (EPV) ‚Äîque mide cu√°ntos eventos, como diagn√≥sticos positivos, hay por cada predictor en el modelo‚Äî, situ√°ndola frecuentemente por debajo del umbral recomendado de 10-20.

El resultado es predecible: modelos inestables y sesgados. El rendimiento aparente (ej., un AUC alto en el split de entrenamiento) puede caer dr√°sticamente en datos nuevos (por ejemplo, de 0.85 a un decepcionante 0.65). En contextos cl√≠nicos, donde capturar patrones reales es vital, la recomendaci√≥n es clara: **maximizar todos los datos y aplicar t√©cnicas de remuestreo para una estimaci√≥n m√°s honesta.**

### 2. Evaluaciones de rendimiento con alta variabilidad e inestabilidad

Cuando dependemos de una sola partici√≥n simple, las m√©tricas de rendimiento (como el AUC para la discriminaci√≥n o el Brier score para la calibraci√≥n) se vuelven muy sensibles al azar de c√≥mo se dividieron los datos.

Las simulaciones demuestran que esta "suerte" puede generar variaciones de hasta $\pm 0.10-0.15$ en el AUC, un ruido significativo. Esta varianza se dispara (2-3 veces m√°s) en muestras peque√±as. Un resultado inestable puede inflar o subestimar la calibraci√≥n (ej. en el test de Hosmer-Lemeshow), lo que podr√≠a llevar a tomar decisiones cl√≠nicas err√≥neas y, lo m√°s importante, poner en riesgo a los pacientes (por ejemplo, subtratando a quienes lo necesitan).

Para ilustrar esta inestabilidad, aqu√≠ se simula la variabilidad del AUC en splits simples repetidos utilizando el dataset de nuestro ejemplo:

```{r Simular}
auc_splits <- replicate(100, {
  trainIndex <- createDataPartition(datos$diabetes, p = 0.7, list = FALSE)
  train <- datos[trainIndex, ]; test <- datos[-trainIndex, ]
  
  model_simple <- glm(diabetes ~ glucose + mass + age, family = binomial, data = train)
  pred <- predict(model_simple, test, type = "response")
  
  roc(test$diabetes, pred)$auc
})

hist(auc_splits,
     main = "Distribuci√≥n de AUC en 100 Splits Simples",
     xlab = "AUC",
     col = "lightgray",
     border = "black")
```



### 3. Riesgo de sobreajuste y falta de generalizaci√≥n robusta

Una simple "instant√°nea" proporcionada por la partici√≥n de prueba no es suficiente para corregir el sobreajuste. El modelo, al ajustarse al conjunto de entrenamiento (incluyendo su ruido), ofrece un rendimiento aparente (un "optimismo") que, seg√∫n Harrell, puede ser un 5-20% superior al real.

Sin m√©todos de remuestreo, es imposible obtener un error de generalizaci√≥n realista. M√©todos repetidos (como la validaci√≥n cruzada de k-pliegues) promedian las estimaciones a lo largo de m√∫ltiples splits, lo que reduce el sesgo y mejora la estabilidad.

Cuando no se corrige este optimismo, no solo se afecta la validez interna del estudio, sino que se compromete la generalizaci√≥n del modelo, pues fallar√° al aplicarse a cohortes externas debido a las idiosincrasias (detalles √∫nicos) de la muestra original.

### üí° Mi Sugerencia

En lugar de depender de particiones simples, mi recomendaci√≥n es clara: utilizar m√©todos que aprovechen la totalidad de los datos y ofrezcan estimaciones m√°s estables. Esto incluye:

Validaci√≥n Cruzada Repetida: Divide los datos en particiones y repite el proceso de divisi√≥n y evaluaci√≥n m√∫ltiples veces para promediar los resultados.

Bootstrapping: Muestrea con reemplazo para estimar la estabilidad y corregir el optimismo inherente.

Estos m√©todos no solo corrigen el optimismo sino que reducen la variabilidad, aline√°ndose con las buenas pr√°cticas en el desarrollo de modelos de pron√≥stico cl√≠nico. Por esta raz√≥n, seleccion√© bibliotecas como rms::validate() en R para el ejemplo, ya que se adhieren a esta filosof√≠a.

Para conjuntos de datos peque√±os, la robustez es lo primero. Evite las divisiones simples y priorice la estabilidad para generar resultados que sean verdaderamente √∫tiles en la pr√°ctica asistencial.

**Un Apunte sobre la Validaci√≥n Externa**

Coincido en que la validaci√≥n externa es crucial y debe realizarse con un conjunto de datos independiente en otros contextos, ya sea en diferentes momentos (validaci√≥n temporal) o en distintos hospitales (validaci√≥n geogr√°fica), para confirmar la verdadera generalizaci√≥n.

En mi experiencia, las cuestiones operativas y log√≠sticas que implica realizar una validaci√≥n externa en otro centro o en otro momento a menudo desmotivan al investigador que est√° desarrollando un modelo con datos de su propia consulta. Sin embargo, creo que este enfoque es el que abre el camino para obtener mejores resultados a partir de la investigaci√≥n en condiciones reales. Adem√°s, es un acto de m√©rito y un pilar de la ciencia realizar validaciones de modelos de otros en su propia consulta; la ciencia se nutre de la replicaci√≥n y la colaboraci√≥n.


## ¬°Tu Turno! Pasa de la Teor√≠a a la Pr√°ctica

Este material es solo la punta del iceberg. Hay muchos temas cruciales que, por s√≠ntesis, no abordamos aqu√≠, pero que se construyen sobre esta base: la reproducibilidad, el ajuste del intercepto de la regresi√≥n log√≠stica en escenarios con prevalencias diferentes, los m√©todos de recalibraci√≥n continua, o la construcci√≥n de escalas y clasificaciones cl√≠nicas.

Ahora, me encantar√≠a leerte. ¬°La experiencia es la que enriquece el conocimiento!

üí¨ D√©janos tu Comentario
¬øHas aplicado estas t√©cnicas de remuestreo en tus proyectos de modelos predictivos?

¬øQu√© estrategias usas habitualmente para entrenar y validar tus modelos cl√≠nicos?

Tus comentarios, tus dificultades y tus logros nos ayudan a todos a seguir aprendiendo.

üöÄ Lleva el C√≥digo a tu Proyecto 

√önete a nuestra comunidad de bioestad√≠sticaedu. Al suscribirte, te enviar√© inmediatamente una plantilla completa con todo el c√≥digo comentado (¬°incluyendo tablas y estructura!) para aplicar este proceso en tu pr√≥xima publicaci√≥n o tesis.

ü§ù ¬øNecesitas un Enfoque Personalizado? 

Si quieres aplicar estas estrategias a un dataset espec√≠fico y necesitas la seguridad de un experto a tu lado, tambi√©n puedes contactarme para una consultor√≠a personalizada sobre este tema.


Y recuerda siempre la regla de oro: **¬°Si vas a cometer errores que sean nuevos! üòâ** 

## Bibliograf√≠a

1. Collins GS, Moons KGM, Dhiman P, Riley RD, Beam AL, Van Calster B, et¬†al. TRIPOD+AI statement: updated guidance for reporting clinical prediction models that use regression or machine learning methods. BMJ [Internet]. 16 de abril de 2024 [citado 3 de octubre de 2025];e078378. Disponible en: https://www.bmj.com/lookup/doi/10.1136/bmj-2023-078378


2.  Steyerberg EW, Vergouwe Y. Towards better clinical prediction models: seven steps for development and an ABCD for validation. European Heart Journal [Internet]. 1 de agosto de 2014 [citado 9 de mayo de 2021];35(29):1925-31. Disponible en: <https://academic.oup.com/eurheartj/article-lookup/doi/10.1093/eurheartj/ehu207>

3. Steyerberg E. Clinical Prediction Models. 1th ed. USA: Springer; 2009. (Statistics for Biology and Health). 

4. Harrell Jr FE. Regression Modeling Strategies. 1era ed. New York: Springer; 2015.

5.  Heinze G, Wallisch C, Dunkler D. Variable selection - A review and recommendations for the practicing statistician. Biom J [Internet]. mayo de 2018 [citado 9 de mayo de 2021];60(3):431-49. Disponible en: http://doi.wiley.com/10.1002/bimj.201700067



