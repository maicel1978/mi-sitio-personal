---
title: "C√≥mo entrenar y validar un modelo de predicci√≥n cl√≠nica"
subtitle: "Tutorial para el desarrollo de un modelo de predicci√≥n cl√≠nica con ejemplos en R" 
categories: ["C√≥digo Pr√°ctico en R o Python"]
summary: "Gu√≠a pr√°ctica con R para desarrollar modelos predictivos robustos en entornos cl√≠nicos"
authors:
  - "admin"
date: "2025-10-22" #a√±o mes dia
show_author_profile: true
tags: 
  - "entrenar modelo en R para predicciones cl√≠nicas"
  - "tutorial en r para modelos predictivos m√©dicos"
  - "gu√≠a Steyerberg para modelos predictivos m√©dicos"
  - "evitar overfitting en validaci√≥n de ML"
  - "paso a paso R para machine learning en tesis"
slug: como-entrenar-y-validar-un-modelo-de-machine-learnig
featured: true  # Destacar en la p√°gina principal
# languages:
#   es: "/es/como-entrenar-y-validar-un-modelo-de-machine-learnig"
#   en: "/en/how-to-train-and-validate-clinical-prediction-model"
related_posts:
  - "eda"  # Slug de tu post sobre EDA, luego a√±adir categoria superior para cluster
# control
show_date: true
reading_time: true
share: true
feedback: true
profile: true
commentable: true
pager: true
show_related: true
show_breadcrumb: true
draft: false# True para un borrador
---


üéß **Escucha el podcast de esta publicaci√≥n**

{{< audio src="https://ia600607.us.archive.org/24/items/articulo-steyember/_Articulo_Steyember.mp3" controls="yes" >}}






Si est√° leyendo esta publicaci√≥n, es probable que le interese desarrollar **modelos de predicci√≥n cl√≠nica** para **diagnosticar** o **pronosticar** enfermedades en pacientes. Seguramente sea investigador o estudiante de posgrado ‚Äîen maestr√≠a o doctorado‚Äî en ciencias biom√©dicas, y busque c√≥mo **desarrollar** y **validar** esos modelos en un **art√≠culo cient√≠fico** o **tesis**.

Casi seguro ha llegado a esa fase en la que la metodolog√≠a se siente como un muro de ladrillos: no sabe **qu√© pasos seguir**, **qu√© software usar**, si lo que necesita se puede hacer en **SPSS** o si tendr√° que meterse con esos **c√≥digos** inquietantes de **R o Python** ‚Äîy encima no tiene experiencia en programaci√≥n. Y, lo peor de todo: le asalta la duda de si su modelo servir√° para algo m√°s all√° de llenar un repositorio de tesis‚Ä¶ o si siquiera **ser√° √∫til** para los propios pacientes de su **muestra**, sin hablar de otros **contextos cl√≠nicos**.

Seamos francos: en el fondo, todos le huimos a ese comentario punzante que nos estalla la burbuja predictiva de un solo golpe:

> ¬°Vaya!, un √°rea bajo la curva de ensue√±o, sin rastros de calibraci√≥n ni validaci√≥n externa‚Ä¶ probablemente fruto de una receta cl√°sica: exceso de predictores, dicotomizaci√≥n entusiasta, escasez de eventos y una pizca de selecci√≥n autom√°tica. TRIPOD lo llamar√≠a ‚Äòoptimismo aparente‚Äô; nosotros, la receta del sobreajuste gourmet ‚Äîcon un cigarro y un gui√±o para rematar.

{{% callout note %}} El **sobreajuste** ocurre cuando un modelo aprende tan bien los datos de entrenamiento (incluyendo ruido o detalles irrelevantes) que su rendimiento en datos nuevos es mucho peor. {{% /callout %}}

En otras palabras, el modelo **funciona** espectacularmente bien‚Ä¶ pero **solo con su muestra** de pacientes. *¬°Las m√©tricas de desempe√±o en otros conjuntos de datos son un desastre!* M√°s adelante retomaremos esa idea.

Mientras las tesis de alto nivel se centran en construir modelos de predicci√≥n cl√≠nica con **t√©cnicas estad√≠sticas** cl√°sicas o sofisticados **algoritmos de aprendizaje autom√°tico** como los bosques aleatorios, los cursos de bioestad√≠stica *‚Äî¬°incluso en programas de estudio de esta especialidad!‚Äî* apenas rozan el asunto. La **brecha es real**, y con ella viene la ansiedad de tener que aprenderlo todo ya.

Pero respire hondo. Este post puede endurecer su burbuja predictiva con una capa extra de titanio: su traductor metodol√≥gico no oficial, (casi) libre de l√°grimas y cargado de herramientas pr√°cticas. Yo mismo navegu√© ese infierno metodol√≥gico en mi **proceso doctoral**, as√≠ que no solo le ofrezco **consejos gen√©ricos**: comparto las lecciones de alguien que ya se quem√≥ las pesta√±as por usted, para que avance con **confianza** y **evite** los mismos **tropiezos**.

Olv√≠dese del c√≥digo complejo y enrevesado. Aqu√≠ nos enfocamos en la **l√≥gica esencial** para construir un **modelo predictivo fiable**. Desglosaremos el **proceso paso a paso**, quit√°ndole el miedo al procesamiento de datos con ejemplos de **c√≥digo pr√°ctico en R**.

Si bien este post se centra en las cuestiones pr√°cticas del **procesamiento de datos**, es importante destacar que la elecci√≥n del dise√±o de la investigaci√≥n est√° determinada por el **objetivo del estudio** de predicci√≥n, el cual seg√∫n *TRIPOD (Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis)* se **clasifica** en tres **categor√≠as fundamentales** (1):

- **Desarrollo** de un nuevo modelo

- **Validaci√≥n** de un modelo existente

- **Actualizaci√≥n** de un modelo

Seg√∫n el **objetivo de la investigaci√≥n** se pueden optar por uno de los siguientes **dise√±os** de estudios:

```markmap {height="200px"}
- Dise√±os seg√∫n Objetivo
  - Desarrollo de Modelos
    - Estudios de Cohorte
    - Ensayos Aleatorizados
    - Datos de Atenci√≥n Rutinaria
  - Validaci√≥n de Modelos
    - Estudios de Cohorte Independientes
    - Meta-an√°lisis
    - Datos de Registro
  - Actualizaci√≥n de Modelos
    - Cualquier dise√±o anterior con nuevos datos
    - Datos con Estructura de Cl√∫ster
```
Se recomienda profundizar en estos temas a partir de la bibliograf√≠a propuesta para esta publicaci√≥n. En este post no centraremos en la **secuencia de etapas clave** para desarrollar un **modelo predictivo**. Como base usar√© el art√≠culo *‚ÄúSiete pasos para el desarrollo y un ABCD para la validaci√≥n‚Äù* de  Ewout Steyerberg‚ÄØ(1), as√≠ como su libro *Clinical Prediction Models*‚ÄØ(2). A la receta a√±adir√© las recomendaciones de Frank Harrell desde su obra *Regression Modeling Strategies*‚ÄØ(3), y nos basaremos en su biblioteca **rms** para desarrollar este tutorial con **c√≥digo pr√°ctico en R**. Finalmente dar√© mi propia visi√≥n del pol√©mico asunto de particionar los datos en **conjunto de entrenamiento** y **conjunto de prueba** en una **proporci√≥n 70:30** y otras cuestiones que seguramente resultar√°n interesantes.

Empezaremos con la **estrategia de modelado**; el tutorial con todo el c√≥digo vendr√° despu√©s.  
**¬°Empecemos!**

## Estrategia de modelado

Seguir una **estrategia de modelado** adecuada es esencial para **desarrollar y validar modelos de predicci√≥n**: no solo **mitiga el sesgo** y el **sobreajuste**, sino que nos **orienta** en un proceso que, de otro modo, puede volverse ca√≥tico.

A continuaci√≥n, se muestran un conjunto de **bibliotecas en R** que nos ayudar√° implementar estas ideas, pero este proceso se puede realizar de igual manera en en SPSS o Python [^1]. 

[^1]: Puede contactarme para una consultor√≠a personalizada donde puedo ayudarle a desarrollar un modelo predictivo o ense√±arle a desarrollar la estrategia de modelado  con otras herramientas como **SPSS** o **Python** a mi correo personal maicel.monzon@gmail.com .

[Puedes  descargar e instalar el paquete **rms** desde su p√°gina oficial en el repositorio de CRAN.](https://cran.r-project.org/web/packages/rms/index.html) 
Tambien le har√°n falta otras bibliotecas para el manejo de datos,  confeccionar tablas y gr√°ficos para su publicaci√≥n, entre otras acciones. 


``` r
library(conflicted)   # Detecta y resuelve conflictos de funciones
library(rms)          # Modelado log√≠stico restringido (lrm) y splines (rcs)
library(pROC)         # Curvas ROC y AUC
library(ggplot2)      # Gr√°ficos pulidos (usado por rms internamente)
library(missRanger)   # Imputaci√≥n iterativa con random forests
library(mlbench)      # Dataset de ejemplo: PimaIndiansDiabetes
library(dplyr)        # Manipulaci√≥n de datos (pipe %>%)
library(caret)        # Particiones de datos (createDataPartition)
```



### 1. Definici√≥n del problema e inspecci√≥n de datos

El primer paso en cualquier proyecto de modelado es **definir claramente el problema de investigaci√≥n** y seleccionar la **variable de resultado** adecuada.


{{% callout note %}} La **variable de resultado** debe definirse con precisi√≥n: especifique **qu√© evento se predice**, **c√≥mo y cu√°ndo se mide**, y el **horizonte temporal de predicci√≥n** (por ejemplo, mortalidad a los 30 d√≠as). Tambi√©n es clave indicar el m√©todo de evaluaci√≥n del evento y si se aplic√≥ **cegamiento** respecto a los predictores, para asegurar coherencia interna y validez del modelo. {{% /callout %}}

Durante esta fase tambi√©n realizamos un *an√°lisis exploratorio de datos (EDA, por sus siglas en ingles)* para entender las caracter√≠sticas de las variables y detectar posibles problemas, como **datos at√≠picos** o **valores faltantes**. 

[Para conocer m√°s detalles, ver mi post sobre EDA]({{< relref "/post/2025-02-11-eda" >}}).



``` r
set.seed(123)  # Reproducibilidad total (imputaci√≥n, splits, bootstrap)
# Carga del dataset nativo
data("PimaIndiansDiabetes")
datos <- PimaIndiansDiabetes
# Variables cl√≠nicas con 0s imposibles (biol√≥gicamente)
vars_clinicas <- c("glucose", "pressure", "triceps", "insulin", "mass")
# Limpieza: 0 ‚Üí NA, luego imputaci√≥n
datos <- datos %>%
  mutate(across(all_of(vars_clinicas), ~ ifelse(.x == 0, NA, .x))) %>%
  missRanger()  # Imputa NA con random forests iterativos
# Configuraci√≥n rms: datadist para predicciones autom√°ticas
dd <- datadist(as.data.frame(datos))
options(datadist = "dd")
```


### 2. Codificaci√≥n de las variables predictoras

La **codificaci√≥n adecuada de las variables predictoras** es fundamental para construir **modelos robustos**.  

Es probable que necesites **agrupar categor√≠as poco frecuentes** o crear **predictores resumen** para condensar informaci√≥n redundante o altamente correlacionada. Y si tu modelo se basa en **regresi√≥n log√≠stica**, no asumas linealidad de entrada: muchas veces es necesario aplicar **splines c√∫bicos restringidos** para relajar el **supuesto de linealidad** entre  los predictores y el resultado.

üí° **Tip para la publicaci√≥n:**  Reporta cada predictor con su *m√©todo de medici√≥n*, *momento* de registro y *unidades* (si es continuo). Si categorizas, *justifica los puntos de corte*. Si es categ√≥rico, muestra todas las categor√≠as y la de referencia. El modelo final debe reflejar *exactamente* la *codificaci√≥n* usada.


{{% callout warning %}}
**Alerta:** Dicotomizar predictores cuantitativos ‚Äîpor ejemplo, convertir una variable continua como la edad o la presi√≥n arterial en una binaria (‚Äú‚â•65 a√±os = 1‚Äù, ‚Äú<65 = 0‚Äù)‚Äî es una mala pr√°ctica ampliamente desaconsejada. Esta estrategia desperdicia informaci√≥n, reduce el poder estad√≠stico, introduce puntos de corte arbitrarios y aumenta el riesgo de sobreajuste. En lugar de categorizar, modela la relaci√≥n continua (por ejemplo, con splines) para preservar la se√±al cl√≠nica real.
{{% /callout %}}


### 3 .Especificaci√≥n del tipo de modelo

En esta etapa se define la **estructura formal del modelo**, lo que incluye el tipo de relaci√≥n entre variables (p. ej., lineal, no lineal) y, de manera crucial, la **selecci√≥n de los predictores** finales que lo integrar√°n.


{{% callout warning %}}
**Alerta:** La elecci√≥n de predictores no debe basarse en la aplicaci√≥n mec√°nica de m√©todos algor√≠tmicos como la Regresi√≥n Paso a Paso (RPP), ya que suelen producir modelos inestables y sobreajustados, especialmente en contextos biom√©dicos y sociales. 
{{% /callout %}}


En  lugar de la preselecci√≥n de variables basada √∫nicamente en valores *p* de an√°lisis bivariados, se recomienda:

-   Priorizar el **juicio cl√≠nico, la revisi√≥n sistem√°tica de la literatura y la experiencia previa.**

-   Optar por un conjunto reducido de predictores cl√≠nicamente relevantes definidos a priori, o incluir todos los candidatos en el modelo multivariable inicial sin filtrado estad√≠stico previo.

Existen algoritmos que tienen  enfoques alternativos a la selecci√≥n autom√°tica de predictores  como la regresi√≥n Lasso o los √°rboles de clasificaci√≥n. En la gu√≠a propuesta por Heinze se puede profundizar sobre este tema (5)


### 4. Estimaci√≥n del Modelo

Una vez especificado el modelo (es decir, definidos los predictores y la estructura funcional), el siguiente paso de estimaci√≥n tiene como objetivo calcular los coeficientes o par√°metros que mejor se ajusten a los datos.


``` r
modelo <- lrm(
  diabetes ~ rcs(glucose, 3) + rcs(mass, 3) + rcs(age, 3) + rcs(pedigree, 3) + pregnant,
  data = datos,
  x = TRUE,  # Guarda dise√±o para bootstrap
  y = TRUE   # Guarda respuesta para bootstrap
)
```

En modelos de regresi√≥n, la **estimaci√≥n del modelo**  se realiza a partir de m√©todos como la **m√°xima verosimilitud**. Sin embargo, cuando el n√∫mero de eventos es limitado o el de predictores es alto, el riesgo de sobreajuste es elevado, lo que genera predicciones extremas y poco generalizables. Para mitigarlo, se emplean t√©cnicas de **regularizaci√≥n, penalizaci√≥n o shrinkage**, que ajustan los coeficientes hacia cero para mejorar la estabilidad y la calibraci√≥n en nuevas poblaciones.

El objetivo final no es maximizar el rendimiento aparente en la muestra de desarrollo, sino obtener un modelo con **predicciones estables, bien calibradas y cl√≠nicamente √∫tiles**.

üí° **Tip para la publicaci√≥n:** La ecuaci√≥n final del modelo debe presentarse de forma completa ‚Äîincluyendo todos los coeficientes, el intercepto y, si corresponde, la supervivencia basal‚Äî, reportando m√©tricas de calibraci√≥n y discriminaci√≥n con sus intervalos de confianza. 

En machine learning es com√∫n dividir los datos en entrenamiento y prueba. Sin embargo, en contextos cl√≠nicos es diferente:

- Los **conjuntos de datos** suelen ser **peque√±os o medianos** (usualmente menos de 1000 filas) ‚Üí dividir reduce la informaci√≥n disponible para entrenar.

- Las **m√©tricas** derivadas de la prueba pueden ser **altamente variables**, dependiendo de qu√© observaciones caen en la partici√≥n.

- Esto genera un **modelo menos estable** y menos confiable, especialmente en estimaciones de probabilidades individuales.

{{% callout note %}} La ausencia de particiones en su estudio ser un **tema pol√©mico**, pero no se preocupe!
[M√°s adelante, se comentan algunos argumentos para la discusi√≥n.](#hi) 
{{% /callout %}}


### 5. Evaluaci√≥n del Rendimiento del Modelo


Una vez desarrollado el modelo, es esencial cuantificar su capacidad predictiva antes de su validaci√≥n. Esta evaluaci√≥n se centra en tres aspectos clave:

-   **Calibraci√≥n:** Mide la concordancia entre las probabilidades predichas y las observadas. Por ejemplo, ¬øun 10% de riesgo predicho se corresponde con un 10% de eventos observados? Se eval√∫a visualmente con curvas de calibraci√≥n y cuantitativamente con par√°metros como el intercepto (A, calibraci√≥n-in-the-large) y la pendiente de calibraci√≥n (B).


``` r
cal_boot <- calibrate(modelo, method = "boot", B = 100)

plot(cal_boot, main = "Calibraci√≥n: Predichas vs Observadas")
## 
## n=768   Mean absolute error=0.016   Mean squared error=0.00045
## 0.9 Quantile of absolute error=0.034
abline(0, 1, lty = 2, col = "red")  # L√≠nea ideal
```

<img src="calibracion-1.png" width="80%" style="display: block; margin: auto;" />

-   **Discriminaci√≥n:** Eval√∫a la capacidad del modelo para distinguir entre pacientes que experimentan el evento y aquellos que no. La m√©trica m√°s com√∫n es el estad√≠stico C (o AUC-ROC), que representa la probabilidad de que un paciente con el evento tenga una puntuaci√≥n de riesgo m√°s alta que uno sin √©l.




``` r
val_boot <- validate(modelo, method = "boot", B = 100)

roc_obj <- roc(datos$diabetes, predict(modelo, type = "fitted"))
plot(roc_obj,
     main = paste("Curva ROC - AUC =", round(auc(roc_obj), 3)),
     col = "blue",
     legacy.axes = TRUE)
```

<img src="Discriminacion-1.png" width="80%" style="display: block; margin: auto;" />





-   **Utilidad Cl√≠nica:** Determina si el modelo es √∫til para la toma de decisiones. El an√°lisis de curvas de decisi√≥n y el Beneficio Neto (NB) permiten evaluar si el uso del modelo conduce a mejores resultados cl√≠nicos netos en comparaci√≥n con estrategias alternativas (como tratar a todos o a ninguno).

### 6. Evaluaci√≥n de la Validez del Modelo

La **evaluaci√≥n del rendimiento** en los datos suele ser optimista. Por ello, es crucial evaluar la validez del modelo en datos no utilizados para su construcci√≥n, un proceso que se divide en:

- **Validaci√≥n Interna:** Eval√∫a la reproducibilidad del modelo, es decir, su rendimiento en m√∫ltiples muestras de la misma poblaci√≥n subyacente. T√©cnicas como el bootstrapping o la validaci√≥n cruzada son superiores a la divisi√≥n simple de la muestra, ya que cuantifican y corrigen el optimismo en las m√©tricas de rendimiento sin reducir el tama√±o de la muestra de desarrollo.

- **Validaci√≥n Externa:** Es la prueba definitiva de la generalizabilidad o transportabilidad del modelo. Consiste en aplicar el modelo a una poblaci√≥n completamente independiente, lo que puede incluir:

  -   **Validaci√≥n Temporal:** Usar pacientes reclutados en un per√≠odo posterior.

  -   **Validaci√≥n Geogr√°fica:** Aplicar el modelo en pacientes de otros centros u hospitales.


### 7. Presentaci√≥n del modelo

La presentaci√≥n efectiva es crucial para la adopci√≥n cl√≠nica. Un modelo perfecto es in√∫til si los m√©dicos no pueden usarlo f√°cilmente. Algunas opciones para llevar el modelo a la pr√°ctica asistencial son:

-   **Nomogramas**: Ideales para uso r√°pido en consulta.


``` r
nom <- nomogram(modelo, fun = plogis, funlabel = "Riesgo de Diabetes")
plot(nom, main = "Nomograma del Modelo")
```

<img src="nomograma-1.png" width="80%" style="display: block; margin: auto;" />


-   **Aplicaciones web/m√≥viles**: Para integraci√≥n en flujos de trabajo cl√≠nicos

Una aplicaci√≥n funcional para ejecutar el modelo en una pagina web, tel√©fono movil o tableta es una muy buena opci√≥n para generalizar el modelo.

En este enlace te muestro la calculadora web que program√© en js, html y css, para el modelo de mi tesis.

[ver CovidCencecAPK]({{< relref "/project/2025-02-28-covidcencecapk" >}})


## Recomendaciones y an√°lisis cr√≠tico {#hi}


En el contexto de modelos predictivos cl√≠nicos, una pr√°ctica com√∫n pero problem√°tica es el uso exclusivo de una **partici√≥n simple (entranamento/preuba)** para el desarrollo, ajuste y evaluaci√≥n final. A continuaci√≥n, detallo los motivos clave con argumentos fundamentados.

El enfoque es aceptado por buena parte de la comunidad cient√≠fica como se pono de manifiesto en el siguiente tweet:


<!-- {{< x user="MaartenvSmeden" id="1544599686488723461" >}} -->




Pero abordemos en detalles algunas de las principales razones para abandonar esa pr√°ctica en contextos como los que habitualmente nos enfrentamos. 



### 1. Ineficiencia en el uso de los datos y p√©rdida de poder

Los datasets cl√≠nicos suelen ser peque√±os (por ejemplo, menos de 500 pacientes) debido a limitaciones como costos o eventos raros. Una partici√≥n t√≠pica (70% train / 30% test) desperdicia hasta un 30% de los datos en la evaluaci√≥n, reduciendo el tama√±o efectivo para entrenar. Esto baja los eventos por variable predictora (EPV) ‚Äîuna m√©trica que indica cu√°ntos eventos (como diagn√≥sticos positivos) hay por cada predictor en el modelo‚Äî por debajo de 10-20, lo que genera modelos inestables y sesgados. Como resultado, el rendimiento aparente (por ejemplo, AUC alta en train) cae dr√°sticamente en datos nuevos (de 0.85 a 0.65). En contextos cl√≠nicos, maximizar todos los datos es clave para capturar patrones reales sin perder poder estad√≠stico.

### 2. Evaluaciones de rendimiento con alta variabilidad e inestabilidad

Con una sola partici√≥n, las m√©tricas (como AUC para discriminaci√≥n o Brier score para calibraci√≥n, que mide el acuerdo entre predicciones y resultados reales) dependen del azar de c√≥mo se dividen los datos. Simulaciones muestran variaciones de ¬±0.10-0.15 en AUC, similar a ruido aleatorio, y una varianza 2-3 veces mayor en muestras peque√±as (por ejemplo, 200 pacientes con 10% de eventos). Esto puede inflar o subestimar la calibraci√≥n (evaluada con tests como Hosmer-Lemeshow), llevando a decisiones cl√≠nicas err√≥neas, como subtratar pacientes.

Para ilustrarlo en R con un dataset de ejemplo (PimaIndiansDiabetes), simula la variabilidad de AUC en splits repetidos:


``` r
auc_splits <- replicate(100, {
  trainIndex <- createDataPartition(datos$diabetes, p = 0.7, list = FALSE)
  train <- datos[trainIndex, ]; test <- datos[-trainIndex, ]
  
  model_simple <- glm(diabetes ~ glucose + mass + age, family = binomial, data = train)
  pred <- predict(model_simple, test, type = "response")
  
  roc(test$diabetes, pred)$auc
})

hist(auc_splits,
     main = "Distribuci√≥n de AUC en 100 Splits Simples",
     xlab = "AUC",
     col = "lightgray",
     border = "black")
```

<img src="Simular-1.png" width="80%" style="display: block; margin: auto;" />

En comparaci√≥n, m√©todos repetidos como validaci√≥n cruzada son m√°s estables y confiables.

### 3. Riesgo de sobreajuste y falta de generalizaci√≥n robusta

Una "instant√°nea" en el test set no corrige sobreajuste: el modelo se ajusta al train (incluyendo ruido), inflando rendimiento aparente (optimismo de 5-20% en R¬≤, per Harrell). Sin repetici√≥n, no se obtiene un error de generalizaci√≥n realista. M√©todos repetidos (e.g., k-fold CV) promedian estimaciones para reducir sesgo, mejorando estabilidad. Esto afecta validez interna (sin correcci√≥n de optimismo) y generalizaci√≥n (fallos en cohortes externas por idiosincrasias de la muestra).



### Recomendaciones pr√°cticas

En su lugar, usa m√©todos que aprovechen todos los datos, como validaci√≥n cruzada repetida (divide en folds, repite m√∫ltiples veces para promediar) o bootstrapping (muestrea con reemplazo para estimar estabilidad). Estos corrigen optimismo y reducen variabilidad, aline√°ndose con buenas pr√°cticas para modelos cl√≠nicos. En R, prueba rms::validate() para bootstrapping. Para datasets peque√±os, evita splits simples: prioriza robustez para resultados √∫tiles en la pr√°ctica asistencial.

{{% callout warning %}}
**Recomendaci√≥n clave:** En datasets cl√≠nicos peque√±os, evite particiones simples; opte por bootstrapping en `rms` para validaci√≥n interna. Esto asegura modelos estables y √∫tiles, alineados con evidencia cient√≠fica.
{{% /callout %}}



## Llamada a la acci√≥n 

¬øHas aplicado estas t√©cnicas en tus proyectos de Machine Learning? ¬øQu√© estrategias usas para entrenar y validar tus modelos? D√©jame tus comentarios üí¨: comparte tus experiencias, dificultades o tips contigo. ¬°Juntos podemos enriquecer este conocimiento!


## Bibliograf√≠a

1. Collins GS, Moons KGM, Dhiman P, Riley RD, Beam AL, Van Calster B, et¬†al. TRIPOD+AI statement: updated guidance for reporting clinical prediction models that use regression or machine learning methods. BMJ [Internet]. 16 de abril de 2024 [citado 3 de octubre de 2025];e078378. Disponible en: https://www.bmj.com/lookup/doi/10.1136/bmj-2023-078378


2.  Steyerberg EW, Vergouwe Y. Towards better clinical prediction models: seven steps for development and an ABCD for validation. European Heart Journal [Internet]. 1 de agosto de 2014 [citado 9 de mayo de 2021];35(29):1925-31. Disponible en: <https://academic.oup.com/eurheartj/article-lookup/doi/10.1093/eurheartj/ehu207>

3. Steyerberg E. Clinical Prediction Models. 1th ed. USA: Springer; 2009. (Statistics for Biology and Health). 

4. Harrell Jr FE. Regression Modeling Strategies. 1era ed. New York: Springer; 2015.

5.  Heinze G, Wallisch C, Dunkler D. Variable selection - A review and recommendations for the practicing statistician. Biom J [Internet]. mayo de 2018 [citado 9 de mayo de 2021];60(3):431-49. Disponible en: http://doi.wiley.com/10.1002/bimj.201700067



