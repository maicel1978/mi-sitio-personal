---
title: "El Frankenstein MetodolÃ³gico: La Bestia Silenciosa en los Ensayos ClÃ­nicos"
subtitle: "CÃ³mo Protocolos Impecables Generan Evidencia FrÃ¡gil y Decisiones ErrÃ³neas"
summary: "En los ensayos clÃ­nicos, un anÃ¡lisis correcto puede ocultar evidencia falsa si la pregunta clÃ­nica no se alinea con los estimandos reales. Exploramos este monstruo con un caso simulado en R."
author: "admin"
date: "2025-12-12"
categories: ["EvaluaciÃ³n Regulatoria"]
tags:
  - "evaluacion-regulatoria"
  - "ensayos-clÃ­nicos"
  - "simulacion-R"
  - "sesgo-ensayos"
slug: "er-01"
featured: true
draft: false
share: true
commentable: true
show_related: true
show_breadcrumb: true
type: post
---

## IntroducciÃ³n

Abres un informe de estudio clÃ­nico (CSR) o un artÃ­culo en *The Lancet* y todo brilla: estructura impecable, tablas claras, variables bien definidas y un plan de anÃ¡lisis estadÃ­stico (SAP) que parece inquebrantable.

Pero, debajo de esa fachada, a veces acecha el **Frankenstein metodolÃ³gico**: piezas tÃ©cnicamente correctas que, al ensamblarse, terminan respondiendo una pregunta que nadie formulÃ³ de manera explÃ­cita.

No es un error de principiante. Es un problema sistÃ©mico: la fricciÃ³n entre la intenciÃ³n clÃ­nica y la realidad operativa del estudio.

Casos histÃ³ricos como rofecoxib (Vioxx) recuerdan que un programa de ensayos puede presentar resultados convincentes en un aspecto y, al mismo tiempo, subestimar riesgos relevantes. MÃ¡s allÃ¡ del caso concreto, la lecciÃ³n general es clara: un informe puede ser â€œcorrectoâ€ en lo tÃ©cnico y, aun asÃ­, conducir a una lectura equivocada si la pregunta, los datos y el anÃ¡lisis no estÃ¡n alineados.

El nÃºcleo del problema no es que â€œlos nÃºmeros estÃ©n malâ€, sino que **la seÃ±al estadÃ­stica puede quedar desconectada del sentido clÃ­nico**. Y entonces la historia se sostiene en el papel, pero se desmorona cuando hay que tomar decisiones.


---

## QuÃ© es el Frankenstein metodolÃ³gico

Llamo **Frankenstein metodolÃ³gico** a esta situaciÃ³n:

> El protocolo promete responder una pregunta clÃ­nica, pero el anÃ¡lisis final termina estimando otro efecto con implicaciones causales distintas (por cÃ³mo se manejan abandonos, tratamientos de rescate, cambios de tratamiento y datos faltantes), sin explicitarlo con claridad.

No es necesariamente fraude, ni siempre mala prÃ¡ctica individual. A menudo es el resultado de procesos bien intencionados que pierden coherencia entre etapas.

Suele aparecer cuando confluyen tres elementos:

1. **Promesa inicial:** el protocolo define la pregunta clÃ­nica y la operacionaliza mediante *estimandos* (segÃºn ICH E9(R1)).  
2. **FricciÃ³n inevitable:** desviaciones, pÃ©rdidas de seguimiento y **sucesos intercurrentes** (por ejemplo, discontinuaciÃ³n, rescate, cambio de tratamiento) alteran el escenario previsto.  
3. **PresentaciÃ³n confusa:** el informe conserva etiquetas (por ejemplo, Â«intenciÃ³n de tratarÂ») aunque el anÃ¡lisis, en la prÃ¡ctica, estÃ© estimando algo distinto, sin suficiente transparencia.

---

## GÃ©nesis del Monstruo: De la TeorÃ­a a la Tragedia


### 1. TraducciÃ³n Traicionera: ClÃ­nica vs. Operacional


Los clÃ­nicos piensan en fenÃ³menos (â€œmejorar la calidad de vidaâ€, â€œreducir exacerbacionesâ€, â€œprolongar la supervivenciaâ€), mientras que los equipos estadÃ­sticos necesitan variables medibles, reglas de mediciÃ³n y decisiones claras ante eventos que interrumpen el curso ideal del tratamiento.

Si ese diÃ¡logo no se cierra, el resultado es previsible: la pregunta clÃ­nica y el efecto que se termina estimando se separan. Un ejemplo tÃ­pico es tratar la supervivencia libre de progresiÃ³n como si fuera inmune a discontinuaciones por toxicidad o a tratamientos posteriores, cuando esos sucesos pueden sesgar la interpretaciÃ³n.

**RecomendaciÃ³n:** define explÃ­citamente, en lenguaje operativo, los cuatro componentes del estimando: poblaciÃ³n, variable, manejo de sucesos intercurrentes y medida resumen, conforme a ICH E9(R1).


### 2. Etiquetas como Armadura Falsa

La expresiÃ³n **Â«anÃ¡lisis por intenciÃ³n de tratarÂ» (ITT)** evoca rigor, pero una etiqueta no sustituye una definiciÃ³n.

En ensayos aleatorizados, ITT suele entenderse como â€œanalizar segÃºn el grupo asignado al azarâ€, porque la aleatorizaciÃ³n es la protecciÃ³n causal. En estudios no aleatorizados o de un solo brazo, esa protecciÃ³n no existe; usar Â«ITTÂ» sin aclarar quÃ© significa (y quÃ© se hizo con abandonos, cambios de tratamiento o faltantes) puede inducir a error.

El problema central no es usar siglas, sino usarlas para evitar preguntas incÃ³modas: Â¿quiÃ©n entrÃ³ al anÃ¡lisis, bajo quÃ© reglas, y quÃ© supuestos sostienen la estimaciÃ³n?


### 3. SeÃ±al estadÃ­stica frente a relevancia clÃ­nica

Un valor de *p* menor que 0,05 no es sinÃ³nimo de â€œimpacto clÃ­nicoâ€.

Es frecuente confundir significaciÃ³n estadÃ­stica con relevancia clÃ­nica: se detecta un efecto, pero no se responde si ese efecto cambia decisiones. Por eso es preferible reportar magnitudes interpretables (por ejemplo, diferencias absolutas de riesgo, cocientes de riesgos, NNT) y contextualizar el balance entre beneficios, riesgos y cargas.

AdemÃ¡s, cuando hay una proporciÃ³n importante de datos faltantes (sobre todo si estÃ¡ desbalanceada entre brazos), la conclusiÃ³n puede volverse muy dependiente de supuestos. En ese escenario, los anÃ¡lisis de sensibilidad dejan de ser un â€œextraâ€ y se convierten en parte esencial de la evidencia.


---

## AnatomÃ­a del Frankenstein: etapas crÃ­ticas


- **PlanificaciÃ³n (la ilusiÃ³n):** el protocolo promete un estimando (por ejemplo, bajo una estrategia tipo â€œpolÃ­tica de tratamientoâ€), pero no define con precisiÃ³n quÃ© ocurrirÃ¡ ante sucesos intercurrentes y faltantes.  
- **EjecuciÃ³n (el caos):** pÃ©rdidas de seguimiento y faltantes cuyo mecanismo no es aleatorio distorsionan lo que realmente se observa.  
- **PresentaciÃ³n (la confusiÃ³n):** el informe declara â€œÃ©xitoâ€ sin mostrar con claridad la distancia entre lo planificado y lo efectivamente estimado.

**RecomendaciÃ³n prÃ¡ctica:** audita la robustez con anÃ¡lisis de sensibilidad. Modifica supuestos plausibles sobre datos faltantes y sucesos intercurrentes y verifica si la conclusiÃ³n se mantiene.


---

## Caso prÃ¡ctico en R: faltantes que revelan el monstruo

Simulamos un ensayo con desenlace binario (Ã©xito/fracaso):

- 60 % de Ã©xito en el grupo control,  
- 65 % de Ã©xito en el grupo de tratamiento.

Introducimos **datos faltantes no al azar**: en el grupo de tratamiento, quienes no responden tienen mayor probabilidad de abandonar. El objetivo no es recomendar mÃ©todos, sino mostrar cÃ³mo decisiones analÃ­ticas distintas generan historias distintas si no se explicitan supuestos.



|estrategia           | p_control| p_trat|  diff|
|:--------------------|---------:|------:|-----:|
|Solo casos completos |      0.63|   0.75|  0.12|
|ImputaciÃ³n simplista |      0.67|   0.80|  0.13|
|Escenario pesimista  |      0.67|   0.60| -0.07|

---

### Un grÃ¡fico, tres narrativas


<img src="{{< blogdown/postref >}}index_files/figure-html/grafico_faltantes-1.png" width="672" />

Con el mismo conjunto de datos, el anÃ¡lisis â€œsolo casos completosâ€ puede exagerar el beneficio, mientras que un escenario pesimista puede invertirlo. El punto central es este: **cada forma de manejar los faltantes introduce supuestos** y, en la prÃ¡ctica, puede acercarte o alejarte de la pregunta clÃ­nica original.


---


## Lista de verificaciÃ³n: cÃ³mo cazar al monstruo

1. **Â¿Coincide la pregunta clÃ­nica con el efecto objetivo (estimando) y con lo que realmente se estima en el anÃ¡lisis?**  
2. **Â¿Las etiquetas (por ejemplo, Â«ITTÂ») estÃ¡n definidas con reglas operativas claras y conteos verificables?**  
3. **Â¿El manejo de sucesos intercurrentes estÃ¡ especificado (y no â€œresueltoâ€ a posteriori)?**  
4. **Â¿Hay anÃ¡lisis de sensibilidad para datos faltantes y supuestos crÃ­ticos?**  
5. **Â¿Se reporta relevancia clÃ­nica (por ejemplo, DMCI, diferencias absolutas, NNT) ademÃ¡s de significaciÃ³n estadÃ­stica?**  
6. **Â¿La conclusiÃ³n depende de decisiones analÃ­ticas poco transparentes presentadas como detalles menores?**

Si la respuesta es â€œnoâ€ en dos o mÃ¡s, no basta con â€œmejorar el reporteâ€: hay que replantear el diseÃ±o o, al menos, el marco de inferencia.


---

## ConclusiÃ³n: domar al Frankenstein para producir evidencia robusta

La evidencia clÃ­nica rara vez colapsa por errores burdos. MÃ¡s a menudo se debilita por incoherencias elegantes: un protocolo que promete una cosa, una ejecuciÃ³n que fuerza concesiones y un informe que no explicita cÃ³mo esas concesiones cambiaron la pregunta.

Un buen antÃ­doto combina:

- alineaciÃ³n rigurosa entre pregunta clÃ­nica, estimando y anÃ¡lisis;  
- transparencia sobre datos faltantes y sucesos intercurrentes;  
- un cambio de enfoque: de â€œestadÃ­stica como finâ€ a â€œestadÃ­stica como herramienta para decidirâ€.

Leer ICH E9(R1), hacer pilotos internos y entrenar a los equipos en trazabilidad inferencial ayuda a que el ensayo responda, de verdad, la pregunta que importa.

Â¿Tu estudio resiste esta auditorÃ­a? Si no, mejor detectarlo hoy que defenderlo maÃ±ana.
---

## Cierre: cuÃ©ntame tu caso (sin datos sensibles)

Si trabajas con protocolos, SAP o CSR, me interesa un ejemplo real (anÃ³nimo) para futuros posts.

Copia y pega esto en comentarios y completa lo que puedas:

- **Tipo de estudio / Ã¡rea terapÃ©utica:** ___  
- **Variable principal:** ___  
- **Pregunta clÃ­nica (1 frase):** ___  
- **Estimando que se estÃ¡ usando (o el que deberÃ­a usarse):** ___  
- **Suceso intercurrente que mÃ¡s te preocupa** (abandono, rescate, cambio de tratamiento, etc.): ___  
- **% de datos faltantes** (observado o esperado): ___  
- **DÃ³nde â€œnace el Frankensteinâ€ en tu caso:** (diseÃ±o / ejecuciÃ³n / anÃ¡lisis / reporte) ___  
- **QuÃ© sensibilidad te gustarÃ­a ver sÃ­ o sÃ­:** ___  

ResponderÃ© con una sugerencia concreta de sensibilidad o de redacciÃ³n del estimando (sin asesorÃ­a clÃ­nica individual, solo enfoque metodolÃ³gico).


## SuscripciÃ³n: 

[**SuscrÃ­bete a bioestadÃ­sticaedu**]({{< relref "/subscribe/" >}}) y recibe directamente en tu bandeja de entrada:

- Caja de Herramientas Antiâ€‘Frankenstein


<!-- Protocolos impecables. AnÃ¡lisis correctos. Conclusiones equivocadas. -->

<!-- En ensayos clÃ­nicos, el problema no siempre estÃ¡ en los nÃºmeros. -->
<!-- A veces estÃ¡ en la pregunta que realmente se terminÃ³ respondiendo. -->

<!-- Es posible tener: -->
<!-- â€“ un protocolo bien escrito, -->
<!-- â€“ un SAP formalmente correcto, -->
<!-- â€“ valores de p convincentes, -->

<!-- y aun asÃ­ generar evidencia frÃ¡gil para la toma de decisiones. -->

<!-- A este fenÃ³meno lo llamo el Frankenstein metodolÃ³gico: -->
<!-- piezas tÃ©cnicamente vÃ¡lidas que, al ensamblarse, estiman un efecto distinto del que la pregunta clÃ­nica sugerÃ­a. -->

<!-- No es fraude. -->
<!-- No suele ser mala fe. -->
<!-- Es una desalineaciÃ³n inferencial entre intenciÃ³n clÃ­nica, estimando y anÃ¡lisis. -->

<!-- En el blog muestro: -->
<!-- â€“ por quÃ© ocurre (incluso en equipos experimentados), -->
<!-- â€“ cÃ³mo las etiquetas clÃ¡sicas (â€œITTâ€, â€œanÃ¡lisis principalâ€) pueden ocultar supuestos crÃ­ticos, -->
<!-- â€“ y un ejemplo en R donde el mismo conjunto de datos cuenta tres historias distintas segÃºn cÃ³mo se manejen los faltantes. -->

<!-- ğŸ“Œ Si tu conclusiÃ³n depende mÃ¡s de decisiones analÃ­ticas implÃ­citas que de la pregunta clÃ­nica original, el problema no es de reporte: es de diseÃ±o inferencial. -->

<!-- ğŸ‘‰ El anÃ¡lisis completo estÃ¡ aquÃ­: -->
<!-- [https://bioestadisticaedu.com/post/er-01/] -->

<!-- Â¿Tu estudio resistirÃ­a una auditorÃ­a de coherencia entre pregunta, estimando y anÃ¡lisis? -->
