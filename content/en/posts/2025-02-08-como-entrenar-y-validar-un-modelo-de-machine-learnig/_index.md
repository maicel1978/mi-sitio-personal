---
title: "C√≥mo entrenar y validar un modelo de predicci√≥n cl√≠nica"
subtitle: "Tutorial para el desarrollo de un modelo de predicci√≥n cl√≠nica con ejemplos en R" 
summary: "Gu√≠a pr√°ctica con R para desarrollar modelos predictivos robustos en entornos cl√≠nicos"
authors: ["admin"]
date: "2024-02-08"
categories: ["C√≥digo Pr√°ctico en R o Python"]
tags: 
  - "entrenar modelo en R para predicciones cl√≠nicas"
  - "tutorial en r para modelos predictivos m√©dicos"
  - "gu√≠a Steyerberg para modelos predictivos m√©dicos"
  - "evitar overfitting en validaci√≥n de ML"
  - "paso a paso R para machine learning en tesis"
slug: como-entrenar-y-validar-un-modelo-de-machine-learnig
featured: true  # Destacar en la p√°gina principal
languages:
  es: "/es/como-entrenar-y-validar-un-modelo-de-machine-learnig"
  en: "/en/como-entrenar-y-validar-un-modelo-de-machine-learnig"
related_posts:
  - "eda"  # Slug de tu post sobre EDA, luego a√±adir categoria superior para cluster
commentable: yes
draft: false
---


![](featured.png)


# Introducci√≥n

Si usted es Investigador en Salud o, peor a√∫n, estudiante de posgrado, maestr√≠a o doctorado en el campo de las ciencias biom√©dicas... **¬°bienvenido al club del estr√©s!**

Seguro que usted est√° enfocado en desarrollar esos geniales **modelos de predicci√≥n cl√≠nica** para el **diagn√≥stico** o el **pron√≥stico**. Y es casi seguro que ha llegado a esa fase donde la metodolog√≠a se siente como un muro de ladrillos. Un d√≠a est√°s bien, y al siguiente te das cuenta de que no sabes qu√© pasos seguir, qu√© software usar, o lo peor: si tu modelo servir√° para algo m√°s all√° de llenar un repositorio de tesis. Lo digo sin tapujos: esa frase lapidaria de **'Ese valor tan alto de la Curva Roc y la ausencia de m√©tricas de calibraci√≥n'** puede significar que tu modelo tiene el famoso **s√≠ndrome del "sobreajuste"**. En otras palabras, funciona espectacularmente bien... solo para la muestra que lo cre√≥.

Es una locura, ¬øverdad? Mientras las tesis de alto nivel giran en torno a crear estos modelos de predicci√≥n cl√≠nica, los cursos de Bioestad√≠stica (¬°incluida la especialidad!) pasan de puntillas por el tema. Hay una brecha gigantesca, y la ansiedad de tener que aprenderlo todo YA es real. Pero respira. Este post es tu chaleco salvavidas: tu Traductor Metodol√≥gico no oficial y (casi) libre de l√°grimas. Yo mismo pas√© por ese infierno metodol√≥gico en mi tesis de doctorado, as√≠ que no solo doy consejos: comparto la experiencia de quien ya quem√≥ sus pesta√±as por ti.

Olvida el c√≥digo complejo y enrevesado. Aqu√≠ nos enfocamos en la l√≥gica esencial para construir un modelo predictivo robusto y fiable. Desglosaremos el proceso paso a paso, quit√°ndole el miedo a la estad√≠stica con un **C√≥digo Pr√°ctico en R**. Empezaremos con la **Estrategia de Modelado**; el c√≥digo (y opciones para quienes prefieren SPSS) vendr√° despu√©s. **¬°Empecemos!**

# Estrategia de modelado

Contar con una estrategia de modelado adecuada es esencial para desarrollar y validar modelos de predicci√≥n. En este art√≠culo exploraremos las siete etapas clave propuestas por Ewout Steyerberg en su trabajo (1).

## 1. Definici√≥n del problema e inspecci√≥n de datos 

El primer paso en cualquier proyecto de modelado es **definir claramente el problema de investigaci√≥n** y seleccionar la **variable de resultado** adecuada.

{{% callout note %}} La **variable de resultado** debe definirse con precisi√≥n: especifica **qu√© evento se predice**, **c√≥mo y cu√°ndo se mide**, y el **horizonte temporal de predicci√≥n** (por ejemplo, mortalidad a 30 d√≠as). Indica adem√°s el m√©todo de evaluaci√≥n y si hubo **cegamiento** respecto a los predictores, para garantizar coherencia y validez del modelo. {{% /callout %}}

Durante esta fase, tambi√©n realizamos un an√°lisis exploratorio de datos (EDA) para comprender las caracter√≠sticas de las variables y detectar posibles problemas, como **datos at√≠picos** o **valores faltantes**.


``` r
# Realiza una descripci√≥n general del conjunto de datos
# Instala y Carga de Librer√≠as √∫tiles 
library(caret)
library(MLDataR) # para utilizar la biblioteca diabetes_data
library(dplyr)
library(dlookr) # para EDA
library(predtools)
# Cargar el conjunto de datos
data("gusto")
gusto <- gusto
# EDA
descripcion <- overview(gusto)
summary(descripcion) # descripci√≥n general del conjunto de datos
```

<!-- Para conocer m√°s detalles sobre el proceso de ¬®Exploratory Data Analysis (EDA)¬® [ver la publicaci√≥n dedicada a este tema](es/post/2025-02-11-eda/_index.Rmd) -->

## 2. Codificaci√≥n de las variables predictoras 

La **codificaci√≥n** adecuada de las **variables predictoras** es fundamental para construir modelos robustos.

Es probable que debas utilizar t√©cnicas como la agrupaci√≥n de categor√≠as poco frecuentes y la creaci√≥n de predictores res√∫menes para simplificar informaci√≥n correlacionada. Si el algoritmo seleccionado es la **regresi√≥n log√≠stica**, es posible que le haga falta aplicar t√©cnicas como los **splines c√∫bicos restringidos** para relajar el supuesto de linealidad entre la variable dependiente y el resultado.

{{% callout note %}} Cada variable predictora debe definirse con su m√©todo de medici√≥n y momento. Las variables continuas deben reportarse con sus unidades; si se categorizan, debe justificarse los puntos de corte. Las variables categ√≥ricas deben listar todas sus categor√≠as y especificar la categor√≠a de referencia. El modelo final debe presentar la codificaci√≥n completa utilizada para cada predictor. {{% /callout %}}

üí° **Tip:** Dicotomizar predictores cuantitativos (por ejemplo, transformar una variable continua como la edad o la presi√≥n arterial en una variable binaria, como ‚Äú‚â•65 a√±os = 1‚Äù y ‚Äú\<65 = 0‚Äù) es considerada una mala pr√°ctica metodol√≥gica por p√©rdida de informaci√≥n, menos poder estad√≠stico, puntos de corte arbitrarios y riesgo de sobreajuste.

## 3 .Especificaci√≥n del tipo de modelo

En esta etapa se define la estructura formal del modelo, lo que incluye el tipo de relaci√≥n entre variables (p. ej., lineal, no lineal) y, de manera crucial, la selecci√≥n de los predictores finales que lo integrar√°n.

![](modelos_elecion.png)

La elecci√≥n de predictores no debe basarse en la aplicaci√≥n mec√°nica de m√©todos algor√≠tmicos como la **Regresi√≥n Paso a Paso (RPP)**, ya que suelen producir modelos inestables y sobreajustados, especialmente en contextos biom√©dicos y sociales. En su lugar, se recomienda:

-   Priorizar el **juicio cl√≠nico, la revisi√≥n sistem√°tica de la literatura y la experiencia previa.**

-   Evitar la preselecci√≥n de variables basada √∫nicamente en valores *p* de an√°lisis bivariados.

-   Optar por un conjunto reducido de predictores cl√≠nicamente relevantes definidos a priori, o incluir todos los candidatos en el modelo multivariable inicial sin filtrado estad√≠stico previo.

## 4. Estimaci√≥n del Modelo 

Una vez especificado el modelo (es decir, definidos los predictores y la estructura funcional), el paso de estimaci√≥n tiene como objetivo calcular los coeficientes o par√°metros que mejor se ajusten a los datos de entrenamiento.

En modelos de regresi√≥n, esto implica t√≠picamente el uso de m√©todos como la **m√°xima verosimilitud**. Sin embargo, cuando el n√∫mero de eventos es limitado o el de predictores es alto, el riesgo de sobreajuste es elevado, lo que genera predicciones extremas y poco generalizables. Para mitigarlo, se emplean t√©cnicas de **regularizaci√≥n, penalizaci√≥n o shrinkage**, que ajustan los coeficientes hacia cero para mejorar la estabilidad y la calibraci√≥n en nuevas poblaciones.

El objetivo final no es maximizar el rendimiento aparente en la muestra de desarrollo, sino obtener un modelo con **predicciones estables, bien calibradas y cl√≠nicamente √∫tiles**.

{{% callout note %}} Es recomendable aplicar siempre una validaci√≥n interna (como bootstrapping o cross-validation) para cuantificar y corregir el optimismo en las m√©tricas de rendimiento. La ecuaci√≥n final del modelo debe presentarse de forma completa ‚Äîincluyendo todos los coeficientes, el intercepto y, si corresponde, la supervivencia basal‚Äî, reportando m√©tricas de calibraci√≥n y discriminaci√≥n con sus intervalos de confianza. {{% /callout %}}

## 5. Evaluaci√≥n del Rendimiento del Modelo 

Una vez desarrollado el modelo, es esencial cuantificar su capacidad predictiva antes de su validaci√≥n. Esta evaluaci√≥n se centra en tres aspectos clave:

-   **Calibraci√≥n:** Mide la concordancia entre las probabilidades predichas y las observadas. Por ejemplo, ¬øun 10% de riesgo predicho se corresponde con un 10% de eventos observados? Se eval√∫a visualmente con curvas de calibraci√≥n y cuantitativamente con par√°metros como el intercepto (A, calibraci√≥n-in-the-large) y la pendiente de calibraci√≥n (B).

-   **Discriminaci√≥n:** Eval√∫a la capacidad del modelo para distinguir entre pacientes que experimentan el evento y aquellos que no. La m√©trica m√°s com√∫n es el estad√≠stico C (o AUC-ROC), que representa la probabilidad de que un paciente con el evento tenga una puntuaci√≥n de riesgo m√°s alta que uno sin √©l.

-   **Utilidad Cl√≠nica:** Determina si el modelo es √∫til para la toma de decisiones. El an√°lisis de curvas de decisi√≥n y el Beneficio Neto (NB) permiten evaluar si el uso del modelo conduce a mejores resultados cl√≠nicos netos en comparaci√≥n con estrategias alternativas (como tratar a todos o a ninguno).

## 6. Evaluaci√≥n de la Validez del Modelo 

La evaluaci√≥n del rendimiento en los datos de desarrollo suele ser optimista. Por ello, es crucial evaluar la validez del modelo en datos no utilizados para su construcci√≥n, un proceso que se divide en:

**Validaci√≥n Interna:** Eval√∫a la reproducibilidad del modelo, es decir, su rendimiento en m√∫ltiples muestras de la misma poblaci√≥n subyacente. T√©cnicas como el bootstrapping o la validaci√≥n cruzada son superiores a la divisi√≥n simple de la muestra, ya que cuantifican y corrigen el optimismo en las m√©tricas de rendimiento sin reducir el tama√±o de la muestra de desarrollo.

**Validaci√≥n Externa:** Es la prueba definitiva de la generalizabilidad o transportabilidad del modelo. Consiste en aplicar el modelo a una poblaci√≥n completamente independiente, lo que puede incluir:

-   **Validaci√≥n Temporal:** Usar pacientes reclutados en un per√≠odo posterior.

-   **Validaci√≥n Geogr√°fica:** Aplicar el modelo en pacientes de otros centros u hospitales.

-   **Validaci√≥n Fuerte:** Probar el modelo en un entorno cl√≠nico o una poblaci√≥n con caracter√≠sticas diferentes a las originales.

Este paso es fundamental para confirmar que el modelo mantiene su calibraci√≥n, discriminaci√≥n y utilidad cl√≠nica en la pr√°ctica real.

## 7. Presentaci√≥n del modelo 

La presentaci√≥n efectiva es crucial para la adopci√≥n cl√≠nica. Un modelo perfecto es in√∫til si los m√©dicos no pueden interpretarlo f√°cilmente. Considera:

- **Nomogramas**: Ideales para uso r√°pido en consulta
- **Aplicaciones web/m√≥viles**: Para integraci√≥n en flujos de trabajo cl√≠nicos
- **Puntuaciones de riesgo**: Simplificadas para triaje r√°pido
- **Documentaci√≥n clara**: Incluyendo limitaciones y casos de uso

Recuerda: la transparencia en la presentaci√≥n favorece la confianza cl√≠nica.

¬øHas aplicado estas t√©cnicas en tus proyectos de Machine Learning? ¬øQu√© estrategias usas para entrenar y validar tus modelos? D√©jame tus comentarios üí¨: comparte tus experiencias, dificultades o tips contigo. ¬°Juntos podemos enriquecer este conocimiento!

# Recursos adicionales

üéß **Escucha el podcast de esta publicaci√≥n**

{{< audio src="https://ia600607.us.archive.org/24/items/articulo-steyember/_Articulo_Steyember.mp3" controls="yes" >}}


# Bibliograf√≠a

1.  Steyerberg EW, Vergouwe Y. Towards better clinical prediction models: seven steps for development and an ABCD for validation. European Heart Journal [Internet]. 1 de agosto de 2014 [citado 9 de mayo de 2021];35(29):1925-31. Disponible en: <https://academic.oup.com/eurheartj/article-lookup/doi/10.1093/eurheartj/ehu207>
