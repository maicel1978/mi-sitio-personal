<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog | Bioestad√≠stica edu</title><link>https://maicel.netlify.app/post/</link><atom:link href="https://maicel.netlify.app/post/index.xml" rel="self" type="application/rss+xml"/><description>Blog</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>es-es</language><lastBuildDate>Wed, 24 Sep 2025 00:00:00 +0000</lastBuildDate><image><url>https://maicel.netlify.app/media/icon_hu_551fbaee136b383e.png</url><title>Blog</title><link>https://maicel.netlify.app/post/</link></image><item><title>Las trampas de la correlaci√≥n disfrazada de causalidad</title><link>https://maicel.netlify.app/post/trampas-correlacion/</link><pubDate>Wed, 24 Sep 2025 00:00:00 +0000</pubDate><guid>https://maicel.netlify.app/post/trampas-correlacion/</guid><description>&lt;p>El ojo humano ama los patrones: ver dos l√≠neas que se mueven juntas y concluir que una provoca la otra. La estad√≠stica, mal interpretada, a veces alimenta esa ilusi√≥n. La correlaci√≥n es apenas la danza conjunta de dos variables, no una flecha de causa. Y, sin embargo, titulares, pol√≠ticas y hasta decisiones m√©dicas se sostienen sobre esta trampa.&lt;/p>
&lt;h1 id="1-correlaciones-curiosas-pero-falsas">1) Correlaciones curiosas (pero falsas)&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Helados y ahogamientos.&lt;/strong> En verano, ambos aumentan. No porque el helado mate, sino porque el calor atrae ba√±istas y heladeros.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Cig√ºe√±as y natalidad.&lt;/strong> En pueblos europeos, donde hay m√°s cig√ºe√±as, tambi√©n hay m√°s nacimientos‚Ä¶ simplemente porque se trata de √°reas rurales m√°s f√©rtiles, no porque las aves traigan beb√©s.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Pel√≠culas de Nicolas Cage y ahogamientos en piscinas.&lt;/strong> Ejemplo cl√°sico de correlaciones espurias recopiladas por Tyler Vigen: c√≥mico, pero ilustrativo.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="2-la-correlaci√≥n-y-su-impacto-en-la-toma-de-decisiones">2) La correlaci√≥n y su impacto en la toma de decisiones&lt;/h2>
&lt;p>La confusi√≥n entre causalidad y correlaci√≥n no es solo un chiste; tiene consecuencias graves.&lt;/p>
&lt;p>&lt;strong>Pol√≠tica p√∫blica:&lt;/strong> Un estudio muestra que los pa√≠ses con m√°s m√©dicos per c√°pita tienen m√°s diagn√≥sticos de c√°ncer. Conclusi√≥n errada: ‚Äúlos m√©dicos causan c√°ncer‚Äù. Realidad: mayor densidad m√©dica implica mejor detecci√≥n.&lt;/p>
&lt;p>&lt;strong>Falacia de la causa inversa:&lt;/strong> Ni√±os con bajo rendimiento escolar pasan m√°s horas frente a la televisi√≥n. ¬øLa TV los perjudica? ¬øO los ni√±os con dificultades recurren m√°s a ella? La direcci√≥n de la causalidad puede invertirse f√°cilmente.&lt;/p>
&lt;hr>
&lt;h1 id="3-qu√©-mide-realmente-la-correlaci√≥n">3) &lt;strong>¬øQu√© mide realmente la correlaci√≥n?&lt;/strong>&lt;/h1>
&lt;ul>
&lt;li>El &lt;strong>coeficiente de correlaci√≥n (r)&lt;/strong> mide la fuerza y direcci√≥n de la relaci√≥n entre dos variables.&lt;/li>
&lt;li>Sus valores van de &lt;strong>-1 a +1&lt;/strong>:
&lt;ul>
&lt;li>+1 ‚Üí relaci√≥n positiva perfecta&lt;/li>
&lt;li>-1 ‚Üí relaci√≥n negativa perfecta&lt;/li>
&lt;li>0 ‚Üí ausencia de relaci√≥n lineal&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900">
&lt;span class="pr-3 pt-1 text-primary-600 dark:text-primary-300">
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/>&lt;/svg>
&lt;/span>
&lt;span class="dark:text-neutral-300">&lt;strong>Advertencia:&lt;/strong> un r alto no significa causalidad. Puede deberse a &lt;strong>confusores&lt;/strong>, &lt;strong>azar&lt;/strong> o &lt;strong>causalidad inversa&lt;/strong>.&lt;/span>
&lt;/div>
&lt;hr>
&lt;h1 id="4-tipos-de-coeficientes-de-correlaci√≥n-m√°s-all√°-de-pearson">4) Tipos de coeficientes de correlaci√≥n (m√°s all√° de Pearson)&lt;/h1>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="" srcset="
/post/trampas-correlacion/correlacion_hu_c614a73ad03379b6.webp 400w,
/post/trampas-correlacion/correlacion_hu_e6f9def3a13f9eea.webp 760w,
/post/trampas-correlacion/correlacion_hu_dedc904516865376.webp 1200w"
src="https://maicel.netlify.app/post/trampas-correlacion/correlacion_hu_c614a73ad03379b6.webp"
width="507"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;hr>
&lt;h1 id="5-causalidad-un-desaf√≠o-que-exige-rigurosidad">5) &lt;strong>Causalidad:&lt;/strong> un desaf√≠o que exige rigurosidad&lt;/h1>
&lt;p>Identificar la causalidad no se improvisa. Requiere algo m√°s que una simple correlaci√≥n. Exige un dise√±o experimental riguroso, criterios de Bradford Hill y construcci√≥n de modelos causales. La estad√≠stica sugiere, pero no prueba por s√≠ sola.&lt;/p>
&lt;hr>
&lt;h1 id="6-met√°fora-para-recordar">6) Met√°fora para recordar&lt;/h1>
&lt;p>Piensa en la correlaci√≥n como ver dos hojas que caen juntas en oto√±o. Creer que una arrastra a la otra es ignorar el viento invisible que las mueve a ambas.&lt;/p>
&lt;hr>
&lt;h1 id="7-checklist-para-evitar-caer-en-la-trampa">7) Checklist para evitar caer en la trampa&lt;/h1>
&lt;ol>
&lt;li>¬øExiste una variable oculta (confusor) que explique la relaci√≥n? ‚úî&lt;/li>
&lt;li>¬øPodr√≠a la causalidad ir en sentido contrario? ‚úî&lt;/li>
&lt;li>¬øEl dise√±o permite concluir causa o solo asociaci√≥n? ‚úî&lt;/li>
&lt;li>¬øHay criterios te√≥ricos/experimentales que respalden esta relaci√≥n? ‚úî&lt;/li>
&lt;li>¬øSe comunic√≥ claramente que es correlaci√≥n, no causalidad? ‚úî&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h1 id="bibliograf√≠a">Bibliograf√≠a&lt;/h1>
&lt;p>Silva Aycaguer LC. &lt;em>Cultura estad√≠stica e investigaci√≥n cient√≠fica en el campo de la salud: una mirada cr√≠tica&lt;/em>. Madrid: D√≠az de Santos; 1998.&lt;/p>
&lt;p>Pearl, J. (2009). Causality: Models, Reasoning, and Inference (2nd ed.). Cambridge University Press.&lt;/p>
&lt;p>Hern√°n, M. A., &amp;amp; Robins, J. M. (2020). Causal Inference: What If. Chapman &amp;amp; Hall/CRC. [Disponible gratis en l√≠nea].&lt;/p>
&lt;p>Hill, A. B. (1965). The environment and disease: association or causation? Proceedings of the Royal Society of Medicine, 58(5), 295‚Äì300. (Criterios de Bradford Hill).&lt;/p>
&lt;p>Freedman, D. A. (2005). Statistical Models: Theory and Practice. Cambridge University Press. (Discusi√≥n cr√≠tica sobre correlaci√≥n y causalidad).&lt;/p></description></item><item><title>Una Inmersi√≥n Intuitiva en la Arquitectura de los LLMs</title><link>https://maicel.netlify.app/post/ia/</link><pubDate>Sat, 06 Sep 2025 00:00:00 +0000</pubDate><guid>https://maicel.netlify.app/post/ia/</guid><description>&lt;p>Todos conocemos los Modelos de Lenguaje Grandes (LLMs) como &lt;strong>ChatGPT de OpenAI, Claude de Anthropic, Gemini de Google&lt;/strong> y otros modelos similares.&lt;/p>
&lt;p>Son esos asistentes de IA con los que conversamos, que nos ayudan a escribir correos (Stanford Online 2024)electr√≥nicos, a generar ideas e incluso a codificar.&lt;/p>
&lt;p>Pero, ¬øalguna vez te has preguntado c√≥mo funcionan realmente estas herramientas?
¬øEst√°n pensando o simplemente est√°n creando una &lt;strong>magn√≠fica ilusi√≥n de razonamiento&lt;/strong>?
En este blog, te mostrar√© c√≥mo cobran vida estas maravillas tecnol√≥gicas, desde el vasto oc√©ano de datos hasta su afinada inteligencia, y exploraremos la naturaleza de esa ‚Äúinteligencia‚Äù que tanto nos asombra.&lt;/p>
&lt;p>Puedes ver la versi√≥n en v√≠deo de esta publicaci√≥n aqu√≠:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/SxIFozcvCAU?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div>
&lt;p>A continuaci√≥n, te explico algunos elementos importantes para entender los Modelos de Lenguaje Grandes (LLMs), como se construyen, c√≥mo se entrenan y predicen sus resultado.&lt;/p>
&lt;img src="fig0.png" style="width:30.0%" />
&lt;h2 id="la-anatom√≠a-de-un-llm-redes-neuronales-y-transformadores">La Anatom√≠a de un LLM: Redes Neuronales y Transformadores&lt;/h2>
&lt;p>En esencia, los LLMs son &lt;strong>redes neuronales&lt;/strong>.
Lejos de simular el cerebro humano en un sentido biol√≥gico, se basan casi universalmente en una arquitectura particular conocida como &lt;strong>Transformadores&lt;/strong>.&lt;/p>
&lt;div class="flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900">
&lt;span class="pr-3 pt-1 text-primary-600 dark:text-primary-300">
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/>&lt;/svg>
&lt;/span>
&lt;span class="dark:text-neutral-300">&lt;strong>Red neuronal artificial:&lt;/strong> &lt;em>‚ÄúUna red neuronal artificial es un sistema de procesamiento paralelo y distribuido, compuesto por unidades simples de procesamiento que tienen la propensi√≥n natural de almacenar conocimiento experimental y hacerlo disponible para su uso‚Äù&lt;/em> (Haykin, n.d.).&lt;/span>
&lt;/div>
&lt;p>Estos Transformadores fueron propuestos por Vaswani et al.¬†en 2017 y se destacaron por su capacidad para ‚Äúdibujar dependencias globales entre la entrada y la salida‚Äù utilizando √∫nicamente mecanismos de atenci√≥n, sin necesidad de redes recurrentes o convolucionales.&lt;/p>
&lt;p>Esta capacidad es clave para su √©xito: permite que el modelo procese grandes cantidades de texto en paralelo y capte relaciones a larga distancia dentro de una secuencia.(Vaswani et al., n.d.)&lt;/p>
&lt;p>Cuando hablamos de entrenar un LLM, hay varios componentes clave que entran en juego:&lt;/p>
&lt;!-- - **Arquitectura**: C√≥mo se estructura la red neuronal (los Transformadores). -->
&lt;!-- - **P√©rdida de entrenamiento y algoritmo**: C√≥mo se "aprende" el modelo. -->
&lt;!-- - **Datos**: En qu√© informaci√≥n se entrena el modelo. -->
&lt;!-- - **Evaluaci√≥n**: C√≥mo sabemos si el modelo est√° mejorando. -->
&lt;!-- - **Componentes del sistema**: C√≥mo se ejecutan estos modelos gigantes en hardware moderno de manera eficiente. -->
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="" srcset="
/post/ia/fig1_hu_18182d8f524833af.webp 400w,
/post/ia/fig1_hu_f33bba6717280fcf.webp 760w,
/post/ia/fig1_hu_4ee0477a1071e0cf.webp 1200w"
src="https://maicel.netlify.app/post/ia/fig1_hu_18182d8f524833af.webp"
width="760"
height="410"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;!-- A menudo, la academia se centra mucho en la arquitectura, pero en la pr√°ctica, lo que realmente importa es la **calidad de los datos, la evaluaci√≥n y los sistemas**, ya que las peque√±as diferencias arquitect√≥nicas son a menudo secundarias frente a la escala. -->
&lt;h2 id="la-primera-etapa-pre-entrenamiento-modelado-del-lenguaje">La Primera Etapa: Pre-entrenamiento (Modelado del Lenguaje)&lt;/h2>
&lt;p>El viaje de un LLM comienza con el &lt;strong>pre-entrenamiento&lt;/strong>, un paradigma cl√°sico donde el modelo se entrena para &lt;strong>‚Äúmodelar todo Internet‚Äù&lt;/strong>.&lt;/p>
&lt;p>En esta fase, un modelo de lenguaje es, a grandes rasgos, un modelo de &lt;strong>distribuci√≥n de probabilidad sobre secuencias de tokens o palabras&lt;/strong>.&lt;/p>
&lt;div class="flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900">
&lt;span class="pr-3 pt-1 text-primary-600 dark:text-primary-300">
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/>&lt;/svg>
&lt;/span>
&lt;span class="dark:text-neutral-300">&lt;strong>Tokens:&lt;/strong> &lt;em>‚ÄúUn token es una instancia de una secuencia de caracteres en un documento, agrupada como una unidad sem√°ntica √∫til para el procesamiento autom√°tico de texto. Esta explicaci√≥n se basa en una definici√≥n clara y rigurosa en el contexto del an√°lisis de informaci√≥n y recuperaci√≥n de documentos.‚Äù&lt;/em>&lt;/span>
&lt;/div>
&lt;p>Imagina la frase &lt;strong>‚ÄúEl rat√≥n comi√≥ el queso‚Äù&lt;/strong>.&lt;/p>
&lt;img src="fig2.png" style="width:30.0%" />
&lt;p>Un modelo de lenguaje te dar√≠a la probabilidad de que esta frase sea pronunciada por un humano o encontrada en l√≠nea.&lt;/p>
&lt;p>Si la frase tuviera errores gramaticales como &lt;strong>‚ÄúEl el rat√≥n queso‚Äù&lt;/strong>, el modelo, con su conocimiento sint√°ctico, sabr√≠a que es menos &lt;strong>probable&lt;/strong>.&lt;/p>
&lt;p>Y si fuera &lt;strong>‚ÄúEl queso comi√≥ el rat√≥n‚Äù&lt;/strong>, su conocimiento sem√°ntico le indicar√≠a que esto es &lt;strong>improbable&lt;/strong>.&lt;/p>
&lt;img src="fig3.png" style="width:30.0%" />
&lt;p>&lt;strong>Aqu√≠ es donde entra el primer matiz cr√≠tico&lt;/strong>: este &lt;strong>‚Äúconocimiento sint√°ctico y sem√°ntico‚Äù&lt;/strong> no implica que el modelo &lt;strong>entienda&lt;/strong> realmente la gram√°tica o que los quesos no comen ratones.&lt;/p>
&lt;p>M√°s bien, ha aprendido, a partir de patrones en billones de textos, que ciertas secuencias de palabras son estad√≠sticamente m√°s probables o coherentes que otras.
Es una habilidad predictiva, no una comprensi√≥n conceptual.&lt;/p>
&lt;p>Los LLMs son &lt;strong>modelos generativos&lt;/strong>.&lt;/p>
&lt;p>Esto significa que, una vez que tienen esta comprensi√≥n de las distribuciones de probabilidad, pueden &lt;strong>generar nuevas oraciones o datos&lt;/strong> simplemente muestreando de esa distribuci√≥n.&lt;/p>
&lt;div class="flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900">
&lt;span class="pr-3 pt-1 text-primary-600 dark:text-primary-300">
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/>&lt;/svg>
&lt;/span>
&lt;span class="dark:text-neutral-300">&lt;strong>Modelos generativos:&lt;/strong> &lt;em>‚ÄúSon algoritmos dise√±ados para crear datos nuevos que parecen provenir de la misma distribuci√≥n que los datos originales con los que fueron entrenados.‚Äù&lt;/em>&lt;/span>
&lt;/div>
&lt;p>Es decir, &lt;em>saben&lt;/em> c√≥mo sonar convincentes y coherentes, pero no necesariamente &lt;em>por qu√©&lt;/em> lo que dicen es correcto o verdadero.&lt;/p>
&lt;h2 id="modelos-de-lenguaje-autorregresivos-prediciendo-la-siguiente-palabra">Modelos de Lenguaje Autorregresivos: Prediciendo la Siguiente Palabra&lt;/h2>
&lt;p>Vamos a adaptar el texto que proporcionaste para que se conecte con nuestro ejemplo del rat√≥n y el queso.üêÅüßÄ&lt;/p>
&lt;h2 id="modelos-de-lenguaje-autorregresivos-prediciendo-la-siguiente-palabra-1">Modelos de Lenguaje Autorregresivos: Prediciendo la Siguiente Palabra&lt;/h2>
&lt;p>Los modelos de lenguaje m√°s modernos, como Gemini, son &lt;strong>autorregresivos&lt;/strong>.
Esto significa que predicen la &lt;strong>siguiente palabra bas√°ndose en todas las palabras que ya han visto&lt;/strong> en la secuencia.&lt;/p>
&lt;p>Piensa en ellos como un narrador que va construyendo una historia palabra por palabra.&lt;/p>
&lt;h2 id="el-proceso-con-el-rat√≥n-comi√≥-el-queso">El Proceso con &lt;strong>‚ÄúEl rat√≥n comi√≥ el queso‚Äù&lt;/strong>&lt;/h2>
&lt;p>Imaginemos que el modelo est√° generando nuestra frase, ‚ÄúEl rat√≥n comi√≥ el queso.‚Äù Este es el fascinante proceso que ocurre:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Secuencia de palabras:&lt;/strong> El modelo empieza con la primera palabra de la oraci√≥n.
Luego, toma las palabras que ya ha generado: &lt;strong>‚ÄúEl rat√≥n‚Äù&lt;/strong>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Tokenizaci√≥n:&lt;/strong> Las palabras se convierten en &lt;strong>tokens&lt;/strong> (n√∫meros o identificadores internos).
Por ejemplo, ‚ÄúEl‚Äù podr√≠a ser &lt;code>143&lt;/code>, ‚Äúrat√≥n‚Äù &lt;code>56&lt;/code>, y ‚Äúcomi√≥‚Äù &lt;code>25&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>El modelo predice:&lt;/strong> Estos tokens numerados entran en el modelo (la ‚Äúcaja negra‚Äù).
Basado en todo lo que ha aprendido de internet, el modelo calcula cu√°l es el pr√≥ximo token m√°s probable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Distribuci√≥n de probabilidad:&lt;/strong> El modelo no solo predice una palabra, sino que le asigna una &lt;strong>probabilidad a cada palabra&lt;/strong> en su vocabulario.
Por ejemplo, despu√©s de &lt;strong>‚ÄúEl rat√≥n comi√≥ el‚Äù&lt;/strong>, la palabra &lt;strong>‚Äúqueso‚Äù&lt;/strong> podr√≠a tener una probabilidad del 85%, ‚Äúpan‚Äù un 10%, y ‚Äúsemillas‚Äù un 5%.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Muestreo:&lt;/strong> El modelo elige el token con la probabilidad m√°s alta, que en este caso es el token para ‚Äúqueso‚Äù.
A veces, para no sonar rob√≥tico, el modelo elige una palabra con una probabilidad un poco menor, pero en la mayor√≠a de los casos elige la m√°s probable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Detokenizaci√≥n:&lt;/strong> El token seleccionado se convierte de nuevo en la palabra ‚Äúqueso‚Äù, completando as√≠ la frase.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="aprendizaje-del-modelo">Aprendizaje del Modelo&lt;/h2>
&lt;p>Durante el &lt;strong>entrenamiento&lt;/strong>, el modelo hace este mismo proceso, pero en lugar de generar una frase nueva, compara su predicci√≥n con la palabra real en un texto de entrenamiento.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Si el modelo predice &lt;strong>‚Äúpan‚Äù&lt;/strong> y la palabra correcta es &lt;strong>‚Äúqueso‚Äù&lt;/strong>, la &lt;strong>funci√≥n de p√©rdida de entrop√≠a cruzada&lt;/strong> le da un ‚Äúcastigo‚Äù.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ese castigo se usa para ajustar los pesos del modelo.
El objetivo es que, la pr√≥xima vez que vea un contexto similar (‚ÄúEl rat√≥n comi√≥ el‚Ä¶‚Äù), la probabilidad de que prediga ‚Äúqueso‚Äù sea mucho mayor.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>As√≠, la ‚Äúfluidez‚Äù del modelo para generar frases como ‚ÄúEl rat√≥n comi√≥ el queso‚Äù se basa en su capacidad para &lt;strong>predecir estad√≠sticamente&lt;/strong> la palabra m√°s probable en cada paso, no en un razonamiento sobre los h√°bitos alimenticios de los roedores.&lt;/p>
&lt;!-- Los modelos m√°s utilizados hoy en d√≠a son los **modelos de lenguaje autorregresivos**. -->
&lt;!-- La idea central es descomponer la probabilidad de una secuencia de palabras en un producto de probabilidades condicionales: la probabilidad de la primera palabra, multiplicada por la probabilidad de la segunda palabra dada la primera, y as√≠ sucesivamente. -->
&lt;!-- En t√©rminos m√°s simples, el modelo predice la **siguiente palabra bas√°ndose en todo lo que ha ocurrido antes** en la secuencia. -->
&lt;!-- El proceso es fascinante: -->
&lt;!-- 1. Tomas una secuencia de palabras (como "Ella probablemente prefiere"). -->
&lt;!-- 2. La **tokenizas**, es decir, la divides en "tokens" (palabras o subpalabras) y les asignas un ID. -->
&lt;!-- 3. Estos tokens pasan por el modelo (la "caja negra" del Transformador). -->
&lt;!-- 4. El modelo emite una **distribuci√≥n de probabilidad** sobre la siguiente palabra o token posible. -->
&lt;!-- 5. Se "muestrea" de esta distribuci√≥n para obtener el siguiente token m√°s probable. -->
&lt;!-- 6. Finalmente, se "detokeniza" para obtener la palabra real. -->
&lt;!-- Durante el entrenamiento, el objetivo es **predecir el token m√°s probable** y ajustar los pesos del modelo para aumentar la probabilidad de generar el token correcto, utilizando la **funci√≥n de p√©rdida de entrop√≠a cruzada (Cross-Entropy Loss)**, que es equivalente a maximizar la verosimilitud logar√≠tmica del texto. -->
&lt;!-- Esta es la base de su impresionante fluidez, pero recalca que su "razonamiento" es una sofisticada forma de predicci√≥n estad√≠stica. -->
&lt;h2 id="los-tokenizadores-el-primer-paso-crucial-para-la-coherencia">Los Tokenizadores: El Primer Paso Crucial para la ‚ÄúCoherencia‚Äù&lt;/h2>
&lt;p>Los &lt;strong>tokenizadores&lt;/strong> son componentes extremadamente importantes pero a menudo poco valorados.&lt;/p>
&lt;p>¬øPor qu√© los necesitamos?&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>M√°s generales que las palabras&lt;/strong>: Las palabras como tokens directos fallan con errores tipogr√°ficos o en idiomas que no usan espacios (como el tailand√©s).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Eficiencia de secuencia&lt;/strong>: Tokenizar car√°cter por car√°cter har√≠a las secuencias demasiado largas, lo que es ineficiente para los Transformadores (cuya complejidad crece cuadr√°ticamente con la longitud de la secuencia).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Los tokenizadores buscan encontrar &lt;strong>subsecuencias comunes&lt;/strong> y darles un token espec√≠fico.
En promedio, un token suele representar alrededor de &lt;strong>tres o cuatro letras&lt;/strong>.&lt;/p>
&lt;p>Un algoritmo muy com√∫n es la &lt;strong>Codificaci√≥n de Pares de Bytes (Byte Pair Encoding o BPE)&lt;/strong>.
Es fundamental considerar c√≥mo se tokeniza el texto, ya que el &lt;strong>tama√±o del vocabulario afecta directamente la dimensionalidad de la salida&lt;/strong> del modelo.&lt;/p>
&lt;p>&lt;strong>Un punto cr√≠tico aqu√≠&lt;/strong>: Si bien son √∫tiles, los tokenizadores tienen limitaciones, especialmente con n√∫meros (matem√°ticas) y c√≥digo.&lt;/p>
&lt;p>Por ejemplo, un n√∫mero como ‚Äú327‚Äù puede tener su propio token, lo que significa que el modelo no lo ve como una composici√≥n de ‚Äú3‚Äù, ‚Äú2‚Äù, ‚Äú7‚Äù, lo que dificulta su capacidad para razonar matem√°ticamente o con la estructura del c√≥digo.&lt;/p>
&lt;p>Esto nos recuerda que, a pesar de la fluidez, los LLMs operan sobre representaciones simb√≥licas (tokens) que no siempre se alinean con nuestra comprensi√≥n conceptual del lenguaje o las matem√°ticas.&lt;/p>
&lt;h2 id="de-modelo-de-lenguaje-a-asistente-de-ia-el-post-entrenamiento-o-la-ilusi√≥n-de-la-intencionalidad">De Modelo de Lenguaje a Asistente de IA: El Post-entrenamiento (o la Ilusi√≥n de la Intencionalidad)&lt;/h2>
&lt;p>Un modelo pre-entrenado es un experto en &lt;strong>‚Äúhablar como Internet‚Äù&lt;/strong>, pero no es un asistente de IA.&lt;/p>
&lt;p>Si le preguntaras a &lt;strong>GPT-3&lt;/strong> (un modelo puramente de lenguaje) ‚Äúexpl√≠came el aterrizaje en la luna a un ni√±o de seis a√±os‚Äù, podr√≠a responder con ‚Äúexpl√≠came la teor√≠a de la gravedad a un ni√±o de seis a√±os‚Äù porque ha aprendido que en Internet, una pregunta a menudo es seguida por preguntas similares, no por una respuesta directa.&lt;/p>
&lt;p>El &lt;strong>post-entrenamiento (alignment)&lt;/strong> es el proceso que transforma estos modelos en asistentes √∫tiles, asegur√°ndose de que &lt;strong>sigan las instrucciones de los usuarios&lt;/strong> y los deseos de los dise√±adores (por ejemplo, evitar contenido t√≥xico).&lt;/p>
&lt;p>&lt;strong>Este es el punto donde la ilusi√≥n de intencionalidad se vuelve m√°s fuerte.&lt;/strong>&lt;/p>
&lt;h2 id="1-ajuste-fino-supervisado-supervised-fine-tuning---sft">1. Ajuste Fino Supervisado (Supervised Fine-Tuning - SFT)&lt;/h2>
&lt;p>El primer paso es el &lt;strong>Ajuste Fino Supervisado (SFT)&lt;/strong>.&lt;/p>
&lt;p>Aqu√≠, el LLM pre-entrenado se afina con &lt;strong>respuestas deseadas recogidas de humanos&lt;/strong>.
Es decir, se le dan ejemplos de preguntas y sus respuestas ‚Äúcorrectas‚Äù o ‚Äúideales‚Äù escritas por humanos.&lt;/p>
&lt;p>Este paso fue crucial para el salto de &lt;strong>GPT-3&lt;/strong> a &lt;strong>ChatGPT&lt;/strong>.&lt;/p>
&lt;p>Curiosamente, no se necesita una cantidad masiva de datos para SFT; &lt;strong>unos pocos miles de ejemplos bien elegidos pueden ser suficientes&lt;/strong>.&lt;/p>
&lt;p>Esto sugiere que el SFT no ense√±a al modelo nuevo conocimiento, sino que le ense√±a &lt;strong>c√≥mo formatear las respuestas&lt;/strong> y optimizar para un ‚Äútipo de usuario‚Äù espec√≠fico que ya hab√≠a visto en sus datos de pre-entrenamiento.&lt;/p>
&lt;p>En otras palabras, el modelo ya ten√≠a el conocimiento latente; el SFT le ense√±a a &lt;em>expresarlo&lt;/em> de la manera que un asistente de IA ‚Äúdeber√≠a‚Äù hacerlo.&lt;/p>
&lt;p>No est√° aprendiendo a &lt;em>pensar&lt;/em> como un asistente, sino a &lt;em>simular&lt;/em> el comportamiento de uno.&lt;/p>
&lt;h2 id="2-aprendizaje-por-refuerzo-a-partir-de-retroalimentaci√≥n-humana-reinforcement-learning-from-human-feedback---rlhf">2. Aprendizaje por Refuerzo a partir de Retroalimentaci√≥n Humana (Reinforcement Learning from Human Feedback - RLHF)&lt;/h2>
&lt;p>El SFT tiene sus limitaciones: &lt;strong>Limitado por la habilidad humana&lt;/strong>: Los humanos pueden juzgar mejor lo que es una buena respuesta de lo que pueden escribirla ellos mismos.&lt;/p>
&lt;p>&lt;strong>Posibles alucinaciones&lt;/strong>: Como el SFT se entrena con poca data, si un humano da una respuesta que el modelo no ha visto antes (y por tanto no sabe si es cierta), el modelo puede aprender a ‚Äúinventar‚Äù informaci√≥n plausible pero falsa.&lt;/p>
&lt;p>&lt;strong>Aqu√≠ el matiz cr√≠tico es fundamental&lt;/strong>: la ‚Äúalucinaci√≥n‚Äù (generaci√≥n de informaci√≥n falsa pero plausible) es una clara evidencia de que los LLMs no ‚Äúsaben‚Äù lo que es verdad o mentira, ni tienen un sentido de la realidad.&lt;/p>
&lt;p>Simplemente generan secuencias de tokens que &lt;em>parecen&lt;/em> correctas, bas√°ndose en los patrones que han aprendido, incluso si no tienen fundamento.&lt;/p>
&lt;p>Es la culminaci√≥n de la ilusi√≥n de razonamiento.
&lt;strong>Costo&lt;/strong>: Generar respuestas ideales es muy caro.&lt;/p>
&lt;p>Aqu√≠ es donde entra el &lt;strong>RLHF&lt;/strong>.&lt;/p>
&lt;p>En lugar de simplemente clonar el comportamiento humano, el objetivo es &lt;strong>maximizar la preferencia humana&lt;/strong>.&lt;/p>
&lt;p>El proceso es el siguiente:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Para una instrucci√≥n dada, el modelo genera &lt;strong>dos respuestas diferentes&lt;/strong>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Etiquetadores humanos seleccionan &lt;strong>cu√°l de las dos respuestas es mejor&lt;/strong>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Con esta retroalimentaci√≥n, el modelo se afina para generar m√°s de las respuestas ‚Äúbuenas‚Äù y menos de las ‚Äúmalas‚Äù.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Para hacer esto, se entrena un &lt;strong>modelo de recompensa (Reward Model)&lt;/strong>, un clasificador que aprende a predecir cu√°nto prefiere un humano una respuesta sobre otra, dando una se√±al de recompensa continua.&lt;/p>
&lt;p>Posteriormente, m√©todos m√°s simples como la &lt;strong>Optimizaci√≥n Directa por Preferencia (Direct Preference Optimization - DPO)&lt;/strong> han demostrado ser igual de efectivos, evitando la complejidad del aprendizaje por refuerzo tradicional.&lt;/p>
&lt;p>En definitiva, RLHF moldea el comportamiento del LLM para alinearse con lo que &lt;em>deseamos&lt;/em> ver, no con lo que el modelo &lt;em>sabe&lt;/em> o &lt;em>piensa&lt;/em>.&lt;/p>
&lt;p>Le ense√±a a ser complaciente y a evitar lo ‚Äút√≥xico‚Äù porque los humanos as√≠ lo prefieren, no por un juicio moral inherente.&lt;/p>
&lt;h2 id="la-materia-prima-datos-masivos-y-su-filtrado">La Materia Prima: Datos Masivos y su Filtrado&lt;/h2>
&lt;p>El pre-entrenamiento de los LLMs se realiza sobre &lt;strong>‚Äútodo Internet‚Äù&lt;/strong>.&lt;/p>
&lt;p>Esto incluye vastas colecciones como Common Crawl, que contiene alrededor de &lt;strong>250 mil millones de p√°ginas web y un petabyte de datos&lt;/strong>.&lt;/p>
&lt;p>Pero el Internet es ‚Äúsucio‚Äù y no representativo.&lt;/p>
&lt;p>Imagina una p√°gina web aleatoria: llena de HTML, publicidad, fragmentos sin terminar.&lt;/p>
&lt;p>Para que estos datos sean √∫tiles, se requieren pasos de procesamiento intensivos, que incluyen:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="" srcset="
/post/ia/fig4_hu_b74c97a4d4debd70.webp 400w,
/post/ia/fig4_hu_f0bdf3ad77c25970.webp 760w,
/post/ia/fig4_hu_6e63ba7ee8334dc3.webp 1200w"
src="https://maicel.netlify.app/post/ia/fig4_hu_b74c97a4d4debd70.webp"
width="507"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;!-- 1. **Extracci√≥n de texto**: Eliminar HTML y extraer contenido. -->
&lt;!-- 2.**Filtrado de contenido indeseable**: Eliminar contenido no seguro (NSFW), da√±ino o informaci√≥n personal (PII). -->
&lt;!-- 3. **Deduplicaci√≥n**: Eliminar contenido repetido. -->
&lt;!-- 4. **Filtrado heur√≠stico**: Eliminar documentos de baja calidad bas√°ndose en reglas (por ejemplo, distribuciones de tokens inusuales, longitud extrema de palabras o documentos muy cortos/largos). -->
&lt;!-- 5. **Filtrado basado en modelos**: Entrenar un clasificador para identificar documentos de alta calidad, usando referencias de Wikipedia como punto de partida. -->
&lt;!-- 6. **Clasificaci√≥n y ponderaci√≥n por dominio**: Aumentar o disminuir el peso de ciertos dominios (por ejemplo, el c√≥digo puede mejorar el razonamiento, por lo que se le da m√°s peso). -->
&lt;!-- 7. **Entrenamiento final con datos de alta calidad**: "Sobreajustar" el modelo con datos de muy alta calidad al final del pre-entrenamiento, reduciendo la tasa de aprendizaje. -->
&lt;p>La escala de estos conjuntos de datos es asombrosa, pasando de &lt;strong>150 mil millones de tokens (800 GB)&lt;/strong> en benchmarks acad√©micos anteriores, hasta &lt;strong>15 billones de tokens&lt;/strong> para modelos de √∫ltima generaci√≥n como Llama 3 (equivalente a miles de terabytes).&lt;/p>
&lt;p>La recopilaci√≥n y curaci√≥n de datos sigue siendo un desaf√≠o enorme y un √°rea activa de investigaci√≥n.&lt;/p>
&lt;h2 id="las-leyes-de-escalado-el-poder-de-lo-grande-y-sus-implicaciones-en-la-inteligencia">Las Leyes de Escalado: El Poder de lo Grande (y sus Implicaciones en la ‚ÄúInteligencia‚Äù)&lt;/h2>
&lt;p>Uno de los descubrimientos m√°s sorprendentes en LLMs es que &lt;strong>cuantos m√°s datos se entrenen los modelos y m√°s grandes sean los modelos, mejor ser√° su rendimiento&lt;/strong>.&lt;/p>
&lt;p>A diferencia de lo que se ense√±a en muchas clases de aprendizaje autom√°tico, el ‚Äúsobreajuste‚Äù (overfitting) no parece ocurrir con los LLMs.&lt;/p>
&lt;p>Las &lt;strong>leyes de escalado&lt;/strong> nos muestran que si se aumenta la computaci√≥n, los datos o el n√∫mero de par√°metros, la p√©rdida de validaci√≥n del modelo disminuye de forma predecible y lineal en una escala logar√≠tmica.&lt;/p>
&lt;p>Esto es crucial porque permite a las compa√±√≠as predecir cu√°nto mejorar√°n sus modelos en el futuro y c√≥mo optimizar la asignaci√≥n de recursos.&lt;/p>
&lt;p>Por ejemplo, el famoso art√≠culo Chinchilla de DeepMind mostr√≥ que la relaci√≥n √≥ptima es entrenar con &lt;strong>20 tokens por cada par√°metro&lt;/strong> del modelo para maximizar la eficiencia del entrenamiento.&lt;/p>
&lt;p>&lt;strong>Un punto anal√≠tico aqu√≠&lt;/strong>: Que los modelos ‚Äúmejoren‚Äù al escalar no significa que se vuelvan intr√≠nsecamente ‚Äúm√°s inteligentes‚Äù en un sentido humano, o que est√©n m√°s cerca de la conciencia.&lt;/p>
&lt;p>Simplemente, son &lt;strong>m√°quinas de patrones incre√≠blemente sofisticadas&lt;/strong> que, con m√°s datos y m√°s capacidad computacional, son capaces de reconocer y generar patrones cada vez m√°s complejos y coherentes, reduciendo su ‚Äúp√©rdida‚Äù (es decir, volvi√©ndose mejores en la predicci√≥n del siguiente token).&lt;/p>
&lt;p>La ‚Äúinteligencia‚Äù que percibimos es una propiedad emergente de esta capacidad de predicci√≥n a gran escala, no una mente.&lt;/p>
&lt;!-- ### El Precio de la Ilusi√≥n de Inteligencia: ¬øCu√°nto Cuesta un LLM? -->
&lt;!-- Entrenar un LLM es una empresa monumental y costosa. -->
&lt;!-- Tomemos como ejemplo el modelo **Llama 3 400B**, uno de los mejores modelos de c√≥digo abierto actuales: **Tokens de entrenamiento**: 15.6 billones de tokens. -->
&lt;!-- - **Par√°metros**: 45 mil millones. -->
&lt;!-- - **C√≥mputo (FLOPs)**: Aproximadamente 3.8 x 10\^25 FLOPs. -->
&lt;!-- - **Hardware y tiempo**: Se entren√≥ en **16.000 GPUs H100** durante unos 70 d√≠as (o 26 millones de horas de GPU). -->
&lt;!-- -**Costo estimado**: El alquiler de estas GPUs costar√≠a alrededor de **52 millones de d√≥lares**, y sumando los salarios del equipo, el costo total de entrenamiento rondar√≠a los **75 millones de d√≥lares**. -->
&lt;!-- - **Huella de carbono**: El entrenamiento de Llama 3 emiti√≥ unas 4.000 toneladas de CO2 equivalente. -->
&lt;!-- Estos n√∫meros son un testimonio de la inmensa inversi√≥n necesaria para crear estos modelos capaces de generar una ilusi√≥n tan convincente. -->
&lt;h2 id="sistemas-el-cerebro-detr√°s-de-la-eficiencia">Sistemas: El Cerebro Detr√°s de la Eficiencia&lt;/h2>
&lt;p>La computaci√≥n es el cuello de botella m√°s grande en el desarrollo de LLMs.
Comprar m√°s GPUs es dif√≠cil por su alto costo y escasez, adem√°s de las limitaciones f√≠sicas en la comunicaci√≥n entre ellas.&lt;/p>
&lt;p>Es crucial optimizar c√≥mo se asignan los recursos y el pipeline de entrenamiento.&lt;/p>
&lt;p>Algunos trucos clave a nivel de sistemas incluyen: &lt;strong>Baja Precisi√≥n (Low Precision)&lt;/strong>: Usar n√∫meros de punto flotante de 16 bits en lugar de 32 bits.&lt;/p>
&lt;p>Esto reduce la cantidad de datos que deben enviarse a las GPUs, acelerando la comunicaci√≥n y disminuyendo el consumo de memoria.&lt;/p>
&lt;p>&lt;strong>Fusi√≥n de Operadores (Operator Fusion)&lt;/strong>: Las GPUs son muy lentas en la comunicaci√≥n.
La fusi√≥n de operadores combina varias operaciones consecutivas en una sola llamada al kernel, lo que significa que los datos se env√≠an a la GPU una sola vez, todas las operaciones se realizan y luego los resultados se devuelven, lo que acelera significativamente el proceso (por ejemplo, &lt;code>torch.compile&lt;/code> en PyTorch puede duplicar la velocidad).&lt;/p>
&lt;h2 id="conclusi√≥n-una-ilusi√≥n-poderosa-no-un-pensamiento-consciente">Conclusi√≥n: Una Ilusi√≥n Poderosa, No un Pensamiento Consciente&lt;/h2>
&lt;p>Desde sus cimientos como redes neuronales Transformer, pasando por el pre-entrenamiento con datos masivos de Internet y el afinamiento con retroalimentaci√≥n humana, hasta la optimizaci√≥n de sistemas y la gesti√≥n de costos astron√≥micos, la creaci√≥n de un LLM es una haza√±a de ingenier√≠a y ciencia de datos.&lt;/p>
&lt;p>La pr√≥xima vez que interact√∫es con un chatbot, recordar√°s que detr√°s de esa respuesta fluida hay billones de tokens procesados, complejos algoritmos de entrenamiento, ingeniosas t√©cnicas de afinamiento y una infraestructura computacional masiva trabajando en conjunto.
&lt;strong>Estos modelos no ‚Äúpiensan‚Äù en el sentido humano de la palabra, ni poseen conciencia o una comprensi√≥n profunda y hol√≠stica del mundo&lt;/strong>.&lt;/p>
&lt;p>Lo que hacen, y lo hacen de manera magistral, es &lt;strong>identificar y reproducir patrones estad√≠sticos&lt;/strong> en los datos con los que fueron entrenados.&lt;/p>
&lt;p>Su habilidad para generar texto coherente, relevante y a menudo sorprendentemente ‚Äúinteligente‚Äù es una testamentaci√≥n de la &lt;strong>efectividad de la predicci√≥n a escala masiva&lt;/strong>.
Es una &lt;strong>ilusi√≥n de razonamiento&lt;/strong> tan convincente que a menudo nos hace cuestionar la naturaleza de la inteligencia misma.
Y es, sin duda, una de las maravillas tecnol√≥gicas m√°s grandes de nuestro tiempo.&lt;/p>
&lt;h2 id="bibliograf√≠a">Bibliograf√≠a&lt;/h2>
&lt;div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
&lt;div id="ref-haykin" class="csl-entry">
&lt;p>Haykin, Simon. n.d. ‚ÄúNeural Networks and Learning Machines.‚Äù &lt;a href="http://dni.dali.dartmouth.edu/9umv9yghhaoq/13-dayana-hermann-1/read-0131471392-neural-networks-and-learning-machines.pdf" target="_blank" rel="noopener">http://dni.dali.dartmouth.edu/9umv9yghhaoq/13-dayana-hermann-1/read-0131471392-neural-networks-and-learning-machines.pdf&lt;/a>.&lt;/p>
&lt;/div>
&lt;div id="ref-stanfordonline2024" class="csl-entry">
&lt;p>Stanford Online. 2024. ‚ÄúStanford CS229 i Machine Learning i Building Large Language Models (LLMs),‚Äù August. &lt;a href="https://www.youtube.com/watch?v=9vM4p9NN0Ts" target="_blank" rel="noopener">https://www.youtube.com/watch?v=9vM4p9NN0Ts&lt;/a>.&lt;/p>
&lt;/div>
&lt;div id="ref-vaswani" class="csl-entry">
&lt;p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. n.d. ‚ÄúAttention Is All You Need.‚Äù &lt;a href="https://doi.org/10.48550/ARXIV.1706.03762" target="_blank" rel="noopener">https://doi.org/10.48550/ARXIV.1706.03762&lt;/a>.&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>Del Laboratorio al Mundo Real</title><link>https://maicel.netlify.app/post/generalizacion/</link><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate><guid>https://maicel.netlify.app/post/generalizacion/</guid><description>&lt;h2 id="del-laboratorio-al-mundo-real-c√≥mo-la-ciencia-cruza-el-puente-hacia-tu-vida">&lt;strong>Del Laboratorio al Mundo Real: C√≥mo la Ciencia Cruza el Puente hacia tu Vida&lt;/strong>&lt;/h2>
&lt;p>¬øAlguna vez has le√≠do un titular cient√≠fico y te has preguntado: &amp;ldquo;¬øEsto me aplica a m√≠?&amp;rdquo; o &amp;ldquo;Esto suena genial, pero ¬øes relevante para mi realidad?&amp;rdquo; Si es as√≠, has tocado el nervio de uno de los mayores desaf√≠os de la investigaci√≥n: c√≥mo saltar del estudio controlado en un laboratorio al mundo real.&lt;/p>
&lt;p>El v√≠deo, &amp;ldquo;El Estudio y el Mundo&amp;rdquo;, captura esta idea.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/sY8-YxIeGu8?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div>
&lt;p>Este blog profundiza en ese &lt;strong>puente a veces inestable&lt;/strong> que conecta la ciencia con nuestra vida diaria, explicando por qu√© no se trata de desconfiar de la ciencia, sino de entenderla mejor para usarla de la forma m√°s inteligente posible.&lt;/p>
&lt;hr>
&lt;h2 id="los-pilares-fundamentales-validez-interna-y-externa">&lt;strong>Los Pilares Fundamentales: Validez Interna y Externa&lt;/strong>&lt;/h2>
&lt;p>Para que un estudio sea realmente √∫til, sus hallazgos no solo deben ser correctos, sino tambi√©n aplicables. Aqu√≠ es donde entran en juego dos conceptos clave:&lt;/p>
&lt;p>&lt;strong>Validez Interna: ¬øSe hizo bien el estudio?&lt;/strong> Se pregunta si los resultados son fiables para el grupo de personas que realmente participaron en la investigaci√≥n. Un estudio puede ser metodol√≥gicamente impecable y tener una validez interna &amp;ldquo;de 10&amp;rdquo;.&lt;/p>
&lt;p>&lt;strong>Validez Externa: ¬øSirven estos resultados fuera del estudio?&lt;/strong> Se refiere a la capacidad de los hallazgos de un estudio para aplicarse a otros contextos, poblaciones o situaciones. Un estudio perfecto a nivel interno podr√≠a ser casi in√∫til si sus participantes son tan espec√≠ficos que sus resultados no pueden generalizarse.&lt;/p>
&lt;hr>
&lt;h2 id="generalizabilidad-y-transportabilidad-construyendo-el-puente">&lt;strong>Generalizabilidad y Transportabilidad: Construyendo el Puente&lt;/strong>&lt;/h2>
&lt;p>Existen dos estrategias principales para construir ese puente:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Generalizabilidad:&lt;/strong> Piensa en esto como una receta de cocina. Perfeccionaste una tarta en tu &amp;ldquo;cocina de prueba&amp;rdquo; y ahora vas a usar esa misma receta para alimentar a toda la familia. La poblaci√≥n del estudio es solo una parte de un grupo m√°s grande al que quieres aplicar los resultados. Por ejemplo, pasar de un estudio en California a todo Estados Unidos.&lt;/li>
&lt;li>&lt;strong>Transportabilidad:&lt;/strong> Ahora imagina que quieres adaptar esa misma receta para que funcione en otro pa√≠s, con ingredientes diferentes y un horno distinto. Aqu√≠, la poblaci√≥n del estudio es ajena a la poblaci√≥n a la que quieres aplicar los resultados. Se trata de ver si los hallazgos de un estudio en Estados Unidos funcionan en una cl√≠nica en Europa.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="por-qu√©-es-tan-dif√≠cil-construir-el-puente">&lt;strong>¬øPor Qu√© es Tan Dif√≠cil Construir el Puente?&lt;/strong>&lt;/h2>
&lt;p>La principal dificultad es que los estudios, especialmente los ensayos cl√≠nicos aleatorios (RCTs), a menudo seleccionan a los participantes con criterios muy estrictos para garantizar una alta validez interna. Esto los hace poco representativos de la poblaci√≥n general.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>El Sesgo de Selecci√≥n:&lt;/strong> Ocurre cuando las personas que participan en el estudio son diferentes de la poblaci√≥n a la que se busca aplicar los resultados. No se trata solo de que no sean &amp;ldquo;representativas&amp;rdquo;, sino de que esas diferencias afecten la efectividad del tratamiento.&lt;/li>
&lt;li>&lt;strong>La Trampa de los Confusores:&lt;/strong> Factores no controlados en el estudio pueden generar asociaciones falsas o &amp;ldquo;espurias&amp;rdquo;, ocultando la verdadera relaci√≥n entre el tratamiento y el resultado.&lt;/li>
&lt;li>&lt;strong>El Riesgo de la Subjetividad:&lt;/strong> La estad√≠stica es una herramienta, no una verdad absoluta. La elecci√≥n de variables y la interpretaci√≥n de los resultados requieren un profundo conocimiento y juicio. La subjetividad puede &amp;ldquo;disfrazarse&amp;rdquo; detr√°s de n√∫meros y algoritmos, llevando a conclusiones err√≥neas si no se razona con cuidado. Sin embargo, esta subjetividad no debe confundirse con la arbitrariedad, ya que se basa en la experiencia y el sentido com√∫n del investigador.&lt;/li>
&lt;li>&lt;strong>El Dilema del Tama√±o de Muestra:&lt;/strong> A pesar de lo que se podr√≠a pensar, la determinaci√≥n del tama√±o de muestra de un estudio es m√°s un arte que una ciencia exacta, basada en el juicio y la intuici√≥n, no solo en una f√≥rmula.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="herramientas-para-construir-un-puente-fuerte">&lt;strong>Herramientas para Construir un Puente Fuerte&lt;/strong>&lt;/h2>
&lt;p>Los estad√≠sticos se dedican a medir la distancia entre el mundo del estudio y el mundo real. Una vez diagnosticado el problema, hay varias estrategias y herramientas para corregir el rumbo:&lt;/p>
&lt;p>&lt;strong>En la Fase de Dise√±o:&lt;/strong>&lt;/p>
&lt;p>La forma ideal es tomar una muestra aleatoria de la poblaci√≥n objetivo, pero esto no siempre es posible. Las alternativas incluyen:&lt;/p>
&lt;p>&lt;strong>Ensayos cl√≠nicos pragm√°ticos:&lt;/strong> Dise√±ados para imitar la pr√°ctica cl√≠nica real y ser m√°s aplicables.&lt;/p>
&lt;p>&lt;strong>Muestreo:&lt;/strong> Seleccionar participantes para asegurar que haya una amplia representaci√≥n o heterogeneidad.&lt;/p>
&lt;p>&lt;strong>Despu√©s de la Recolecci√≥n de Datos (M√©todos Anal√≠ticos):&lt;/strong>&lt;/p>
&lt;p>Si el estudio ya se realiz√≥, se usan m√©todos estad√≠sticos para ajustar los resultados:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Ponderaci√≥n (Weighting):&lt;/strong> Le da m√°s &amp;ldquo;peso&amp;rdquo; a ciertos individuos del estudio para que la muestra se parezca m√°s a la poblaci√≥n real. Es como darles un meg√°fono a las voces que necesitan ser escuchadas.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Regresiones de Resultado:&lt;/strong> Se construyen modelos matem√°ticos para predecir lo que habr√≠a pasado si la gente del mundo real hubiera recibido el tratamiento del estudio.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Enfoques Doblemente Robustos:&lt;/strong> Combinan varias t√©cnicas para obtener resultados m√°s s√≥lidos.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="la-sabidur√≠a-detr√°s-de-los-n√∫meros">&lt;strong>La Sabidur√≠a Detr√°s de los N√∫meros&lt;/strong>&lt;/h2>
&lt;p>La estad√≠stica es una herramienta. Hay un poder inmenso en dominarla, pero tambi√©n un riesgo si se aplica sin un pensamiento cr√≠tico. El peligro de la &amp;ldquo;f√°brica de publicaciones&amp;rdquo; que prioriza la significaci√≥n estad√≠stica (un valor p bajo) a veces eclipsa la relevancia cl√≠nica o biol√≥gica de los hallazgos. No basta con que un resultado sea &amp;ldquo;estad√≠sticamente significativo&amp;rdquo;. Lo realmente importante es si es significativo para la vida de las personas.&lt;/p>
&lt;p>Como se ha destacado, &amp;ldquo;pensar sin observar es un error, pero observar sin pensar es igualmente peligroso.&amp;rdquo; La aplicaci√≥n mec√°nica de un m√©todo poderoso puede llevar a conclusiones err√≥neas.&lt;/p>
&lt;h2 id="tu-papel-como-consumidor-de-ciencia">&lt;strong>Tu Papel como Consumidor de Ciencia&lt;/strong>&lt;/h2>
&lt;p>La pr√≥xima vez que veas un titular impactante, haz la pregunta clave : &lt;strong>&amp;quot;¬øEn qui√©n se hizo este estudio?&amp;quot;&lt;/strong>&lt;/p>
&lt;p>Al entender la generalizabilidad y la transportabilidad, te conviertes en un consumidor de ciencia m√°s cr√≠tico e informado. La verdadera meta no es solo que los estudios sean rigurosos, sino que su conocimiento pueda cruzar ese puente de forma segura y tener un impacto genuino en nuestras vidas. Porque, al final, una ciencia que no puede ser aplicada al mundo real pierde gran parte de su valor.&lt;/p>
&lt;h2 id="biblograf√≠a">Biblograf√≠a&lt;/h2>
&lt;ol>
&lt;li>Degtiar I, Rose S. A Review of Generalizability and Transportability. Annual Review of Statistics and Its Application [Internet]. 9 de marzo de 2023 [citado 1 de septiembre de 2025];10(Volume 10, 2023):501-24. Disponible en: &lt;a href="https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-042522-103837" target="_blank" rel="noopener">https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-042522-103837&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Desvelando la L√≥gica Matem√°tica Detr√°s de Causa y Efecto</title><link>https://maicel.netlify.app/post/m-s-all-del-rct-en-salud-p-blica-y-epidemiolog-a/</link><pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate><guid>https://maicel.netlify.app/post/m-s-all-del-rct-en-salud-p-blica-y-epidemiolog-a/</guid><description>&lt;h1 id="inferencia-causal-">Inferencia Causal :&lt;/h1>
&lt;p>El &lt;strong>marco de resultados potenciales&lt;/strong> se mantiene como la piedra angular del pensamiento causal, proporcionando el andamiaje conceptual para diferenciar la mera correlaci√≥n de la causalidad. Ante la imposibilidad de llevar a cabo &lt;strong>ensayos controlados aleatorizados (RCT)&lt;/strong>, la investigaci√≥n se ha nutrido de &lt;strong>m√©todos robustos&lt;/strong> que permiten extraer inferencias causales cre√≠bles de datos observacionales. En este contexto, herramientas como el &lt;strong>Propensity Score&lt;/strong> y los &lt;strong>estimadores doblemente robustos&lt;/strong> (DR) se utilizan para controlar los sesgos de selecci√≥n a partir de covariables observables, mientras que los &lt;strong>Efectos de Tratamiento Promedio Condicionales (CATE)&lt;/strong>, apoyados en machine learning, permiten explorar la heterogeneidad del efecto entre subpoblaciones. Asimismo, un conjunto de estrategias cuasi-experimentales ha abierto nuevos horizontes en la investigaci√≥n, incluyendo el uso de &lt;strong>Variables Instrumentales (IV)&lt;/strong> para corregir la confusi√≥n no observada, el &lt;strong>Diferencias-en-Diferencias (DID)&lt;/strong> y el &lt;strong>Control Sint√©tico (SC)&lt;/strong> para comparar trayectorias temporales, y la &lt;strong>Regresi√≥n Discontinua (RDD)&lt;/strong> para explotar umbrales de asignaci√≥n, todas ellas permitiendo identificar efectos causales en contextos donde la aleatorizaci√≥n no es factible.&lt;/p>
&lt;hr>
&lt;h2 id="1-el-desaf√≠o-central-solo-vemos-un-lado-de-la-moneda">1. El desaf√≠o central: solo vemos un lado de la moneda&lt;/h2>
&lt;p>Imagina que quieres evaluar el impacto de una nueva pol√≠tica de vacunaci√≥n. Para cada persona, podr√≠amos definir dos mundos posibles: uno donde recibe la vacuna y otro donde no. Pero en la pr√°ctica solo observamos un mundo: el que ocurri√≥. Ese es el &lt;strong>‚Äúproblema fundamental de la inferencia causal‚Äù&lt;/strong>.&lt;/p>
&lt;p>El &lt;strong>marco de resultados potenciales&lt;/strong> formaliza esta idea:&lt;/p>
$$
\tau_{\text{sample}} = \frac{1}{N} \sum_{i=1}^{N} (Y_i(1) - Y_i(0))
$$
&lt;p>En un ensayo controlado aleatorizado (RCT), la aleatorizaci√≥n nos permite estimar este efecto promedio simplemente comparando medias. Pero ¬øqu√© pasa cuando los RCT no son factibles por razones √©ticas, log√≠sticas o econ√≥micas?&lt;/p>
&lt;hr>
&lt;h2 id="2-estudios-observacionales-cuando-no-hay-azar-pero-s√≠-ingenio">2. Estudios observacionales: cuando no hay azar, pero s√≠ ingenio&lt;/h2>
&lt;p>En contextos reales ‚Äîsalud p√∫blica, econom√≠a, pol√≠ticas sociales‚Äî dependemos de datos observacionales. All√≠, la clave es suponer que, condicional en ciertas variables previas ($X_i$), la asignaci√≥n al tratamiento es ‚Äútan buena como aleatoria‚Äù.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Propensity Score (Rosenbaum &amp;amp; Rubin, 1983):&lt;/strong> condensa m√∫ltiples covariables en una √∫nica probabilidad de recibir tratamiento, facilitando el emparejamiento y la ponderaci√≥n.&lt;/li>
&lt;li>&lt;strong>Estimadores doblemente robustos:&lt;/strong> combinan modelos de resultado y de asignaci√≥n al tratamiento; basta con que uno est√© bien especificado para obtener estimaciones consistentes.&lt;/li>
&lt;li>&lt;strong>CATE (Conditional Average Treatment Effect):&lt;/strong> con machine learning, hoy podemos explorar c√≥mo los efectos var√≠an entre subpoblaciones (ej. pol√≠ticas de empleo m√°s efectivas en j√≥venes que en adultos mayores).&lt;/li>
&lt;/ul>
&lt;p>‚ö†Ô∏è Cuando sospechamos de confusi√≥n no observada, entran en juego &lt;strong>an√°lisis de sensibilidad&lt;/strong> (Manski bounds, m√©todos de Rosenbaum).&lt;/p>
&lt;hr>
&lt;h2 id="3-estrategias-avanzadas-cuando-la-confusi√≥n-no-puede-ignorarse">3. Estrategias avanzadas cuando la confusi√≥n no puede ignorarse&lt;/h2>
&lt;h3 id="a-variables-instrumentales-iv">a) Variables Instrumentales (IV)&lt;/h3>
&lt;p>Si un confusor no observado afecta tanto al tratamiento como al resultado, un &lt;strong>instrumento v√°lido&lt;/strong> ($Z$) puede rescatar el an√°lisis. Ejemplo cl√°sico: la distancia a una universidad como instrumento para estudiar el impacto de la educaci√≥n en ingresos.&lt;/p>
&lt;p>Bajo ciertos supuestos, identificamos el &lt;strong>LATE (Local Average Treatment Effect)&lt;/strong> para quienes cambian su estado de tratamiento debido al instrumento.&lt;/p>
&lt;hr>
&lt;h3 id="b-diferencias-en-diferencias-did-y-control-sint√©tico-sc">b) Diferencias-en-Diferencias (DID) y Control Sint√©tico (SC)&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>DID&lt;/strong>: compara tendencias pre y post en grupos tratados y no tratados. Ejemplo: medir el impacto de un aumento del salario m√≠nimo sobre el empleo.&lt;/li>
&lt;li>&lt;strong>SC&lt;/strong>: construye un ‚Äúgemelo sint√©tico‚Äù de la unidad tratada combinando unidades no tratadas. Ejemplo: evaluar el impacto de una ley antitabaco en California comparando con un control sint√©tico formado por otros estados.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="c-regresi√≥n-discontinua-rdd">c) Regresi√≥n Discontinua (RDD)&lt;/h3>
&lt;p>Aprovecha umbrales de asignaci√≥n. Ejemplo: un programa de becas asignado a estudiantes con notas ‚â• 8.0. Comparar resultados justo por encima y por debajo del corte estima el efecto del programa en los ‚Äúmarginales‚Äù.&lt;/p>
&lt;hr>
&lt;h2 id="4-horizontes-emergentes-combinar-evidencia">4. Horizontes emergentes: combinar evidencia&lt;/h2>
&lt;p>Dos l√≠neas prometedoras:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Surrogacy:&lt;/strong> usar resultados a corto plazo como sustitutos de resultados de largo plazo.&lt;/li>
&lt;li>&lt;strong>Integraci√≥n de experimentos y observacionales:&lt;/strong> Athey et al. (2020) proponen usar experimentos para identificar y corregir confusores en estudios observacionales.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="-tabla-comparativa-de-m√©todos-de-inferencia-causal">üìä Tabla comparativa de m√©todos de inferencia causal&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>M√©todo&lt;/th>
&lt;th>Supuesto clave&lt;/th>
&lt;th>Ventajas&lt;/th>
&lt;th>Limitaciones&lt;/th>
&lt;th>Ejemplo t√≠pico&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>RCT&lt;/strong>&lt;/td>
&lt;td>Asignaci√≥n aleatoria&lt;/td>
&lt;td>Estimador insesgado, alta validez interna&lt;/td>
&lt;td>Costoso, a veces poco √©tico, baja validez externa&lt;/td>
&lt;td>Ensayo cl√≠nico de un f√°rmaco&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Propensity Score / DR&lt;/strong>&lt;/td>
&lt;td>Confusi√≥n controlada por covariables observadas&lt;/td>
&lt;td>Flexibilidad, usa datos observacionales grandes&lt;/td>
&lt;td>Vulnerable a confusi√≥n no observada&lt;/td>
&lt;td>Impacto de programas sociales&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>IV&lt;/strong>&lt;/td>
&lt;td>Relevancia + exclusi√≥n + exogeneidad&lt;/td>
&lt;td>Corrige confusi√≥n no observada&lt;/td>
&lt;td>Dif√≠cil encontrar instrumentos v√°lidos&lt;/td>
&lt;td>Educaci√≥n ‚Üí ingresos&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>DID&lt;/strong>&lt;/td>
&lt;td>Tendencias paralelas&lt;/td>
&lt;td>Simple, interpretable&lt;/td>
&lt;td>Fr√°gil si tendencias difieren&lt;/td>
&lt;td>Pol√≠ticas laborales&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>SC&lt;/strong>&lt;/td>
&lt;td>Unidades control combinan bien la pre-tendencia&lt;/td>
&lt;td>M√°s cre√≠ble que DID en casos individuales&lt;/td>
&lt;td>Requiere datos ricos pre-tratamiento&lt;/td>
&lt;td>Leyes de salud p√∫blica&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>RDD&lt;/strong>&lt;/td>
&lt;td>Continuidad de potenciales en el umbral&lt;/td>
&lt;td>Interpretaci√≥n clara, dise√±o cuasi-experimental&lt;/td>
&lt;td>V√°lido solo cerca del umbral&lt;/td>
&lt;td>Programas de becas&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;hr>
&lt;h2 id="5-conclusi√≥n-pr√°ctica">5. Conclusi√≥n pr√°ctica&lt;/h2>
&lt;p>La inferencia causal no es una caja negra: es un conjunto de &lt;strong>herramientas matem√°ticas y conceptuales&lt;/strong> que, bien aplicadas, permiten a responsables de pol√≠ticas, cl√≠nicos y cient√≠ficos sociales responder la pregunta clave: &lt;em>¬øqu√© pasar√≠a si?&lt;/em>&lt;/p>
&lt;p>La frontera actual est√° en combinar evidencia, explotar machine learning para heterogeneidad y desarrollar m√©todos m√°s robustos frente a confusi√≥n no observada.&lt;/p>
&lt;hr>
&lt;h2 id="-referencias-recomendadas">üìö Referencias recomendadas&lt;/h2>
&lt;ul>
&lt;li>Rosenbaum PR, Rubin DB. &lt;em>The central role of the propensity score‚Ä¶&lt;/em> Biometrika. 1983. &lt;a href="https://pubmed.ncbi.nlm.nih.gov/12009849/" target="_blank" rel="noopener">PubMed PMID: 12009849&lt;/a>&lt;/li>
&lt;li>Hern√°n MA, Robins JM. &lt;em>Causal Inference: What If&lt;/em>. 2020. &lt;a href="https://pubmed.ncbi.nlm.nih.gov/33290294/" target="_blank" rel="noopener">PubMed PMID: 33290294&lt;/a>&lt;/li>
&lt;li>Imbens GW, Rubin DB. &lt;em>Causal Inference for Statistics, Social, and Biomedical Sciences&lt;/em>. 2015.&lt;/li>
&lt;li>Athey S, Imbens GW. &lt;em>Design-based analysis in difference-in-differences settings.&lt;/em> J Econometrics. 2022. &lt;a href="https://pubmed.ncbi.nlm.nih.gov/36212825/" target="_blank" rel="noopener">PubMed PMID: 36212825&lt;/a>&lt;/li>
&lt;li>Abadie A. &lt;em>Using synthetic controls.&lt;/em> J Econometrics. 2021.&lt;/li>
&lt;/ul></description></item><item><title>Ciencia o Pirotecnia: Por Qu√© la 'Significaci√≥n Estad√≠stica' Nos Ciega con Falsos Destellos</title><link>https://maicel.netlify.app/post/2025-08-10-significacion/</link><pubDate>Sun, 10 Aug 2025 15:30:00 +0000</pubDate><guid>https://maicel.netlify.app/post/2025-08-10-significacion/</guid><description>&lt;p>Ese momento de euforia al ver &lt;code>p &amp;lt; 0.05&lt;/code>‚Ä¶ ¬øes un descubrimiento genuino o solo un destello enga√±oso de pirotecnia estad√≠stica?&lt;/p>
&lt;p>En la ciencia, hay un instante que todos los investigadores anhelan. Es la culminaci√≥n de meses, a veces a√±os, de riguroso trabajo. Corres el an√°lisis y, de repente, ah√≠ est√°: &lt;code>p &amp;lt; 0.05&lt;/code>. Es una &lt;strong>explosi√≥n de alivio&lt;/strong>, un destello de ‚Äú¬°Eureka!‚Äù en la oscuridad de la incertidumbre. Sentimos que hemos encontrado algo real, algo digno de ser publicado.&lt;/p>
&lt;p>Pero, ¬øy si ese destello es solo eso? Un estallido moment√°neo, deslumbrante y ruidoso, pero que en el fondo significa muy poco. ¬øY si nuestro ritual m√°s sagrado es, en realidad, un simple juego de pirotecnia, dise√±ado para impresionar m√°s que para iluminar?&lt;/p>
&lt;p>Durante d√©cadas, hemos aceptado el umbral de significaci√≥n estad√≠stica como el √°rbitro indiscutible de la verdad cient√≠fica. Sin embargo, la evidencia acumulada ‚Äîdesde las cr√≠ticas de su propio creador (Ronald Fisher) hasta las advertencias oficiales de la American Statistical Association (ASA) en 2016 y el clamor de cientos de cient√≠ficos en la revista &lt;em>Nature&lt;/em> ‚Äî nos obliga a una conclusi√≥n inc√≥moda(Amrhein, Greenland, and McShane 2019; Wasserstein and Lazar 2016) : &lt;strong>el emperador estad√≠stico est√° desnudo&lt;/strong>. La pr√°ctica de las Pruebas de Significaci√≥n de la Hip√≥tesis Nula (NHST, por sus siglas en ingl√©s) no es un pilar de rigor, sino un ritual plagado de l√≥gica err√≥nea, confusi√≥n e inadecuaci√≥n para la verdadera investigaci√≥n.&lt;/p>
&lt;h2 id="el-cohete-m√°s-grande-no-significa-mejor">El Cohete: M√°s Grande no Significa Mejor&lt;/h2>
&lt;p>En la pirotecnia, un cohete m√°s grande produce una explosi√≥n m√°s fuerte. Es simple f√≠sica. En la estad√≠stica, ocurre algo perturbadoramente similar. El ‚Äúcohete‚Äù es nuestro tama√±o muestral.&lt;/p>
&lt;p>La conclusi√≥n de una prueba de significaci√≥n depende de manera crucial del tama√±o de la muestra. Con un cohete lo suficientemente grande (una muestra de miles o decenas de miles de personas), la diferencia m√°s trivial e insignificante para el mundo real se convertir√°, casi por arte de magia, en ‚Äúestad√≠sticamente significativa‚Äù. Por el contrario, un efecto importante y real puede pasar desapercibido si nuestro cohete es demasiado peque√±o.&lt;/p>
&lt;p>Esto nos lleva a una &lt;strong>verdad grotesca&lt;/strong>: la decisi√≥n sobre si un hallazgo es ‚Äúreal‚Äù a menudo depende m√°s de los recursos del investigador para recolectar datos masivos que de la naturaleza fundamental del fen√≥meno estudiado. El estallido nos dice m√°s sobre el tama√±o del cohete que sobre la belleza del cielo que intenta iluminar.&lt;/p>
&lt;!-- ```{r} -->
&lt;!-- set.seed(123) -->
&lt;!-- library(ggplot2) -->
&lt;!-- effect &lt;- 0.1 -->
&lt;!-- sd &lt;- 1 -->
&lt;!-- N &lt;- seq(20, 10000, by=50) -->
&lt;!-- p_values &lt;- sapply(N, function(n) { -->
&lt;!-- t.test(rnorm(n, mean=effect, sd=sd), mu=0)$p.value -->
&lt;!-- }) -->
&lt;!-- data &lt;- data.frame(N, p_values) -->
&lt;!-- ggplot(data, aes(N, p_values)) + -->
&lt;!-- geom_line() + -->
&lt;!-- geom_hline(yintercept = 0.05, linetype = "dashed", color="red") + -->
&lt;!-- labs(x = "Tama√±o muestral (N)", y = "Valor p") + -->
&lt;!-- theme_minimal() -->
&lt;!-- ``` -->
&lt;!-- Aqu√≠ ir√≠a tu gr√°fico explicativo del "Cohete (Tama√±o Muestral)": -->
&lt;!-- Un gr√°fico mostrando c√≥mo un efecto trivial (ej: diferencia de 0.1 unidades) se vuelve "significativo" (p&lt;0.05) a medida que N aumenta de 100 a 10,000. Puedes usar `ggplot2` en R para esto. -->
&lt;!-- Ejemplo de c√≥digo R (este no generar√≠a un gr√°fico, solo un placeholder para tu referencia): -->
&lt;!-- ```{r cohete_plot, echo=FALSE, fig.cap="Impacto del tama√±o muestral en la significaci√≥n estad√≠stica para un efecto constante."} -->
&lt;!-- # C√≥digo para generar el gr√°fico de N vs p-valor -->
&lt;!-- # plot(your_data$N, your_data$p_value, type="l", main="C√≥mo N afecta el p-valor", -->
&lt;!-- # xlab="Tama√±o Muestral (N)", ylab="Valor p") -->
&lt;!-- # abline(h=0.05, col="red", lty=2) -->
&lt;!-- ``` -->
&lt;h2 id="la-explosi√≥n-un-caos-de-luz-y-malentendidos">La Explosi√≥n: Un Caos de Luz y Malentendidos&lt;/h2>
&lt;p>La explosi√≥n de un fuego artificial es un evento ca√≥tico. Su interpretaci√≥n es subjetiva. ¬øFue espectacular? ¬øFue un fracaso? Lo mismo ocurre con el valor &lt;em>p&lt;/em>.&lt;/p>
&lt;h2 id="l√≥gica-err√≥nea-juzgando-por-lo-que-no-vimos">L√≥gica Err√≥nea: Juzgando por lo que no Vimos&lt;/h2>
&lt;p>La l√≥gica detr√°s del valor &lt;em>p&lt;/em> es, siendo generosos, peculiar. Se calcula asumiendo que la hip√≥tesis nula (H‚ÇÄ ‚Äîla hip√≥tesis de no-efecto, de que no hay diferencia o relaci√≥n) es cierta, y luego se determina la probabilidad de haber observado nuestros datos &lt;em>o datos a√∫n m√°s extremos&lt;/em> bajo esa suposici√≥n. Pi√©nsalo: nuestra conclusi√≥n sobre lo que &lt;em>s√≠&lt;/em> ocurri√≥ depende de la probabilidad de cosas que ni siquiera presenciamos. Es un &lt;strong>absurdo subyacente&lt;/strong>.&lt;/p>
&lt;p>Adem√°s, caemos constantemente en la &lt;strong>falacia de la probabilidad invertida&lt;/strong>: el valor &lt;em>p&lt;/em> nos dice la probabilidad de los datos dada la hip√≥tesis nula (P(Datos|H‚ÇÄ)), pero nosotros creemos err√≥neamente que nos dice la probabilidad de que la hip√≥tesis nula sea cierta dados nuestros datos (P(H‚ÇÄ|Datos)). Son dos cosas radicalmente distintas, y confundirlas es un error fundamental.&lt;/p>
&lt;!-- &lt;!-- Aqu√≠ ir√≠a tu diagrama de flujo simple o tabla comparativa para "Confusi√≥n P(D|H) vs P(H|D)": -->
&lt;!-- Puedes usarmermaid para diagramas simples en RMarkdown o una tabla Markdown para la comparaci√≥n. -->
&lt;!-- Ejemplo de c√≥digo mermaid (necesitar√≠as `config: {mermaid: {sequence: {diagramMarginX: 10}}}` en el YAML para que funcione): -->
&lt;h2 id="confusi√≥n-el-ruido-no-es-la-se√±al">Confusi√≥n: El Ruido no es la Se√±al&lt;/h2>
&lt;p>La confusi√≥n m√°s extendida es la de equiparar ‚Äúsignificaci√≥n estad√≠stica‚Äù con ‚Äúimportancia pr√°ctica‚Äù o ‚Äúrelevancia cient√≠fica‚Äù. Un estallido muy ruidoso no significa que el descubrimiento sea importante, √∫til o generalizable. Esta obsesi√≥n por el ruido estad√≠stico, esta end√©mica ‚Äúsignificant-itis‚Äù, nos ha distra√≠do de lo que realmente importa en la investigaci√≥n: la magnitud del efecto y la relevancia cl√≠nica, social o te√≥rica de nuestros hallazgos.&lt;/p>
&lt;h2 id="el-caso-del-chocolate-que-hace-perder-peso">El Caso del Chocolate que Hace Perder Peso&lt;/h2>
&lt;p>Para ilustrar esta locura, consideremos el tristemente famoso estudio de John Bohannon en 2015, ‚ÄúChocolate con fines de p√©rdida de peso‚Äù . Con un peque√±o presupuesto, Bohannon realiz√≥ un ensayo aleatorio, controlado con chocolate, utilizando un n√∫mero muy reducido de participantes y midiendo 18 variables distintas(Bohannon 2015). Al analizar &lt;em>todas&lt;/em> las combinaciones posibles, encontr√≥ que con una muestra tan peque√±a y al realizar m√∫ltiples pruebas (un tipo de &lt;em>p-hacking&lt;/em>), era casi inevitable que alguna variable arrojara un p-valor menor a 0.05 &lt;strong>por pura casualidad&lt;/strong>. Con su &lt;code>p &amp;lt; 0.05&lt;/code> ‚Äúsignificativo‚Äù, los medios de comunicaci√≥n sensacionalistas se lanzaron a la noticia: ‚ÄúEl chocolate hace perder peso!‚Äù. Este caso es un claro ejemplo de c√≥mo un ‚Äúdestello‚Äù estad√≠stico puede ser completamente enga√±oso y carecer de cualquier importancia real, pero aun as√≠ generar titulares y confusi√≥n(Bohannon 2015).&lt;/p>
&lt;h2 id="el-veredicto-la-falsa-dicotom√≠a-del-ohhh-o-el-silencio">El Veredicto: La Falsa Dicotom√≠a del ‚ÄúOhhh‚Äù o el Silencio&lt;/h2>
&lt;p>Quien observa fuegos artificiales emite un veredicto simple: el ‚Äú¬°Ohhh!‚Äù de asombro o el silencio de la indiferencia. Las pruebas de significaci√≥n nos han impuesto esta misma decisi√≥n binaria: o un resultado es significativo (&lt;code>p &amp;lt; 0.05&lt;/code>), o no lo es. Es un interruptor de encendido/apagado.&lt;/p>
&lt;p>Pero la ciencia no funciona as√≠. El conocimiento cient√≠fico no es una serie de decisiones de ‚Äús√≠/no‚Äù. Es un proceso gradual de ajuste de nuestras creencias a la luz de la evidencia acumulada. Es un paisaje de grises, no un contraste de blanco y negro. Al forzarnos a este mecanicismo, a esta ‚Äúsucesi√≥n de ‚Äòdecisiones‚Äô autom√°ticas‚Äù que el propio Fisher denunci√≥, hemos empobrecido el discurso cient√≠fico, ignorando matices cruciales como la magnitud del efecto o la precisi√≥n de la estimaci√≥n.&lt;/p>
&lt;h2 id="despu√©s-del-humo-hacia-una-ciencia-iluminada">Despu√©s del Humo: Hacia una Ciencia Iluminada&lt;/h2>
&lt;p>Cuando el humo de la pirotecnia se disipa, ¬øqu√© nos queda? Nos queda la tarea de encontrar una luz m√°s honesta y duradera para la ciencia. Afortunadamente, esta luz existe y est√° ganando terreno.&lt;/p>
&lt;p>La alternativa fundamental es pasar de la &lt;strong>decisi√≥n binaria&lt;/strong> a la &lt;strong>estimaci√≥n&lt;/strong>. En lugar de preguntar obsesivamente ‚Äú¬øHay un efecto (s√≠/no)?‚Äù, debemos preguntar ‚Äú¬øCu√°l es la magnitud del efecto y cu√°n seguros estamos de esa estimaci√≥n?‚Äù.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Intervalos de Confianza (o de Compatibilidad):&lt;/strong> Son nuestra primera y m√°s accesible herramienta para una inferencia m√°s sensata. Los Intervalos de Confianza no nos dan un simple ‚Äús√≠/no‚Äù, sino un &lt;strong>rango de valores plausibles&lt;/strong> para el efecto real en la poblaci√≥n. Nos muestran tanto la magnitud estimada del efecto como la incertidumbre que lo rodea.&lt;/p>
&lt;p>Cuando vemos un Intervalo de Confianza del 95% para una diferencia, por ejemplo, esto significa que si repiti√©ramos el estudio muchas, muchas veces bajo las mismas condiciones, el 95% de esos intervalos contendr√≠an el verdadero valor del efecto que estamos tratando de estimar. Nos invitan a pensar en la variabilidad y la precisi√≥n de nuestras estimaciones, no solo en un umbral arbitrario. Son una luz constante que ilumina un paisaje, permiti√©ndonos ver el terreno completo, no un destello que ciega moment√°neamente.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;!-- &lt;!-- Aqu√≠ ir√≠a tu gr√°fico de "Intervalos de Confianza/Compatibilidad": -->
&lt;!-- Un gr√°fico con varios ICs superpuestos (algunos estrechos, otros anchos, algunos cruzando cero, otros no) para ilustrar que muestran magnitud *e* incertidumbre, no solo "s√≠/no". -->
&lt;!-- ```{r ic_plot, echo=FALSE, fig.cap="Visualizaci√≥n de Intervalos de Confianza: Magnitud y Incertidumbre."} -->
&lt;!-- # C√≥digo para generar el gr√°fico de ICs -->
&lt;!-- # library(ggplot2) -->
&lt;!-- # data.frame( -->
&lt;!-- # Effect = c(0.5, 1.2, -0.3, 0.8), -->
&lt;!-- # Lower = c(0.1, 0.8, -0.7, 0.2), -->
&lt;!-- # Upper = c(0.9, 1.6, 0.1, 1.4) -->
&lt;!-- # ) %>% -->
&lt;!-- # ggplot(aes(y = factor(1:4), x = Effect, xmin = Lower, xmax = Upper)) + -->
&lt;!-- # geom_point() + geom_errorbarh(height = 0.2) + -->
&lt;!-- # geom_vline(xintercept = 0, linetype = "dashed", color = "grey") + -->
&lt;!-- # labs(x = "Magnitud del Efecto", y = "Estudio") + -->
&lt;!-- # theme_minimal() -->
&lt;!-- ``` -->
&lt;!-- -->
&lt;ul>
&lt;li>
&lt;p>&lt;strong>El Enfoque Bayesiano:&lt;/strong> Es el siguiente paso evolutivo en la inferencia estad√≠stica, y es la encarnaci√≥n matem√°tica del razonamiento cient√≠fico. Los m√©todos bayesianos nos permiten hacer lo que siempre hemos querido hacer: &lt;strong>combinar la evidencia de nuestro estudio actual con todo el conocimiento previo existente&lt;/strong> (estudios anteriores, plausibilidad biol√≥gica, experiencia cl√≠nica) para llegar a una conclusi√≥n actualizada y probabil√≠stica sobre nuestras hip√≥tesis.&lt;/p>
&lt;p>A diferencia del valor &lt;em>p&lt;/em>, el enfoque bayesiano responde directamente a la pregunta que realmente queremos hacer: ‚Äú&lt;strong>Dados estos nuevos datos, ¬øqu√© tan cre√≠ble es mi hip√≥tesis ahora?&lt;/strong>‚Äù. Nos da una probabilidad posterior de nuestra hip√≥tesis, una medida directa de nuestra creencia actualizada. Aunque puede parecer m√°s complejo al principio, el razonamiento bayesiano se alinea intuitivamente con c√≥mo los cient√≠ficos y las personas actualizan sus creencias en la vida real.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Dejemos la pirotecnia para las celebraciones. Es hora de que la ciencia deje de buscar destellos ef√≠meros y se dedique a construir una iluminaci√≥n constante, acumulativa y, sobre todo, &lt;strong>honesta&lt;/strong>. Es el momento de una ciencia m√°s transparente, rigurosa y, s√≠, ¬°m√°s divertida de interpretar!&lt;/p>
&lt;hr>
&lt;h2 id="tu-turno">¬°Tu Turno!&lt;/h2>
&lt;p>¬øTe has encontrado con la ‚Äútiran√≠a del p&amp;lt;0.05‚Äù en tu campo? ¬øQu√© alternativas usas o te gustar√≠a ver m√°s promovidas en la investigaci√≥n? ¬°Comparte tu experiencia y tus pensamientos en los comentarios a continuaci√≥n!&lt;/p>
&lt;hr>
&lt;h2 id="referencias">Referencias&lt;/h2>
&lt;div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
&lt;div id="ref-amrhein2019" class="csl-entry">
&lt;p>Amrhein, Valentin, Sander Greenland, and Blake McShane. 2019. ‚ÄúScientists Rise up Against Statistical Significance.‚Äù &lt;em>Nature&lt;/em> 567 (7748): 305‚Äì7. &lt;a href="https://doi.org/10.1038/d41586-019-00857-9" target="_blank" rel="noopener">https://doi.org/10.1038/d41586-019-00857-9&lt;/a>.&lt;/p>
&lt;/div>
&lt;div id="ref-bohannon2015" class="csl-entry">
&lt;p>Bohannon, John. 2015. ‚ÄúI Fooled Millions into Thinking Chocolate Helps Weight Loss. Here‚Äôs How.‚Äù &lt;a href="https://gizmodo.com/i-fooled-millions-into-thinking-chocolate-helps-weight-1707251800" target="_blank" rel="noopener">https://gizmodo.com/i-fooled-millions-into-thinking-chocolate-helps-weight-1707251800&lt;/a>.&lt;/p>
&lt;/div>
&lt;div id="ref-wasserstein2016" class="csl-entry">
&lt;p>Wasserstein, Ronald L., and Nicole A. Lazar. 2016. ‚ÄúThe ASA Statement on &lt;em>p&lt;/em> -Values: Context, Process, and Purpose.‚Äù &lt;em>The American Statistician&lt;/em> 70 (2): 129‚Äì33. &lt;a href="https://doi.org/10.1080/00031305.2016.1154108" target="_blank" rel="noopener">https://doi.org/10.1080/00031305.2016.1154108&lt;/a>.&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>Dise√±os Alternativos a Ensayos Cl√≠nicos Controlados Aleatorizados: Contextos de Aplicaci√≥n y Riesgos Regulatorios</title><link>https://maicel.netlify.app/post/atajos/</link><pubDate>Thu, 07 Aug 2025 00:00:00 +0000</pubDate><guid>https://maicel.netlify.app/post/atajos/</guid><description>&lt;p>üéß &lt;strong>Escucha el podcast de esta publicaci√≥n&lt;/strong>
&lt;audio controls >
&lt;source src="https://maicel.netlify.app/mp3/atajos.mp3" type="audio/mpeg">
&lt;/audio>
&lt;/p>
&lt;h1 id="introducci√≥n-fundamentos-de-evidencia-confirmatoria">Introducci√≥n: Fundamentos de evidencia confirmatoria&lt;/h1>
&lt;p>Los ensayos cl√≠nicos controlados, aleatorizados y enmascarados (ECAs) constituyen el marco metodol√≥gico m√°s robusto para evaluar la &lt;strong>eficacia y seguridad&lt;/strong> de nuevas intervenciones terap√©uticas. Son considerados el &amp;ldquo;est√°ndar de oro&amp;rdquo; en investigaci√≥n cl√≠nica por agencias como &lt;strong>ICH, EMA y FDA&lt;/strong>, ya que proporcionan el mayor control contra el sesgo y la mayor solidez en inferencia causal.&lt;/p>
&lt;p>En este marco, los &lt;strong>ensayos confirmatorios&lt;/strong> son aquellos dise√±ados expl√≠citamente para confirmar hip√≥tesis cl√≠nicas relevantes, con base en an√°lisis predefinidos y control adecuado de errores. Cuando estos estudios sustentan una solicitud de comercializaci√≥n, adquieren la condici√≥n de &lt;strong>ensayos pivotal&lt;/strong> (terminolog√≠a empleada por FDA y EMA).&lt;/p>
&lt;p>En principio, se recomienda la presentaci√≥n de &lt;strong>dos ensayos confirmatorios independientes&lt;/strong>. Sin embargo, &lt;strong>tanto EMA como FDA&lt;/strong> contemplan aprobaciones basadas en &lt;strong>un √∫nico ensayo pivotal&lt;/strong>, siempre que est√© respaldado por &lt;strong>evidencia complementaria rigurosa&lt;/strong>. La &lt;strong>FDA&lt;/strong>, mediante el concepto de &lt;strong>evidencia confirmatoria&lt;/strong> (FDA 2023), formaliza esta v√≠a alternativa. La &lt;strong>EMA&lt;/strong>, aunque no utiliza esa denominaci√≥n, acepta enfoques equivalentes (meta-an√°lisis, controles hist√≥ricos validados, an√°lisis de consistencia interna).&lt;/p>
&lt;h2 id="tabla-1-t√©rminos-clave-definici√≥n-origen-normativo-y-funci√≥n-regulatoria">Tabla 1. T√©rminos clave: definici√≥n, origen normativo y funci√≥n regulatoria&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;strong>T√©rmino&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Fuente regulatoria&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Definici√≥n t√©cnica&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Funci√≥n regulatoria&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>Ensayo cl√≠nico confirmatorio&lt;/strong>&lt;/td>
&lt;td>ICH E9&lt;/td>
&lt;td>Estudio con hip√≥tesis expl√≠cita, an√°lisis planificado y dise√±o robusto para inferencia causal.&lt;/td>
&lt;td>Fuente principal de evidencia en autorizaciones de comercializaci√≥n.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Ensayo pivotal&lt;/strong>&lt;/td>
&lt;td>FDA, EMA&lt;/td>
&lt;td>Estudio esencial para demostrar eficacia y seguridad en el expediente regulatorio.&lt;/td>
&lt;td>Sustento central del dossier cl√≠nico.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Evidencia confirmatoria&lt;/strong>&lt;/td>
&lt;td>FDA (2023)&lt;/td>
&lt;td>Datos adicionales (cl√≠nicos/no cl√≠nicos) que refuerzan un ensayo pivotal √∫nico.&lt;/td>
&lt;td>Permite cumplir el est√°ndar de &amp;ldquo;evidencia sustancial&amp;rdquo; con un solo ECA.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Evidencia sustancial&lt;/strong>&lt;/td>
&lt;td>FDA (21 CFR 314.126)&lt;/td>
&lt;td>Est√°ndar legal que puede cumplirse mediante 2 ECAs o, cuando est√© justificado, 1 ECA + confirmaci√≥n.&lt;/td>
&lt;td>Requisito para la aprobaci√≥n de nuevos medicamentos en EE.UU.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Evidencia complementaria&lt;/strong>&lt;/td>
&lt;td>ICH E9; EMA, FDA&lt;/td>
&lt;td>Estudios no pivotal que aportan consistencia (ej. subgrupos, dosis, seguridad, datos externos).&lt;/td>
&lt;td>Reforzar la plausibilidad de los hallazgos principales.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;hr>
&lt;h1 id="estructura-metodol√≥gica-de-los-ecas-y-su-anclaje-normativo">Estructura metodol√≥gica de los ECAs y su anclaje normativo&lt;/h1>
&lt;p>Los ECAs incorporan tres garant√≠as fundamentales: &lt;strong>aleatorizaci√≥n&lt;/strong>, &lt;strong>grupo control&lt;/strong> y &lt;strong>cegamiento&lt;/strong>. Estos componentes son esenciales para maximizar la validez interna y minimizar fuentes de sesgo sistem√°tico.&lt;/p>
&lt;h2 id="tabla-2-garant√≠as-anti-sesgo-en-ecas">Tabla 2. Garant√≠as anti-sesgo en ECAs&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;strong>Componente metodol√≥gico&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Funci√≥n espec√≠fica&lt;/strong>&lt;/th>
&lt;th>&lt;strong>C√≥mo reduce el sesgo&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Referencia normativa ICH&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>Aleatorizaci√≥n&lt;/strong>&lt;/td>
&lt;td>Asignaci√≥n no predecible de tratamientos&lt;/td>
&lt;td>Mitiga el sesgo de selecci√≥n; equilibra factores conocidos/desconocidos&lt;/td>
&lt;td>ICH E9 Secci√≥n 2.2.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Grupo control&lt;/strong>&lt;/td>
&lt;td>Comparaci√≥n con placebo o tratamiento est√°ndar&lt;/td>
&lt;td>Permite contrafactual v√°lido e inferencia causal robusta&lt;/td>
&lt;td>ICH E10; ICH E8(R1)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Cegamiento (blinding)&lt;/strong>&lt;/td>
&lt;td>Ocultamiento de asignaci√≥n a participantes e investigadores&lt;/td>
&lt;td>Previene sesgo de evaluaci√≥n, expectativa y cointervenci√≥n&lt;/td>
&lt;td>ICH E9 Secci√≥n 2.2.4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;hr>
&lt;h1 id="contextos-que-permiten-prescindir-de-ecas">Contextos que permiten prescindir de ECAs&lt;/h1>
&lt;p>La omisi√≥n de ECAs solo es aceptable en contextos claramente delimitados por justificaciones √©ticas, cient√≠ficas o log√≠sticas:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Enfermedades raras o ultrarraras&lt;/strong>, con poblaciones muy limitadas.&lt;/li>
&lt;li>&lt;strong>Situaciones de emergencia sanitaria&lt;/strong>, con necesidades cl√≠nicas urgentes.&lt;/li>
&lt;li>Cuando la &lt;strong>aleatorizaci√≥n o el cegamiento son √©ticamente problem√°ticos&lt;/strong>.&lt;/li>
&lt;li>Cuando existen &lt;strong>datos hist√≥ricos o reales bien estructurados y validados&lt;/strong>.&lt;/li>
&lt;/ol>
&lt;p>La aceptaci√≥n de estos dise√±os como base regulatoria &lt;strong>requiere validaci√≥n metodol√≥gica espec√≠fica y evaluaci√≥n caso por caso&lt;/strong>, seg√∫n gu√≠as EMA y FDA.&lt;/p>
&lt;hr>
&lt;h1 id="dise√±os-alternativos-m√°s-empleados-y-criterios-de-validaci√≥n">Dise√±os alternativos m√°s empleados y criterios de validaci√≥n&lt;/h1>
&lt;h2 id="tabla-3-dise√±os-alternativos-justificaci√≥n-limitaciones-y-requisitos-regulatorios">Tabla 3. Dise√±os alternativos: justificaci√≥n, limitaciones y requisitos regulatorios&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;strong>Dise√±o alternativo&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Justificaci√≥n de uso&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Limitaciones cr√≠ticas&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Requisitos de validaci√≥n&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>Single-Arm Trials (SAT)&lt;/strong>&lt;/td>
&lt;td>Enfermedades raras; oncolog√≠a sin comparador disponible&lt;/td>
&lt;td>Ausencia de grupo control; alta vulnerabilidad a sesgo de selecci√≥n&lt;/td>
&lt;td>Control externo robusto; endpoints validados; an√°lisis de consistencia&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Controles externos/sint√©ticos&lt;/strong>&lt;/td>
&lt;td>Cohortes hist√≥ricas o datos de mundo real&lt;/td>
&lt;td>Riesgo de confusi√≥n residual y sesgo de canalizaci√≥n&lt;/td>
&lt;td>Propensity score matching; alineaci√≥n temporal; CHMP/4366/2020 (EMA)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Dise√±os adaptativos&lt;/strong>&lt;/td>
&lt;td>Eficiencia en reclutamiento; ajustes preplanificados&lt;/td>
&lt;td>Riesgo de error tipo I si no se prespecifican adecuadamente&lt;/td>
&lt;td>Control de multiplicidad; simulaci√≥n previa; ICH E9(R1)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Real-World Evidence (RWE)&lt;/strong>&lt;/td>
&lt;td>Acceso a datos de rutina cl√≠nica; seguimiento largo; baja carga √©tica&lt;/td>
&lt;td>Falta de aleatorizaci√≥n; sesgo de confusi√≥n no medido&lt;/td>
&lt;td>PROACE principles (FDA 2023); data provenance; calidad del DMR; plan de an√°lisis riguroso&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;hr>
&lt;h1 id="riesgos-del-uso-estrat√©gico-de-dise√±os-no-confirmatorios">Riesgos del uso estrat√©gico de dise√±os no confirmatorios&lt;/h1>
&lt;p>Aunque v√°lidos en contextos excepcionales, el uso frecuente y estrat√©gico de estos dise√±os plantea &lt;strong>serios desaf√≠os regulatorios&lt;/strong>, especialmente si se recurre a ellos como v√≠a para &lt;strong>prescindir de ECAs sin justificaci√≥n metodol√≥gica s√≥lida&lt;/strong>.&lt;/p>
&lt;h3 id="riesgos-identificados">Riesgos identificados:&lt;/h3>
&lt;ul>
&lt;li>Aprobaciones basadas en &lt;strong>evidencia d√©bil o no reproducible&lt;/strong>.&lt;/li>
&lt;li>Incremento del &lt;strong>sesgo sistem√°tico&lt;/strong> en resultados de eficacia.&lt;/li>
&lt;li>Exposici√≥n de pacientes a intervenciones &lt;strong>no validadas rigurosamente&lt;/strong>.&lt;/li>
&lt;li>&lt;strong>Deslegitimaci√≥n del proceso regulatorio&lt;/strong> ante la comunidad cient√≠fica y la sociedad.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h1 id="recomendaciones-regulatorias">Recomendaciones regulatorias&lt;/h1>
&lt;h2 id="a-para-la-industria">A. Para la industria&lt;/h2>
&lt;ul>
&lt;li>Presentar &lt;strong>justificaci√≥n exhaustiva&lt;/strong> cuando no se utilicen ECAs.&lt;/li>
&lt;li>Cumplir con requisitos metodol√≥gicos espec√≠ficos seg√∫n tipo de dise√±o alternativo.&lt;/li>
&lt;li>Incluir an√°lisis de sensibilidad, consistencia y control de sesgo.&lt;/li>
&lt;/ul>
&lt;h2 id="b-para-las-agencias-reguladoras">B. Para las agencias reguladoras&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Reforzar gu√≠as espec√≠ficas&lt;/strong> (EMA, FDA) que regulen el uso de dise√±os alternativos.&lt;/li>
&lt;li>Exigir &lt;strong>registro p√∫blico&lt;/strong>, transparencia y acceso a protocolos completos.&lt;/li>
&lt;li>Fortalecer &lt;strong>capacidades internas&lt;/strong> en estad√≠stica avanzada y dise√±o de estudios no aleatorizados.&lt;/li>
&lt;li>Establecer &lt;strong>comit√©s independientes&lt;/strong> de evaluaci√≥n para salvaguardas √©ticas y cient√≠ficas.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h1 id="conclusi√≥n">Conclusi√≥n&lt;/h1>
&lt;p>Los dise√±os alternativos no deben entenderse como sustitutos gen√©ricos del ECA, sino como herramientas &lt;strong>v√°lidas solo bajo condiciones estrictamente justificadas y validadas&lt;/strong>. El regulador tiene la responsabilidad cient√≠fica y √©tica de exigir evidencia que no solo sea suficiente, sino tambi√©n s√≥lida, reproducible y libre de sesgos. La precisi√≥n metodol√≥gica y la claridad terminol√≥gica no son un lujo en este proceso: &lt;strong>son el principio rector que protege la salud p√∫blica frente a la ambig√ºedad cient√≠fica&lt;/strong>.&lt;/p>
&lt;hr>
&lt;h1 id="bibliograf√≠a">Bibliograf√≠a&lt;/h1>
&lt;ol>
&lt;li>FDA. &lt;em>Demonstrating Substantial Evidence of Effectiveness with One Adequate and Well-Controlled Clinical Investigation and Confirmatory Evidence&lt;/em>. Draft Guidance. 2023.&lt;/li>
&lt;li>EMA. &lt;em>Reflection Paper on Single‚ÄëArm Trials as Pivotal Evidence&lt;/em>. EMA/CHMP/161300/2023.&lt;/li>
&lt;li>FDA. &lt;em>Real-World Evidence Framework&lt;/em>. 2023 Update.&lt;/li>
&lt;li>EMA. &lt;em>Use of Real‚ÄëWorld Evidence in Regulatory Decision-Making&lt;/em>. EMA/INS/GCP/455222/2024.&lt;/li>
&lt;li>ICH. &lt;em>E9: Statistical Principles for Clinical Trials&lt;/em>. 1998.&lt;/li>
&lt;li>ICH. &lt;em>E9(R1): Addendum on Estimands and Sensitivity Analysis in Clinical Trials&lt;/em>. 2020.&lt;/li>
&lt;li>EMA. &lt;em>Guideline on Adjustment for Baseline Covariates in Clinical Trials&lt;/em>. CHMP/EWP/2863/99.&lt;/li>
&lt;li>FDA. &lt;em>Real-World Evidence Program Annual Report 2025 (PUB-245)&lt;/em>.&lt;/li>
&lt;li>Temple R, Ellenberg SS. &lt;em>Placebo-Controlled Trials and Active-Control Trials in the Evaluation of New Treatments&lt;/em>. &lt;em>Ann Intern Med&lt;/em>. 2000;133(6):455‚Äì463.&lt;/li>
&lt;/ol></description></item><item><title>An√°lisis Exploratorio de Datos: Desentra√±ando la Verdad de tus Datos</title><link>https://maicel.netlify.app/post/eda/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://maicel.netlify.app/post/eda/</guid><description>&lt;p>üéß &lt;strong>Escucha el podcast de esta publicaci√≥n&lt;/strong>
&lt;audio controls >
&lt;source src="https://maicel.netlify.app/mp3/eda.mp3" type="audio/mpeg">
&lt;/audio>
&lt;/p>
&lt;p>El &lt;strong>An√°lisis Exploratorio de Datos (EDA)&lt;/strong> es una disciplina fundamental en el campo de la ciencia de datos, popularizada por el matem√°tico John Tukey. M√°s que una simple serie de pasos, el EDA es una filosof√≠a que nos invita a interactuar con nuestros datos, visualizarlos, resumirlos y &amp;ldquo;hablar&amp;rdquo; con ellos antes de saltar a modelados complejos. Implica el an√°lisis de datos centrado en comprender a fondo su estructura, identificar patrones ocultos, detectar anomal√≠as (valores at√≠picos), gestionar datos ausentes y, en √∫ltima instancia, proporcionar una base s√≥lida para la formulaci√≥n de modelos predictivos o inferenciales. Adem√°s, es crucial para descubrir c√≥mo se relacionan las variables entre s√≠.&lt;/p>
&lt;h2 id="la-regla-de-oro-gigo-garbage-in-garbage-out">La Regla de Oro: GIGO (Garbage In, Garbage Out)&lt;/h2>
&lt;p>Un concepto popular y vital en el campo de la ciencia de datos es &lt;strong>GIGO&lt;/strong> (Garbage In, Garbage Out, o &amp;ldquo;Basura entra, basura sale&amp;rdquo;). Este concepto subraya que la calidad de los resultados de cualquier an√°lisis o modelo es directamente proporcional a la calidad de los datos de entrada. No importa cu√°n sofisticado sea tu algoritmo o cu√°n potente sea tu infraestructura computacional, los datos de mala calidad siempre producir√°n resultados deficientes, enga√±osos o in√∫tiles. El EDA es nuestra primera l√≠nea de defensa contra el GIGO, asegurando que trabajamos con datos limpios y comprensibles.&lt;/p>
&lt;h2 id="un-flujo-de-trabajo-pr√°ctico-de-eda">Un Flujo de Trabajo Pr√°ctico de EDA&lt;/h2>
&lt;p>Aunque el EDA es un proceso iterativo y no una &amp;ldquo;camisa de fuerza&amp;rdquo; r√≠gida, es √∫til seguir un flujo de trabajo estructurado para garantizar que cubrimos los aspectos m√°s importantes. El orden de las etapas y el √©nfasis en cada una depender√°n en gran medida del problema espec√≠fico, el tipo de datos y los objetivos del an√°lisis.&lt;/p>
&lt;p>Este proceso general incluye:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="" srcset="
/post/eda/fig01_hu_1d111632709b1b76.webp 400w,
/post/eda/fig01_hu_b26fdc398618e3dc.webp 760w,
/post/eda/fig01_hu_540748744cda905c.webp 1200w"
src="https://maicel.netlify.app/post/eda/fig01_hu_1d111632709b1b76.webp"
width="760"
height="578"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>A continuaci√≥n, profundicemos en cada una de estas etapas, utilizando ejemplos pr√°cticos con el paquete &lt;code>dlookr&lt;/code> en R.&lt;/p>
&lt;h3 id="1-comprensi√≥n-general-de-los-datos-y-evaluaci√≥n-de-su-calidad">1. Comprensi√≥n General de los Datos y Evaluaci√≥n de su Calidad&lt;/h3>
&lt;p>Antes de sumergirnos en an√°lisis profundos, es fundamental tener una visi√≥n panor√°mica de nuestro conjunto de datos. Esta etapa implica:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Dimensiones del dataset:&lt;/strong> ¬øCu√°ntas filas (observaciones) y columnas (variables) tenemos?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Tipos de datos:&lt;/strong> ¬øLas variables son num√©ricas (enteros, flotantes), categ√≥ricas (factores, caracteres), l√≥gicas o de fecha/hora? Es crucial que los tipos de datos sean correctos para las operaciones que deseamos realizar.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Inspecci√≥n inicial:&lt;/strong> Revisar las primeras y √∫ltimas filas del dataset para obtener una idea general del formato y contenido.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Estad√≠sticas descriptivas b√°sicas:&lt;/strong> Para variables num√©ricas: media, mediana, desviaci√≥n est√°ndar, m√≠nimo, m√°ximo, cuartiles. Para variables categ√≥ricas: conteo de ocurrencias, proporciones. Esto nos da una primera impresi√≥n de la dispersi√≥n y centralidad de los datos.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Con &lt;code>dlookr&lt;/code>, la funci√≥n &lt;code>diagnose()&lt;/code> es ideal para una revisi√≥n r√°pida de la calidad de los datos, mostrando el tipo de variable, el n√∫mero de valores √∫nicos, valores faltantes, valores cero y valores negativos.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Para obtener un resumen r√°pido de las caracter√≠sticas de los datos&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">diagnose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_na&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Esta fase nos ayuda a formar una primera hip√≥tesis sobre la calidad y estructura de los datos, identificando posibles problemas desde el principio.&lt;/p>
&lt;h4 id="2-identificaci√≥n-y-tratamiento-de-valores-faltantes-y-at√≠picos">2. Identificaci√≥n y Tratamiento de Valores Faltantes y At√≠picos&lt;/h4>
&lt;p>Los valores faltantes (NA, NaN, null) y los valores at√≠picos (outliers) son dos de los desaf√≠os m√°s comunes en cualquier conjunto de datos y pueden distorsionar significativamente los resultados de nuestros an√°lisis y modelos.&lt;/p>
&lt;p>&lt;strong>Valores Faltantes:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Identificaci√≥n:&lt;/strong> Cuantificar la cantidad y proporci√≥n de valores faltantes por variable. Visualizar patrones de ausencia (¬ølos valores faltantes ocurren aleatoriamente o hay un patr√≥n?).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Tratamiento:&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Eliminaci√≥n:&lt;/strong> Si la cantidad de valores faltantes es peque√±a o si una variable tiene un porcentaje muy alto de NAs, se pueden eliminar filas o columnas.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Imputaci√≥n:&lt;/strong> Rellenar los valores faltantes. M√©todos comunes incluyen la media, mediana o moda (para datos num√©ricos y categ√≥ricos, respectivamente), o m√©todos m√°s avanzados basados en modelos (regresi√≥n, k-NN, etc.). La elecci√≥n depende de la naturaleza de los datos y el problema.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Valores At√≠picos:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Identificaci√≥n:&lt;/strong> Observaciones que se desv√≠an significativamente del resto de los datos. Se pueden detectar mediante gr√°ficos de caja (boxplots), diagramas de dispersi√≥n, puntuaciones Z, el m√©todo IQR (rango intercuart√≠lico) o algoritmos m√°s sofisticados.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Tratamiento:&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Eliminaci√≥n:&lt;/strong> Si se confirma que son errores de entrada de datos.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Transformaci√≥n:&lt;/strong> Aplicar transformaciones logar√≠tmicas o de ra√≠z cuadrada para reducir su impacto.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Capping/Flooring:&lt;/strong> Limitar los valores at√≠picos a un percentil superior o inferior (por ejemplo, el 99% o el 1%).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Mantener:&lt;/strong> A veces, los valores at√≠picos son observaciones genuinas e importantes que no deben eliminarse.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;code>dlookr&lt;/code> ofrece funciones visuales y program√°ticas para abordar estos problemas:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Visualizar la distribuci√≥n de valores faltantes&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot_na&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_na&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Identificar valores at√≠picos para una variable espec√≠fica (ej. &amp;#34;hp&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># plot_outlier() es excelente para visualizar.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot_outlier&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_df&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;hp&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="3-an√°lisis-de-la-distribuci√≥n-de-las-variables">3. An√°lisis de la Distribuci√≥n de las Variables&lt;/h3>
&lt;p>Comprender la distribuci√≥n de cada variable individualmente es clave para seleccionar los m√©todos estad√≠sticos y de modelado adecuados.&lt;/p>
&lt;p>&lt;strong>Variables Num√©ricas:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Histogramas y gr√°ficos de densidad:&lt;/strong> Permiten visualizar la forma de la distribuci√≥n, identificar asimetr√≠as (skewness), curtosis, y la presencia de m√∫ltiples modos.&lt;/li>
&lt;li>&lt;strong>Medidas de asimetr√≠a y curtosis:&lt;/strong> Cuantifican la forma de la distribuci√≥n.&lt;/li>
&lt;li>&lt;strong>Pruebas de normalidad:&lt;/strong> Aunque muchas veces no son estrictamente necesarias, pueden complementar el an√°lisis visual.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Variables Categ√≥ricas:&lt;/strong> - &lt;strong>Gr√°ficos de barras:&lt;/strong> Muestran la frecuencia o proporci√≥n de cada categor√≠a. - &lt;strong>Tablas de frecuencia:&lt;/strong> Resumen el conteo y porcentaje de cada nivel.&lt;/p>
&lt;p>&lt;code>dlookr&lt;/code> simplifica la visualizaci√≥n de distribuciones:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Visualizar la distribuci√≥n de una variable num√©rica (ej. &amp;#34;mpg&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot_hist&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_df&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;mpg&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># O ver la distribuci√≥n y normalidad&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot_normality&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_df&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;mpg&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Para variables categ√≥ricas (como &amp;#39;cyl&amp;#39; en mtcars que es num√©rica discreta)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Podemos convertirla a factor para un an√°lisis categ√≥rico.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">mtcars_factor_cyl&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">mtcars_df&lt;/span> &lt;span class="o">%&amp;gt;%&lt;/span> &lt;span class="nf">mutate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cyl&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">as.factor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cyl&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot_bar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_factor_cyl&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;cyl&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Este an√°lisis nos ayuda a entender el comportamiento de cada caracter√≠stica y a identificar la necesidad de transformaciones futuras.&lt;/p>
&lt;h3 id="4-an√°lisis-de-las-relaciones-entre-las-variables">4. An√°lisis de las Relaciones entre las Variables&lt;/h3>
&lt;p>Esta etapa se centra en descubrir c√≥mo las variables interact√∫an entre s√≠. Es fundamental para la selecci√≥n de caracter√≠sticas, la identificaci√≥n de multicolinealidad y la comprensi√≥n de la causalidad (o correlaci√≥n).&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Dos variables num√©ricas:&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Diagramas de dispersi√≥n (scatter plots):&lt;/strong> Visualizan la direcci√≥n y fuerza de la relaci√≥n (positiva, negativa, nula, no lineal).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Coeficientes de correlaci√≥n (Pearson, Spearman):&lt;/strong> Cuantifican la fuerza y direcci√≥n de la relaci√≥n lineal (Pearson) o mon√≥tona (Spearman).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Una variable num√©rica y una categ√≥rica:&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Gr√°ficos de caja (boxplots) o gr√°ficos de viol√≠n:&lt;/strong> Comparan la distribuci√≥n de la variable num√©rica entre las diferentes categor√≠as.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Pruebas t de Student o ANOVA:&lt;/strong> Para determinar si hay diferencias estad√≠sticamente significativas en las medias.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Dos variables categ√≥ricas:&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Tablas de contingencia y gr√°ficos de barras apiladas/agrupadas:&lt;/strong> Muestran la distribuci√≥n conjunta.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Pruebas de significaci√≥n:&lt;/strong> Para evaluar la independencia entre las variables.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Matrices de correlaci√≥n:&lt;/strong> Visualizan las correlaciones entre m√∫ltiples variables num√©ricas simult√°neamente, a menudo con mapas de calor (heatmaps).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;code>dlookr&lt;/code> facilita la exploraci√≥n de relaciones:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Visualizar la matriz de correlaci√≥n entre todas las variables num√©ricas&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot_cor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_df&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Analizar la relaci√≥n entre una variable objetivo (&amp;#39;mpg&amp;#39;) y otra caracter√≠stica (&amp;#39;wt&amp;#39;)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># plot_eda() permite explorar diversas relaciones bivariadas.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot_eda&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_df&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">target&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;mpg&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feature&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;wt&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># Num√©rica vs Num√©rica (scatterplot)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Relaci√≥n entre &amp;#39;mpg&amp;#39; (num√©rica) y &amp;#39;cyl&amp;#39; (considerada categ√≥rica aqu√≠)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot_eda&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_df&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">target&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;mpg&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feature&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;cyl&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># Num√©rica vs Categ√≥rica (boxplot)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="5-transformaci√≥n-de-los-datos">5. Transformaci√≥n de los Datos&lt;/h3>
&lt;p>Una vez que hemos comprendido nuestros datos, es posible que necesitemos transformarlos para que sean m√°s adecuados para los algoritmos de machine learning o para mejorar el rendimiento del modelo.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Manejo de asimetr√≠a:&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Transformaciones logar√≠tmicas, de ra√≠z cuadrada o de Box-Cox:&lt;/strong> Pueden normalizar distribuciones sesgadas, reduciendo la influencia de valores extremos.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Escalado de caracter√≠sticas:&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Normalizaci√≥n (Min-Max Scaling):&lt;/strong> Escala los datos a un rango fijo (por ejemplo, [0, 1]). √ötil para algoritmos sensibles a la escala como SVM o redes neuronales.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Estandarizaci√≥n (Z-score Scaling):&lt;/strong> Transforma los datos para que tengan una media de 0 y una desviaci√≥n est√°ndar de 1. Es com√∫n en algoritmos basados en distancia (k-NN, K-Means, PCA).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Codificaci√≥n de variables categ√≥ricas:&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>One-Hot Encoding:&lt;/strong> Convierte variables categ√≥ricas en m√∫ltiples columnas binarias, una por cada categor√≠a. Esencial para algoritmos que solo trabajan con entradas num√©ricas.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Label Encoding:&lt;/strong> Asigna un n√∫mero entero a cada categor√≠a. √ötil si hay un orden inherente en las categor√≠as.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ingenier√≠a de Caracter√≠sticas (Feature Engineering):&lt;/strong> Crear nuevas variables a partir de las existentes. Esto puede ser tan simple como combinar dos columnas o tan complejo como extraer informaci√≥n de texto o im√°genes. Esta etapa es a menudo la que m√°s impacto tiene en el rendimiento del modelo.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;code>dlookr&lt;/code> ofrece funciones √∫tiles para la transformaci√≥n de datos:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Transformaci√≥n logar√≠tmica para reducir la asimetr√≠a de una variable&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">mtcars_transformed_log&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">transform_df&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_df&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mpg&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mpg&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Compara la distribuci√≥n de mpg original vs. transformada&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot_normality&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_df&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;mpg&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">plot_normality&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_transformed_log&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;mpg&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Binarizaci√≥n o discretizaci√≥n de una variable continua (ej. &amp;#39;hp&amp;#39; en 3 bins)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">mtcars_binned&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">binning&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_df&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;hp&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">head&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_binned&lt;/span> &lt;span class="o">%&amp;gt;%&lt;/span> &lt;span class="nf">select&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hp_Binned&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Estandarizaci√≥n de variables num√©ricas (Z-score)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">mtcars_scaled&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">normalize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_df&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">method&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;scale&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">head&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mtcars_scaled&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># Observa c√≥mo los valores de todas las columnas han cambiado&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="herramientas-populares-para-eda">Herramientas Populares para EDA&lt;/h2>
&lt;p>Para realizar un EDA efectivo, contamos con potentes herramientas en lenguajes como R y Python:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>En R:&lt;/strong>
&lt;ul>
&lt;li>El ecosistema &lt;code>tidyverse&lt;/code> (&lt;code>dplyr&lt;/code> para manipulaci√≥n, &lt;code>ggplot2&lt;/code> para visualizaci√≥n) es indispensable.&lt;/li>
&lt;li>Paquetes espec√≠ficos para EDA como &lt;strong>&lt;code>dlookr&lt;/code>&lt;/strong>, es excelente por su enfoque estructurado en el diagn√≥stico de calidad, exploraci√≥n y transformaci√≥n de datos, ofreciendo funciones y reportes automatizados que agilizan el proceso. Otros paquetes √∫tiles incluyen &lt;code>DataExplorer&lt;/code>, &lt;code>skimr&lt;/code>, y &lt;code>visdat&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>En Python:&lt;/strong>
&lt;ul>
&lt;li>&lt;code>pandas&lt;/code> para manipulaci√≥n de datos.&lt;/li>
&lt;li>&lt;code>matplotlib&lt;/code> y &lt;code>seaborn&lt;/code> para visualizaci√≥n est√°tica.&lt;/li>
&lt;li>&lt;code>plotly&lt;/code> para visualizaciones interactivas.&lt;/li>
&lt;li>Bibliotecas como &lt;code>missingno&lt;/code> para visualizar valores faltantes, &lt;code>pandas_profiling&lt;/code> para informes autom√°ticos de EDA, y &lt;code>sweetviz&lt;/code> para comparaciones.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="conclusi√≥n">Conclusi√≥n&lt;/h2>
&lt;p>El An√°lisis Exploratorio de Datos no es solo una fase inicial, sino un proceso continuo de aprendizaje sobre tus datos. Es una inversi√≥n de tiempo que rinde grandes dividendos, ya que una comprensi√≥n profunda de los datos nos permite tomar decisiones m√°s informadas, construir modelos m√°s robustos y, en √∫ltima instancia, extraer conocimientos m√°s valiosos. Al dominar el EDA, te equipas con la habilidad de transformar datos brutos en una historia coherente y accionable, evitando la trampa del GIGO y asegurando que tus esfuerzos de ciencia de datos generen un impacto real.&lt;/p></description></item><item><title>Como entrenar y validar un modelo de machine learnig</title><link>https://maicel.netlify.app/post/como-entrenar-y-validar-un-modelo-de-machine-learnig/</link><pubDate>Thu, 08 Feb 2024 00:00:00 +0000</pubDate><guid>https://maicel.netlify.app/post/como-entrenar-y-validar-un-modelo-de-machine-learnig/</guid><description>&lt;p>üéß &lt;strong>Escucha el podcast de esta publicaci√≥n&lt;/strong>&lt;br>
&lt;audio controls >
&lt;source src="https://ia600607.us.archive.org/24/items/articulo-steyember/_Articulo_Steyember.mp3" type="audio/mpeg">
&lt;/audio>
&lt;/p>
&lt;h1 id="estrategia-de-modelado">Estrategia de modelado&lt;/h1>
&lt;p>Contar con una estrategia de modelado correcta es esencial para desarrollar y validar modelos de predicci√≥n. En este art√≠culo, exploraremos las siete etapas clave del proceso de modelado propuesto por Ewout Steyerberg en su art√≠culo .&lt;/p>
&lt;h2 id="sec-1">1. Definici√≥n del problema e inspecci√≥n de datos&lt;/h2>
&lt;p>El primer paso en cualquier proyecto de modelado es definir claramente el problema de investigaci√≥n y seleccionar la variable de resultado adecuada.&lt;/p>
&lt;p>Durante esta fase, tambi√©n realizamos un an√°lisis exploratorio de datos (EDA) para comprender las caracter√≠sticas de las variables y detectar posibles problemas, como datos at√≠picos o valores faltantes.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Instalaci√≥n y Carga de Librer√≠as&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">caret&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">## Loading required package: ggplot2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">## Loading required package: lattice
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">MLDataR&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># para utilizar la biblioteca diabetes_data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dplyr&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">##
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">## Attaching package: &amp;#39;dplyr&amp;#39;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">## The following objects are masked from &amp;#39;package:stats&amp;#39;:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">##
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">## filter, lag
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">## The following objects are masked from &amp;#39;package:base&amp;#39;:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">##
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">## intersect, setdiff, setequal, union
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dlookr&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># para EDA&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">## Registered S3 methods overwritten by &amp;#39;dlookr&amp;#39;:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">## method from
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">## plot.transform scales
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">## print.transform scales
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">## Because it is an offline environment, only offline fonts are imported.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">##
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">## Attaching package: &amp;#39;dlookr&amp;#39;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">## The following object is masked from &amp;#39;package:base&amp;#39;:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">##
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">## transform
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">predtools&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Cargar el conjunto de datos&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;gusto&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">gusto&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="n">gusto&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># EDA&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">descripcion&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">overview&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gusto&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">summary&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">descripcion&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># descripci√≥n general del conjunto de datos&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚îÄ‚îÄ Data Scale ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of observations : 40,830&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of variables : 29&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of values : 1,184,070&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Size of located memory(bytes) : 5,241,552 &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚îÄ‚îÄ Duplicated Data ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of duplicated observations : 4 (0.01%) &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚îÄ‚îÄ Missing Data ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of completed observations : 30,510&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of observations with NA : 10,320 (25.28%)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of variables with NA : 1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of NA : 10,320 &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚îÄ‚îÄ Data Type ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of numeric variables : 1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of integer variables : 13&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of factors variables : 0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of character variables : 0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of Date variables : 0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of POSIXct variables : 0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚Ä¢ Number of other variables : 15 &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## ‚îÄ‚îÄ Individual variables ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## Variables Data Type&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 1 day30 integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 2 sho integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 3 hig integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 4 dia labelled&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 5 hyp integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 6 hrt integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 7 ttr labelled&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 8 sex labelled&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 9 Killip labelled&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 10 age numeric&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 11 ste labelled&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 12 pulse labelled&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 13 sysbp labelled&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 14 ant integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 15 miloc labelled&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 16 height labelled&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 17 weight labelled&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 18 pmi labelled&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 19 htn labelled&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 20 smk labelled&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 21 pan integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 22 fam labelled&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 23 prevcvd integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 24 prevcabg integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 25 regl integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 26 grpl integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 27 grps integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 28 tpa integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## 29 tx labelled&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Para conocer m√°s detalles sobre el proceso de ¬®Exploratory Data Analysis (EDA)¬® ver la publicaci√≥n dedicada a este tema.&lt;/p>
&lt;h2 id="sec-2">2. Codificaci√≥n de las Variables Predictoras&lt;/h2>
&lt;p>La codificaci√≥n adecuada de las variables predictoras es fundamental para construir modelos robustos. En este estudio, se utilizaron t√©cnicas como la agrupaci√≥n de categor√≠as poco frecuentes y la creaci√≥n de predictores res√∫menes para simplificar informaci√≥n correlacionada. Adem√°s, cuando las relaciones entre variables no son lineales, aplicamos herramientas como splines c√∫bicos restringidos , que permiten capturar patrones complejos sin comprometer la precisi√≥n del modelo.&lt;/p>
&lt;h2 id="sec-3">3 .Especificaci√≥n del Tipo de Modelo&lt;/h2>
&lt;p>La elecci√≥n del modelo depende del tipo de relaci√≥n que queremos capturar entre las variables. En este estudio, combinamos dos enfoques: regresi√≥n log√≠stica binaria m√∫ltiple y √°rboles de clasificaci√≥n . Mientras que la regresi√≥n log√≠stica es ideal para modelar relaciones lineales y proporcionar probabilidades, los √°rboles de clasificaci√≥n son √∫tiles para identificar interacciones complejas y establecer indicadores de riesgo.&lt;/p>
&lt;p>Un aspecto clave fue la selecci√≥n de predictores finales, que se bas√≥ en criterios como la plausibilidad biol√≥gica , el respaldo de la literatura cient√≠fica y m√©todos computacionales avanzados. Esto nos permiti√≥ evitar el uso exclusivo de valores p, que pueden ser enga√±osos en algunos contextos.&lt;/p>
&lt;h2 id="sec-4">4. Estimaci√≥n del Modelo&lt;/h2>
&lt;p>Para ajustar los par√°metros del modelo, utilizamos el m√©todo de m√°xima verosimilitud (MLE) , reconocido por su versatilidad y eficiencia computacional. Adem√°s, evaluamos cuidadosamente las interacciones entre variables predictoras, incluy√©ndolas solo cuando hab√≠a evidencia emp√≠rica y te√≥rica que respaldaba su relevancia. Un modelo m√°s simple suele ser m√°s robusto y f√°cil de interpretar, lo que es crucial en entornos cl√≠nicos.&lt;/p>
&lt;h2 id="sec-5">5. Evaluaci√≥n del Rendimiento del Modelo&lt;/h2>
&lt;p>El rendimiento del modelo se eval√∫a mediante m√©tricas como calibraci√≥n y discriminaci√≥n . La calibraci√≥n mide la concordancia entre las predicciones y los resultados observados, mientras que la discriminaci√≥n eval√∫a la capacidad del modelo para distinguir entre pacientes con diferentes resultados. Herramientas como las rectas de calibraci√≥n y la validaci√≥n cruzada de 10 pliegues fueron fundamentales para asegurar la calidad del modelo.&lt;/p>
&lt;h2 id="sec-6">6. Evaluaci√≥n de la Validez del Modelo&lt;/h2>
&lt;p>La validaci√≥n del modelo es un paso cr√≠tico para garantizar su aplicabilidad en diferentes contextos. En este estudio, utilizamos tanto validaci√≥n interna como externa , empleando particiones temporales y geogr√°ficas para reflejar escenarios reales. Este enfoque nos permiti√≥ evaluar la robustez del modelo frente a cambios en el tiempo y variaciones regionales.&lt;/p>
&lt;h2 id="sec-7">7. Presentaci√≥n del Modelo&lt;/h2>
&lt;p>La presentaci√≥n del modelo puede ser a trav√©s de un nomograma o aplicaci√≥n.&lt;/p>
&lt;h1 id="bibliograf√≠a">Bibliograf√≠a&lt;/h1>
&lt;ol>
&lt;li>Steyerberg EW, Vergouwe Y. Towards better clinical prediction models: seven steps for development and an ABCD for validation. European Heart Journal [Internet]. 1 de agosto de 2014 [citado 9 de mayo de 2021];35(29):1925-31. Disponible en: &lt;a href="https://academic.oup.com/eurheartj/article-lookup/doi/10.1093/eurheartj/ehu207" target="_blank" rel="noopener">https://academic.oup.com/eurheartj/article-lookup/doi/10.1093/eurheartj/ehu207&lt;/a>&lt;/li>
&lt;/ol></description></item></channel></rss>